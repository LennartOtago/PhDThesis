\chapter{Theoretical and Technical Background}

\section{Forward Model}

\begin{figure}[ht!]
	\centering
	\scalebox{0.9}{\input{FirstLIMB.pdf_tex}}
	\label{fig:FirstLIMB}
\end{figure}

In this section we describe the forward model which we use to simulate data and base the Bayesian inference on.

As shown in Figure~\ref{fig:forModel}, one measurement of a stationary satellite can be describes as the path integral along the line of sight $\Gamma_j$ for $j=1,2,\ldots,m$.
For each measurement we can define a tangent height $h_{\ell_j}$ as the shortest distance along the line of sight to the earth.
%and call that the limb height.

%A stationary satellite takes $m$ measurements in pointing directions that define the lines of sight from the satellite $\Gamma_j$ for $j=1,2,\ldots,m$. Each line of sight $\Gamma_j$ is tangent to a sphere around Earth called a limb, and the tangent height $\ell_j$ is called the limb height. Hence the limb height $\ell_j$ is the lowest height to Earth along the line-of-sight $\Gamma_j$.

The $j^\text{th}$ measurement, taken on line of sight $\Gamma_j$  is modelled by the the radiative transfer equation (RTE)~\cite{united2006handbook}
% \begin{align}
	%    y_j =   \int_{\Gamma_j} \underbrace{  S(\nu, T)   \frac{p(T)}{k_{\text{B}} T(r)} B(\nu,T)}_{ \bm{A_{j}} }  x(r) \tau(r) \text{d}r \, 
	%    \label{eq:RTE}
	% \end{align}
\begin{align}
	y_j =   \int_{\Gamma_j}  B(\nu,T) k(\nu, T)   \frac{\bm{p}(T)}{k_{\text{B}} \bm{T}(r)}  \bm{x}(r)  \tau(r) \text{d}r + \eta_j \, \\
	\tau(r) = \exp{ \Bigl\{ - \int^{r}_{r_\text{obs}}  k(\nu, T)   \frac{\bm{p}(T)}{k_B \bm{T}(r^{\prime})}  \bm{x}(r^{\prime}) \text{d}r^{\prime} \Bigr\} }
	\label{eq:RTE}
\end{align}
where the path from the satellite along the line-of-sight of the $j^\text{th}$ pointing direction is $\Gamma_j$ and the ozone concentration$\bm{x}(r)$ at distance $r$ from the radiometer.
The factor $\tau(r)\leq 1$ accounts for re-absorption of the radiation along the line-of-sight, which makes the RTE non linear.
The noise $\eta_j$ is added to each path integral, where the noise vector $ \bm{\eta} \sim \mathcal{N}(0, \gamma^{-1} \mathbf{I} )$ is normally distributed around zero with the noise precision $\gamma$.
The absorption constant $k(\nu, T)$ for a single gas molecule at a specific wavenumber $\nu$ is given by the HITRAN database \cite{gordon2022hitran2020} and acts as a source function when multiplied with the black body radiation $B(\nu,T)$, given by Planck`s law.
Within the stratosphere the number density $p(T) / (k_{\text{B}} T(r))$ of molecules is dependent on the pressure $p(T)$, the temperature $T(r)$, and the Boltzmann constant $k_{\text{B}}$.
For fundamentals on the Radiative transfer equation we recommend 79BOOKRadiativeProcess.

%The path integral over the line $\Gamma_j$ from the satellite to the end of the atmosphere defines the kernel $\bm{A_{j}}$ that is independent of ozone concentration $x$. 
We parametrize the ozone profile as a function of height, discretized into the $n$ values in each of $n$ layers of the discretized stratosphere where the $i^\text{th}$ layer is defined by two spheres of radii  $h_{i-1} < h_{i}$, $i = 1, \dots, n$, with $h_0$ and $h_{n} $.
In between the heights $h_{i-1}$ and $h_{i}$, each of the ozone concentration $x_{i}$, the pressure $p_{i}$, the temperature $T_{i}$, and thermal radiation is assumed to be constant.
Above $h_{n}$ and below $h_0$, the ozone concentration is set to zero, so no signal can be obtained.
Then depending on the parameter of interest, which is either the ozone volume mixing ratio $\bm{x} =\{x_1,x_2,\ldots,x_n\} \in \mathbb{R}^{n}$ or the fraction of pressure and temperature $\bm{p/T}= \{p_1/T_1,p_2/T_2,\ldots,p_n/T_n\} \in \mathbb{R}^{n} $, we can rewrite the integral in Eq.~\eqref{eq:RTE} as e.g. $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} $, where the absorption $\tau(r)$ induces non-linearity.
Here, the row vector $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{n}$  defines a Kernel for each measurement so that the data vector
\begin{align}
	\bm{y} = \bm{A}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} + \bm{\eta}= \bm{A}(\bm{x},  \bm{p},\bm{T}) \,
	\frac{ \bm{p}}{\bm{T}} + \bm{\eta} \, .
\end{align}
can be written as a matrix vector multiplication, where the matrix $\bm{A}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{m \times n}$  and the noise vector $\bm{\eta} \in \mathbb{R}^{m}$.

Since the absorption $\tau(r)$ reduces measurements by of order $1\%$, or less, making the inverse problem only weakly non-linear. 
We use that to approximate the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T})$ with a map $\bm{M}$ so that $\bm{A}(\bm{x},  \bm{p},\bm{T}) \approx \bm{M} \bm{A}_L $
Where each row $\bm{A}_{L,j} $ of matrix as $\bm{A}_L \in \mathbb{R}^{m \times n}$ is defined by the linear forward model, where absorption is neglected, e.g. $\tau = 1$. 
Then $\bm{A}_{L,j} $ is either defined by $ B(\nu,T) S(\nu, T)   \frac{\bm{p}(T)}{k_{\text{B}} \bm{T}(r)}  \text{d}r$ or $B(\nu,T) S(\nu, T)   \frac{\bm{x}}{k_{\text{B}}}  \text{d}r$, as in Eq..~\eqref{eq:RTE}, depending on the parameter of interest.
This poses a linear inverse problem with the forward map defined by the matrix $\bm{A} = \bm{M} \bm{A}_L$, where $\bm{M}$ is, more specifically, an affine map.


\section{Affine Map}

To approximate the non-linear forward model we use an affine map $ M:\bm{A}_L \bm{x} \rightarrow \bm{A}(\bm{x},  \bm{p},\bm{T})) \bm{x}$, which maps the linear forward model $\bm{A}_L \bm{x}$ onto the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}$.


An affine map is any linear map in between two vector spaces is or affine spaces, where in affine space does not need to have a zero origin. 2.3.1. PROPOSITION AND DEFINITIOn Berge book\cite{}.
In other words an affine map does not need to preserve the origin, or is a linear map on vector spaces including translation, or in the words of my supervisor, C. F., an affine map is a Taylor series of first order.
For more information on affine spaces and maps we refer to \cite{two books}

We generate two affine subspaces spaces \newline $V = \big\{ \bm{A}(\bm{x}^{(1)}, \bm{p,T}), \dots ,\bm{A}(\bm{x}^{(m)}, \bm{p,T})\big\} $ and $W = \big\{ \bm{A}\bm{x}^{(1)}, \dots ,\bm{A}\bm{x}^{(m)}\big\}$ over the same field, with fixed $\bm{p,T}$.
The parameter $\bm{x}$ is distributed as the so-called posterior distribution $\big\{  \bm{x}^{(1)} , \dots, \bm{x}^{(m)} \big\} \sim \pi(\bm{x}|\bm{\theta},\bm{y})$, with hyper-parameters $\bm{\theta}$, according to a Bayesian hierarchical model.



\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[rectnode] at (2,-4) (NL)    {$W$};
		\node[rectnode] at (-2,-4) (L)    {$V$};
		\draw[<-, very thick] (NL.west) -- (L.east); 
		\node[align=center] at (-5.5,-4) (f3) {linear forward model};
		\node[align=center] at (5.5,-4) (f4) {non-linear forward model};
		\node[align=center] at (0,-5) (f5) {$\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}  \approx \bm{M A}_L  \bm{x} = \bm{A x} $ };
		\node[align=center] at (0,-4) (f5) {affine Map \\ $\bm{M}$};
	\end{tikzpicture}
	\caption[Schematics of Affine Map]{Schematics of Affine Map, which approximates the linear forward model to the non-linear forward model.}
\end{figure}

\section{Bayesian Inference}
In this this section we give a short introduction to Bayesian inference for a general parameter $\bm{x}$ given some data 
\begin{align}
	\bm{y} = \bm{Ax} + \bm{\eta} \, 
\end{align}
based on a linear forward model $\bm{A}$ and some noise $ \bm{\eta}$.
Later in section \ref{} we set up a more sophisticated Bayesian framework applied to the forward model in section \ref{}.

We can visualise the correlation structure of a measurement process through a hierarchiallly ordered directed acyclic graph (DAG), see Figure \ref{fig:FirstDAG}.
As an observatory process naturally includes some random noise we include that in our DAG and classify the noise as a hyper-parameter in $\bm{\theta}$.
Other hyper-parameters influence the parmeters $\bm{x}$ detemernistaclly, which are then mapped through the forward model onto the space of all measurables $\bm{u}$, from which we observe some data $\bm{y}$ including noise as previously mentioned.
Drawing a DAG can help us to dependences within the measurement and modelling process.
Given some data we inferer the distribution of the underling parameters and hyper-parameters by following the arrows in Figure \ref{} backwards and set up a Bayesain hierachlly ordered model.


\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
	\node[roundnode2] at (0,3.5) (th)    {$\bm{\theta}$};
	\node[roundnode2] at (0,1.5) (x)    {$\bm{x}$};
	\node[roundnode2] at (0,-1.5) (u)    {$\bm{u}$};
	\node[rectnode] at (0,-3.5) (y)    {$\bm{y}$};
	
	\draw[->, very thick] (th.south) -- (x.north); 
	\draw[->, very thick] (x.south) -- (u.north); 
	\draw[->, very thick, mydotted] (u.south) -- (y.north); 
	\draw[->, very thick, mydotted] (th) edge[bend right=60] (y);  
	
	\node[align=center] at (3,3.5) (tht) {hyper-parameters};
	\node[align=center] at (3,1.5) (xt) {parameters};
	\node[align=center] at (1.1,0) (At) {$\bm{A}$ \quad forward model};
	\node[align=center] at (3,-1.5) (ut) {space of all measurables};
	\node[align=center] at (3,-3.5) (yt) {data};
	\node[align=center] at (-3,0) (nt) {noise};
\end{tikzpicture}

	\caption[Bayesian Inference DAG]{The directed acyclic graph (DAG) for a typical linear inverse problem visualises forward dependencies as solid line arrows for deterministic dependencies and dotted arrows for statistical dependencies.
	Naturally the data $\bm{y}$ has some noise described through included in some hyper-parameters $\bm{\theta}$.
	The parameters $\bm{x}$ have some dependency of those hyper-parameters $\bm{\theta}$. The parameter $\bm{x}$ is mapped onto the space of all measurables $\bm{u}$ through the linear forward model $\bm{A}$, so that $\bm{Ax}$ is a linear operation.
	From the space of all measurables we can observe some data $\bm{y}$, statistically, where as prevoiusly mentioned some random noise is added.
	We set up a more sophisticated Bayesian model in chapter \ref{} explicitly including all hyper-parameters and parameters of interest according to the forward model in section \ref{}.}
	\label{fig:FirstDAG}
\end{figure}



Within a linear Bayesian hierarchial model we need to define a likelihood function as well as distribution over the unknown parameters $\bm{x}$ and hyper-parameters $\bm{\theta}$.
\begin{subequations}
	\begin{align}
		\bm{y}|\bm{x}, \bm{\theta}&\sim \mathcal{N}(\bm{A} \bm{x}, \bm{\Sigma}(\bm{\theta})) \label{eq:likelihood}  \\
		\bm{x}| \bm{\theta} & \sim  \mathcal{N}( \bm{\mu}, \bm{Q}^{-1}(\bm{\theta})  ) \label{eq:xPrior} \\
		\bm{\theta} &\sim  \pi(\bm{\theta}) \label{eq:gammaPrior}\, ,
	\end{align}
	\label{eq:BayMode}
\end{subequations}
with the noise covariance matrix $\bm{\Sigma}(\bm{\theta})$, so that $\bm{\eta}  \sim \mathcal{N}(0, \bm{\Sigma}(\bm{\theta})) $ as in Eq. \ref{}, the prior precision matrix $\bm{Q}(\bm{\theta})$, prior mean $\bm{\mu}$
and some prior distribution over the hyper-parameters $\pi(\bm{\theta})$.
Through sensibly choosing the prior distributions  $\pi(\bm{x}|\bm{y})$ as well as the hyper-parameters $\bm{\theta}$ and their prior distribution  $\pi(\bm{\theta})$, we can incorporate functional dependencies as well physical properties of the parameters.
The likelhood function $\pi(\bm{y}|\bm{x}, \bm{\theta})$ is a measure of how the parameters and hyper-parameters fit to the data according to our forward model, including information of the measurement process.

With a normally distributed prior and likelihood function this becomes a linear-Gaussian Bayesian hierarchical model.
For more detailed Bayesian analysis we recommend \cite{}.

The posterior distribution, the function of interest,
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) = \frac{ \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta})}{\pi(\bm{y})} \, ,
\end{align}
is given according to Bayes' theorem \cite{}, with the prior distribution $\pi(\bm{x}, \bm{\theta})$ and the normalising constant $\pi(\bm{y})$.
If the normalising constant is finite and non-zero we can approximate the posterior distribution
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) \propto \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta}) \, .
\end{align}

Then the expectation of any a function $h(\bm{x},\bm{\theta})$ can be described as 
\begin{align}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})] =  \int \int   h(\bm{x},\bm{\theta}) \,  \pi(\bm{x}, \bm{\theta} | \bm{y} ) \, \text{d} \bm{x}  \, \text{d} \bm{\theta}   \label{eq:expPos} \, ,
\end{align}
which is usually a high dimensional integral and computationally not feasible to solve.

One way to work around the high dimensionality is to parameterise $\bm{x}$ using hyper-parameters $\bm{\theta}$ so that $\bm{x}(\bm{\theta})$. 
Another way is to seperate the posterior distribution over latent field $\bm{x}$ and the hyper-parameters $\bm{\theta}$.
This is particular benefitial, when $\bm{x}$ is high dimensional, e.g. $\bm{x} \in \mathbb{R}^n$ with $n = 45$ and can not be parametrised, and $\bm{\theta}$ is low dimensional, e.g. two dimensional.


\subsection{Marginal and then Conditional}
The marginal and then conditional (MTC) method factorises the full posterior distribution 
\begin{align}
	\pi(\bm{x}, \bm{\theta}|\bm{y}) = \pi(\bm{x}| \bm{\theta}, \bm{y}) \pi(\bm{\theta}|\bm{y})
\end{align}
into the marginal posterior distribtion $ \pi(\bm{\theta}|\bm{y})$ and conditional posterior distribution $\pi(\bm{x}| \bm{\theta}, \bm{y})$.


For the in Eq. \ref{} specified linear-Gaussian Bayesian hierarchical model the marginal posterior distribution is given as
\begin{align}
	\pi(\bm{\theta} | \bm{y}) &= \int \pi(\bm{x}, \bm{\theta} | \bm{y}) \diff \bm{x} \\ 
	\label{eq:condHyper}
	&\propto \sqrt{ \frac{ \det( \bm{\Sigma}^{-1} ) \,  \det( \bm{Q}) }{\det( \bm{Q} + \bm{A}^T \bm{\Sigma}^{-1} \bm{A} ) } } \times \exp \Big[ - \frac{1}{2}(\bm{y} -\bm{A \mu})^T \bm{Q}_{\bm{\theta|y}} (\bm{y} -\bm{A \mu}) \Big] \pi(\bm{\theta}) \, ,
\end{align}
with
\begin{align}
	\bm{Q}_{\bm{\theta|y}} = \bm{\Sigma}^{-1} - \bm{\Sigma}^{-1} \bm{A} (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \bm{A}^T \bm{\Sigma}^{-1} \,  .
\end{align}

See lemma \cite{}.

Then conditioned on the hyper-parameters $\bm{\theta}$ we can draw samples of the conditional posterior distribution
\begin{align}
	\bm{x}| \bm{y} , \bm{\theta} \sim \mathcal{N}\big( \bm{\mu} + (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \bm{A}^T \bm{\Sigma}^{-1} (\bm{y} - \bm{A} \bm{\mu}), (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \big) \, ,
\end{align}
see section \ref{} or calculate weighted expectations of a function $h(\bm{x})$
\begin{align}
	\label{eq:lte}
	\text{E}_{\bm{x}|\bm{y}} [h(\bm{x})] = \int   \text{E}_{\bm{x}|\bm{\theta},\bm{y}} [h(\bm{x})] \, \pi(\bm{\theta} | \bm{y} )  \, \text{d} \bm{\theta} \,  ,
\end{align}
with weights given by $\pi(\bm{\theta} | \bm{y} )$.
\cite{}
Note that the noise covariance $\bm{\Sigma}^{-1} = \bm{\Sigma}^{-1}( \bm{\theta}) $ and the prior precision $\bm{Q} = \bm{Q}( \bm{\theta})$ are depending on hyper-parameters $\bm{\theta}$.

In this thesis we will use sampling and deterministic methods to characterise the posterior distribution over the hyper-parameters and present the basics of those in the following sections.

\section{Regularisation}
Another method to find a solution to a linear inverse problem as in Eq. \ref{} is to find a solution $\bm{x}_{\lambda}$ accdoring to a data misfit norm and a regularisation semi-norm \cite{}.
We will discuss the case of Tikhonov regularisation.

For a parameter $\bm{x}$ a linear forward model and some data $\bm{y}$ the data misfit norm is defined as 
\begin{align}
	\left\lVert \bm{y} - \bm{A}\bm{x} \right\rVert \, .
\end{align}
The regularisation semi norm 
\begin{align}
	\lambda \left\lVert \bm{T}\bm{x}  \right\rVert
\end{align}
penalises $\bm{x}$ according to $\bm{T}$ and the regularisation parameter $\lambda \geq 0$.
Given $\lambda$ the regularised soltuion
\begin{align}
	\bm{x}_{\lambda} = \underset{\bm{x}}{\text{arg min}} \left\lVert \bm{y} - \bm{A}\bm{x}  \right\rVert^2 + \lambda \left\lVert \bm{T}\bm{x} \right\rVert^2
\end{align}
can be found by the derivativ
\begin{align}
&  &\nabla_{\bm{x}} \big\{  (\bm{y} - \bm{A}\bm{x}_{\lambda} )^T  (\bm{y} - \bm{A}\bm{x}_{\lambda})  + \lambda \bm{x}_{\lambda}^T  \bm{T}^T \bm{T} \bm{x}_{\lambda} \big\} &= 0 \\
&\iff &\nabla_{\bm{x}} \big\{ \bm{y}^T \bm{y} + \bm{x}_{\lambda}^T \bm{A}^T  \bm{A}\bm{x}_{\lambda} - \bm{y}^T \bm{A}\bm{x}_{\lambda} - \bm{x}_{\lambda}^T \bm{A}^T   \bm{y}   + \lambda \bm{x}_{\lambda}^T  \bm{T}^T \bm{T} \bm{x}_{\lambda} \big\} & = 0  \\
& \iff& 2\bm{A}^T  \bm{A}\bm{x}_{\lambda} -2 \bm{A}^T   \bm{y}  + 2 \lambda   \bm{T}^T \bm{T} \bm{x}_{\lambda}  & = 0 
\end{align}
Then a regularised solution is given as:
\begin{align}
	\bm{A}^T   \bm{y}  (\bm{A}^T  \bm{A} + \lambda   \bm{L} )^{-1} = \bm{x}_{\lambda}, 
\end{align}
where we can set $\bm{T}^T \bm{T} = \bm{L}$, which is typically, banded matrix approximation to the nth derivative\cite{}.
To find the best regularised solution $\bm{x}_{\lambda}$ is calculated for a range of $\lambda$ so thate the regularised solution minimizes the data-misfit as well as the regularised semi-norm, which can be done via a L-Curve.
We will further specify $\bm{L}$ in section \ref{} as well as plot an L-Curve.



\section{Sampling Methods}
In this chapter we present the sampling methods used in this thesis and also argue why we use sampling methods to calculate the expectation
\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}, \bm{\theta})] = \underbrace{ \int \int h(\bm{x},\bm{\theta}) \, \text{d} \bm{\theta} \, \text{d} \bm{x} }_{\bm{\mu}_{\text{int}}} 
\end{align}
of a function  $h(\bm{x}, \bm{\theta})$ with respect to a probability density $\pi( \bm{x},\bm{\theta}|\bm{y})$.
In the case where the calculation of the integral is not feasible we can approximate Eq. \ref{} with a sample based estimate
\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}, \bm{\theta})] \approx \underbrace{ \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)},\bm{\theta}^{(k)})  }_{\bm{\mu}_{\text{samp}}} \, ,
\end{align}
for large enough samples size $N$ of a sample set $\mathcal{M} = \{  (\bm{x}, \bm{\theta} )^{(1)}, \dots, (\bm{x}, \bm{\theta} )^{(k)} , \dots,  (\bm{x}, \bm{\theta})^{(N)}  \} \sim \pi(\bm{x},\bm{\theta}| \bm{y}) $.
The variance of this unbiased estimator is $\mathcal{O}(1/N)$ so large samples sizes can reduce the uncertainty of $\bm{\mu}_{samp}$ \cite{}. see chapter 5 in Ridley and Roberts 2004
We can do this as the central limit theorem states that the samples means $ \bm{\mu}^{(i)}_{samp} $, of samples sets $\mathcal{M}_i$ for $i = 1, \dots, n$ of any distribution , converge in distribution to a normal distribution so that
\begin{align}
	 \sqrt{n} (\bm{\mu}^{(i)}_{\text{samp}} -  \bm{\mu}_{\text{int}} ) \overset{\mathcal{D}}{\longrightarrow} \mathcal{N} (0,\sigma^2) ,
\end{align}
if $\sigma^2 < \infty$ with a bound error $\bm{\mu}^{(i)}_{\text{samp}} -  \bm{\mu}_{\text{int}} $.

For us it is now important to show that we the methods we use draw samples from a target distribution so that $ \mathcal{M} = \{ (\bm{x}, \bm{\theta} )^{(1)}, \dots, (\bm{x}, \bm{\theta} )^{(k)} , \dots,  (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x},\bm{\theta}| \bm{y}) $ and that we then use sample based estimates as in Eq. \ref{}.
In doing so we will use Markov-Chain Monte Carlo (MCMC) methods, where we hava to show that the produces Markov-Chain $\mathcal{M}$ is ergodic.
The ergodicy theorem states that, if an aperiodic and irreducible Markov chain $\mathcal{M}$ is reversible \ref{} then it converges towards a stationnary distrbution unique equilibirum distribution $\pi(\bm{x},\bm{\theta}| \bm{y}) $.
In other words if from a state in the chain we can reach every other state in the sampling space and the previous state, and we do not get stuck in periodic loop, then the chain converges and we can use sample based estimates.
In practise one can look at the trace $\pi(\bm{x}^{(k)},\bm{\theta}^{(k)}| \bm{y}) $ for $k = 1, \dots, N$ of the samples and eyeball ergodicity.


The sampling methods used in this thesis have proven ergodic properties, so we will cite and refer the reader to the respective documents.
Next we will introduce the methods used in this thesis.

\subsection{Metropolis- within Gibbs sampling}

As introduced in the MTC sektion we will sample from $\pi(\bm{\theta}| \bm{y})$ seperatly from $\pi(\bm{x}|\bm{\theta}, \bm{y}) $.
To sample from $\pi(\bm{\theta}| \bm{y})$ we use a Metropolis-within-Gibbs sampler and introduce this for the 2 dimesnional case, as this is what we deal with in this tehsis, where $\bm{\theta}  =( \theta_1 , \theta_2) $, we do a metroplis step in $\theta_1$ deirection and a gibbs stepp in $\theta_2$ direction.



The Metropolis-within-Gibbs algorithm starts with a initial guess $\bm{\theta}^{(t)}$ at $t=0$.
We propose a new sample $\theta_1 \sim q( \theta_1 | \theta^{(t-1)}_1 )$ condition on the previous state according to a symmetric proposal distribution $q(\theta_1 | \theta^{(t-1)}_1 ) = q( \theta^{(t-1)}_1|\theta_1 ) $ , which is a special case of the Metropolis-Hastings algorithm \cite{} and cancels when computing the accpetacne probailiy $\alpha$. We accept and set $ \theta^{(t)}_1 = \theta_1$ with
\begin{align}
\alpha( \theta_1  | \theta_1^{(t-1)}) = \min \left\{ 1, \frac{\pi(\theta_1  | \theta^{(t-1)}_2, \bm{y}) q(\theta_1^{(t-1)} | \theta_1 )  }{\pi(\theta_1^{(t-1)}| \theta_2^{(t-1)}, \bm{y}) q(\theta_1 | \theta_1^{(t-1)})}  \right\}
\end{align}
or reject and keep $\theta^{(t)}_1 = \theta^{(t-1)}_1$, which we do by comparing $\alpha$ to a uniform random number $u \sim \mathcal{U}(0,1)$.


The we do a gibbs step in $\theta_2$ direction, where Gibbs sampling is a special case of the metropolis hastings algorihtm with the acceptance probaility of 1.
The next samples $\theta_2^{(t)} \sim  \pi( \cdot | \theta_1^{(t)} , \bm{y} ) $ conditioned on $\theta_1^{(t)}$ at step $t$.

We reapet this $N$ times where and assure convergence independent of the intial sample (irreducibility) we discard samples after the so-called burn-in period so that we produce a Markov-Chain of length $N - N_{\text{burn-in}}$.

A more mathematical prove of ergodicity for a more general Metropolis-within-Gibbs algorithm can be found in \cite{}.
ARRIS RECURRENCE OF METROPOLIS-WITHIN-GIBBS AND
TRANS-DIMENSIONAL MARKOV CHAINS
By Gareth O. Roberts and Jeffrey S. Rosentha 2006


\begin{algorithm}
	\caption{Metropolis within Gibbs}
	\begin{algorithmic}[1]
		\STATE Initialize and suppose two dimensional vector \( \bm{\theta}^{(0)}  =( \theta_1^{(0)} , \theta_2^{(0)}  ) \)
		\FOR{ \( k = 1, \dots, N \)}
		\STATE Propose \( \theta_1 \sim q(\cdot   | \theta_1 ^{(t-1)}) = q(\theta_1 ^{(t-1)} |\cdot  ) \)
		\STATE Compute
		\[ \alpha( \theta_1  | \theta_1^{(t-1)}) = \min \left\{ 1, \frac{\pi(\theta_1  | \theta^{(t-1)}_2, \bm{y}) \cancel{q(\theta_1^{(t-1)} | \theta_1 ) } }{\pi(\theta_1^{(t-1)}| \theta_2^{(t-1)}, \bm{y}) \cancel{q(\theta_1 | \theta_1^{(t-1)})} } \right\} \]
		\STATE Draw $u \sim \mathcal{U}(0,1)$
		\IF{$\alpha \geq u$ }
		\STATE Accept and set \( \theta_1^{(t)} = \theta_1 \)
		\ELSE  
		\STATE Reject and keep \(\theta_1^{(t)} = \theta_1^{(t-1)} \)
		\ENDIF
		\STATE Draw \(\theta_2^{(t)} \sim  \pi( \cdot | \theta_1^{(t)} , \bm{y} )\) 
		\ENDFOR
		\STATE Output: $ \bm{\theta}^{(0)}, \dots,  \bm{\theta}^{(k)} , \dots,   \bm{\theta}^{(N)} \sim \pi(\bm{\theta}| \bm{y}) $
	\end{algorithmic}
\end{algorithm}




\subsection{Draw a sample from a multivariate normal distribution}
after sampling from $\pi(\bm{\theta}| \bm{y})$ we draw samples from $\pi(\bm{x}|\bm{\theta}, \bm{y}) $ within the MTC scheme.
For Linear Gaussian Bayesian hierarchical model we can draw a sample $\bm{x}$ from the multivariate normal distribution$\pi(\bm{x}|\bm{\theta}, \bm{y})$  using the radomize then optimise (RTO) method \cite{}.

In doing so we can rewrite the full conditional normal distribution $\pi( \bm{x}| \bm{y} , \bm{\theta} )$ to:
\begin{align}
	\pi(\bm{x}|\bm{y}, \bm{\theta} ) &\propto \pi(\bm{y} | \bm{x} , \bm{\theta} ) \pi(\bm{x}| \bm{\theta}) \\
	&= \exp  \lVert \hat{\bm{A}} \bm{x} - \hat{\bm{y}} \rVert^2 \, ,
\end{align}
where 
\begin{align}
	\label{eq:minimizer}
	\hat{\bm{A}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta})  \bm{A}\\
		\bm{Q}^{1/2}(\bm{\theta}) 
	\end{bmatrix} \, , \quad \hat{\bm{y}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta})  \bm{y} \\
		\bm{Q}^{1/2}(\bm{\theta}) \bm{\mu}
	\end{bmatrix} \, \text{\cite{}}.
\end{align}

Then one sample can be computed by minimising the following equation with respect to $\hat{\bm{x}}$ :
\begin{align}
	\bm{x}_i = \arg \min_{\hat{\bm{x}}} \lVert \hat{\bm{A}} \hat{\bm{x}} - ( \hat{\bm{y}} + \bm{b} ) \rVert^2 , \quad \bm{b} \sim \mathcal{N}(\bm{0}, \mathbf{I}) \, ,
\end{align}
where we add a randomised perturbation $\bm{b}$.
Similarly as in section \ref{} we can rewrite the argument of Eq. \ref{eq:minimizer} to 
\begin{align}
	\label{eq:RTO}
	(\bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A}+
	\bm{Q}(\bm{\theta}) ) \bm{x}_i &= \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{y} +  \bm{Q}(\bm{\theta}) \bm{\mu} + \bm{v}_1 + \bm{v}_2 \,  ,
\end{align}
where we substitute $ - \hat{\bm{A}}^T  \bm{b}  = \bm{v}_1 + \bm{v}_2$ so that $\bm{v}_1 \sim \mathcal{N}(\bm{0}, \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A}) $ and $\bm{v}_2 \sim \mathcal{N}(\bm{0}, \bm{Q}(\bm{\theta}) )$ are independent random variables \cite{}.


Within the MTC scheme ergodicity is implied if the Metropolis-within-Gibbs produces an ergodic chain $ , \bm{\theta}^{(1)}, \dots,  \bm{\theta}^{(k)} , \dots,  \bm{\theta}^{(N)}  \} \sim \pi(\bm{\theta}| \bm{y})$ then $\mathcal{M} = \{  (\bm{x}, \bm{\theta} )^{(1)}, \dots, (\bm{x}, \bm{\theta} )^{(k)} , \dots,  (\bm{x}, \bm{\theta})^{(N)}  \} \sim \pi(\bm{x},\bm{\theta}| \bm{y}) $ is ergodic as well with independent samples $\bm{x}^{(k)}$ from the full conditional $\pi(\bm{x}|\bm{\theta}, \bm{y})$ \cite{}.

\subsection{t-walk}
If there is a functional dependency of the parameters $\bm{x}$ and the hyper-parameters $\bm{\theta}$ so that $\bm{x}(\bm{\theta})$ we can use the t-walk algorithm by Christens and Fox on $\pi(\bm{\theta}|\bm{y} )$.
We use the t-walk as a black box sampler, where convergence is guaranteed by construction \cite{}.



\section{Numerical Approxiamtion Methods - Tensor Train}
Using the tensor train format to approximate a $d$-dimensional function $\pi(x)$ enables us to compute marginal posterior probability distribution cheaply.
As the name suggest the tensor train format is a train of tensors, more specifically two and three dimensional tensors which we call cores $\pi_{k} \in \mathbb{R}^{r_{k-1} \times n \times r_{k}}$  and which are connected through a ranks $r_{k}$ and $r_{k-1}$ for the $k$th dimension and defined by the number of gridpoints $n$, as in Figure \ref{} displayed.
For the first and last dimensional core the outer ranks are $r_0  = r_d = 1$.




\begin{figure}[ht!]
	\centering
\begin{tikzpicture} 
	\node[rectnode] at (-5,0) (T1)    {$1 \times n \times r_1$};
	\node[rectnode] at (-2,0) (T2)    {$r_1 \times n \times r_{2}$};
	
	\node[rectnode] at (3,0) (Tn1)    {$r_{d-2} \times n \times r_{d-1} $};
	\node[rectnode] at (6.75,0) (Tn)    {$r_{d-1} \times n \times 1$};
	\draw[-, very thick] (T1.east) -- (T2.west); 
	\draw[-, very thick] (Tn.west) -- (Tn1.east); 
	\draw[-, mydotted, very thick] (T2.east) -- (Tn1.west);
	
	\node[align=center] at (-3.5,0.25) (R) {$r_1$};
	\node[align=center] at (5,0.25) (R) {$r_{d-1}$};
	
\end{tikzpicture} 
\caption{text}
\label{key}
	
\end{figure}


\begin{figure}[ht!]
	\centering

	\caption{nice matries picture}
	\label{key}
	
\end{figure}


The approximated marginal target function
\begin{align}
	\begin{split}
	f_{X_k}(x_k) = \frac{1}{z}
	\Big|  &\left( \int_{\mathbb{R}} \pi_{1}(x_1) \uplambda_1(x_1)\text{d}x_{1} \right) \cdots \left( \int_{\mathbb{R}} \pi_{k-1}(x_{k-1}) \uplambda_{k-1}(x_{k-1}) \text{d}x_{k-1} \right) \\ & \qquad \pi_{k}(x_k)\uplambda_k(x_{k}) \\ &\quad \left( \int_{\mathbb{R}} \pi_{k+1}(x_{k+1})\uplambda_{k+1}(x_{k+1})\text{d}x_{k+1} \right) \cdots  \left( \int_{\mathbb{R}} \pi_{d}(x_d)\uplambda_d(x_{d})\text{d}x_d \right) \Big| \, ,
	\end{split} 
\end{align}
is given by integration over each core, where $k$th is in $\mathbb{R}^{r_{k-1} \times r_{k}}$ and z is some normalising constant.
Here we introduce some Lebesgue measurable weight function  $\uplambda(x) = \prod^d_{i=1} \uplambda_i(x_i)$.
Why? \cite{}.
\\


%%%%%% Squared and Basis Function %%%%%
From here the notation and procedure is taken mostly from \cite{}.
For numerical stability we can approximate the sqaure root of
\begin{align}
	\sqrt{\pi(x)} \approx \tilde{g}(x) = \bm{G}_1(x_1), \dots ,  \bm{G}_k(x_k), \dots ,  \bm{G}_d(x_d)
\end{align}
where the TT-core
\begin{equation}
	G^{(\alpha_{k-1},\alpha_k)}_k(x_k) = \sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{A}_k[\alpha_{k-1}, i, \alpha_k], \quad k = 1, ..., d,
\end{equation}
with the associated $k$th coefficient tensor $\bm{A}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ and the $k$-th  basis functions $\phi^{(i)}_k(x_k)$.

We assume the function
\begin{align}
	\pi(x) \approx \gamma^{\prime} + g^2(x) \, ,
\end{align} 
where $g(x)$ is defined through the tensor train decomposition plus an error $\gamma^{\prime}$  according to the l2 norm.
Then the normalised target function is 
\begin{align}
	f_X(x) = \frac{1}{z} \pi(x) \uplambda(x) = \frac{1}{z} ( \gamma^{\prime} \uplambda(x) + g^2(x) \uplambda(x))
\end{align} 
with a normalisation constant $z$.
Consquently the approximated marginal functions can be expressed as
\begin{align}
	\begin{split}
		f_{X_k}(x_k) = \frac{1}{z}  \Bigg( \gamma^{\prime}& \prod_{i=1}^{k-1} \uplambda_i(\mathcal{X}_i) \prod_{i=k+1}^{d} \uplambda_i(\mathcal{X}_i) \\&+  \left( \int_{\mathbb{R}} \bm{G}^2_{1}(x_1) \uplambda_1(x_1)\text{d}x_{1} \right) \cdots \left( \int_{\mathbb{R}} \bm{G}^2_{k-1}(x_{k-1}) \uplambda_{k-1}(x_{k-1}) \text{d}x_{k-1} \right) \\ & \bm{G}^2_{k}(x_k)\uplambda_k(x_{k})\\ & \left( \int_{\mathbb{R}}  \bm{G}^2_{k+1}(x_{k+1})\uplambda_{k+1}(x_{k+1})\text{d}x_{k+1} \right) \cdots  \left( \int_{\mathbb{R}} \bm{G}^2_{d}(x_d)\uplambda_d(x_{d})\text{d}x_d \right) \Bigg) \, ,
	\end{split} 
\end{align}
where $\uplambda_k( \mathcal{X}_k) = \int_{ \mathcal{X}_k} \uplambda_k (x_k) \text{d}x_k$.

To effeciently calculate these marginals on can us a procedure which is called left and right orthogonalization of cores \cite{} [32].
To do so we define the mass matrix $\bm{M}_k \in \mathbb{R}^{n_k \times n_k}$ by
\begin{equation}
	\bm{M}_k[i, j] = \int_{X_k} \phi^{(i)}_k(x_k) \phi^{(j)}_k(x_k)  \uplambda(x_k) \,dx_k, \quad i = 1, ..., n_k, \quad j = 1, ..., n_k,
\end{equation}
where $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ is the set of basis functions for the $k$-th coordinate.
%In the case of cartesian basis with $\uplambda= 1$ M is the identity matrix, which we need to compute the marginals.
%For a boundend Parameter space we consider the weihtin funcitn $\uplambda(x) = 1$




\subsection{Marginal Functions}

We calculate the marginal functions through procedures, which we call backward marginalisation \cite{} and forward marginalisation.
We gain the coefficient matrices $\bm{B}_k$ through backward marginalisation and the coefficient matrices $\bm{B}_{pre,n}$ through forward marginalisation, which enables us to calculate marginal fuunction similar to \cite{}.




The proposition \ref{prob:backMarg} to caculte $\bm{B}_k$  is taken from \cite{}.

\begin{prop}[Backward Marginalisation]
	Starting with the last coordinate $k = d$, we set $\bm{B}_d = \bm{A}_d$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{k-1} \in \mathbb{R}^{r_{k-2} \times n_{k-1} \times r_{k-1}}$, which we need for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_k[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{B}_k[\alpha_{k-1}, i, l_k] \bm{L}_k[i, \tau].
		\end{equation}
		\item Unfold $\bm{C}_k$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_k^{(R)} \in \mathbb{R}^{r_{k-1} \times (n_k r_k)}$:
		\begin{equation}
			\bm{Q}_k \bm{R}_k = {(\bm{C}_k^{(R)})}^{\top}.
		\end{equation}
		\item Compute the new coefficient tensor:
		\begin{equation}
			\bm{B}_{k-1}[\alpha_{k-2}, i, l_{k-1}] = \sum_{\alpha_{k-1}=1}^{r_{k-1}} \bm{A}_{k-1}[\alpha_{k-2}, i, \alpha_{k-1}] \bm{R}_k[l_{k-1}, \alpha_{k-1}].
		\end{equation}
	\end{enumerate}
\label{prob:backMarg}
\end{prop}

Then we need to do this the other way as well.


In addiotn we also have to  do forward marginalistion starig with the first dimension
\begin{prop}[Forward Marginalistaion]
	Starting with the first coordinate $k = 1$, we set $\bm{B}_{pre,1} = \bm{A}_1$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{pre,k+1} \in \mathbb{R}^{r_{k} \times n_{k+1} \times r_{k+1}}$ for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_{pre,k}[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{L}_k[i, \tau] \bm{B}_{pre,k}[\alpha_{k-1}, i, l_k] .
		\end{equation}
		\item Unfold $\bm{C}_{pre,k}$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_{pre,k}^{(R)} \in \mathbb{R}^{(r_{k-1} n_k ) \times r_k}$:
		\begin{equation}
			\bm{Q}_{pre,k}\bm{R}_{pre,k} = {(\bm{C}_{pre,k}^{(R)})}.
		\end{equation}
		\item Compute the new coefficient tensor $\bm{B}_{pre, k+1} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k} $:
		\begin{equation}
			\bm{B}_{pre, k+1}[l_{k+1}, i, \alpha_{k+1}] = \sum_{\alpha_{k}=1}^{r_{k}} \bm{R}_{pre,k}[l_{k+1}, \alpha_{k}] \bm{A}_{k+1}[\alpha_{k}, i, \alpha_{k+1}] .
		\end{equation}
	\end{enumerate}
\end{prop}



The marginal PDF of $X_{k}$ can be expressed as
\begin{equation}
	f_{X_k}(x_k) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=1}^{k-1} \lambda_i(X_i) \prod_{i=k+1}^{d} \lambda_i(X_i) + \sum_{l_{k-1}=1}^{r_{k-1}} \sum_{l_k=1}^{r_k} \left(\sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{D}_k[l_{k-1},i, l_k] \right)^2 \right) \lambda_k(x_k),
\end{equation}
where $\bm{D}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ and $\bm{R}_{pre,k-1}\in \mathbb{R}^{r_{k-1} \times r_{k-1}}$ and $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$
\begin{equation}
	\bm{D}_k[l_{k-1},i,l_k] = \sum_{\alpha_{k-1}=1}^{r_{k-1}}  \bm{R}_{pre,k-1}[l_{k-1}, \alpha_{k-1}] \bm{B}_k[\alpha_{k-1}, i, l_k].
\end{equation}


Special Cases
The marginal PDF of $X_1$ can be expressed as
\begin{equation}
	f_{X_1}(x_1) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=2}^{d} \uplambda_i(\mathcal{X}_i) + \sum_{l_1=1}^{r_1} \left(\sum_{i=1}^{n_1} \phi^{(i)}_1(x_1) \bm{D}_1[i, l_1] \right)^2 \right) \uplambda_1(x_1),
\end{equation}
where $\bm{D}_1[i, l_1] = \bm{B}_1[\alpha_0, i, l_1]$ and $\alpha_0 = 1$.\\
The marginal PDF of $X_n$ can be expressed as
\begin{equation}
	f_{X_n}(x_n) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=1}^{d-1} \uplambda_i(\mathcal{X}_i) + \sum_{l_{n-1}=1}^{r_{n-1}} \left(\sum_{i=1}^{n_1} \phi^{(i)}_1(x_1) \bm{D}_n[l_{n-1},i] \right)^2 \right) \uplambda_n(x_n),
\end{equation}
where $\bm{D}_n[l_{n-1},i] = \bm{B}_{pre,n}[l_{n-1}, i, \alpha_n]$ and $\alpha_n = 1$.
