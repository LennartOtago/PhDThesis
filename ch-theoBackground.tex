\chapter{Theoretical Background}

\section{Forward Model}

\begin{figure}[ht!]
	\centering
	\scalebox{0.9}{\input{FirstLIMB.pdf_tex}}
	\label{fig:FirstLIMB}
\end{figure}

In this section we describe the forward model which we use to simulate data and base the Bayesian inference on.

As shown in Figure~\ref{fig:forModel}, one measurement of a stationary satellite can be describes as the path integral along the line of sight $\Gamma_j$ for $j=1,2,\ldots,m$.
For each measurement we can define a tangent height $h_{\ell_j}$ as the shortest distance along the line of sight to the earth.
%and call that the limb height.

%A stationary satellite takes $m$ measurements in pointing directions that define the lines of sight from the satellite $\Gamma_j$ for $j=1,2,\ldots,m$. Each line of sight $\Gamma_j$ is tangent to a sphere around Earth called a limb, and the tangent height $\ell_j$ is called the limb height. Hence the limb height $\ell_j$ is the lowest height to Earth along the line-of-sight $\Gamma_j$.

The $j^\text{th}$ measurement, taken on line of sight $\Gamma_j$  is modelled by the the radiative transfer equation (RTE)~\cite{united2006handbook}
% \begin{align}
	%    y_j =   \int_{\Gamma_j} \underbrace{  S(\nu, T)   \frac{p(T)}{k_{\text{B}} T(r)} B(\nu,T)}_{ \bm{A_{j}} }  x(r) \tau(r) \text{d}r \, 
	%    \label{eq:RTE}
	% \end{align}
\begin{align}
	y_j =   \int_{\Gamma_j}  B(\nu,T) k(\nu, T)   \frac{\bm{p}(T)}{k_{\text{B}} \bm{T}(r)}  \bm{x}(r)  \tau(r) \text{d}r + \eta_j \, \\
	\tau(r) = \exp{ \Bigl\{ - \int^{r}_{r_\text{obs}}  k(\nu, T)   \frac{\bm{p}(T)}{k_B \bm{T}(r^{\prime})}  \bm{x}(r^{\prime}) \text{d}r^{\prime} \Bigr\} }
	\label{eq:RTE}
\end{align}
where the path from the satellite along the line-of-sight of the $j^\text{th}$ pointing direction is $\Gamma_j$ and the ozone concentration$\bm{x}(r)$ at distance $r$ from the radiometer.
The factor $\tau(r)\leq 1$ accounts for re-absorption of the radiation along the line-of-sight, which makes the RTE non linear.
The noise $\eta_j$ is added to each path integral, where the noise vector $ \bm{\eta} \sim \mathcal{N}(0, \gamma^{-1} \mathbf{I} )$ is normally distributed around zero with the noise precision $\gamma$.
The absorption constant $k(\nu, T)$ for a single gas molecule at a specific wavenumber $\nu$ is given by the HITRAN database \cite{gordon2022hitran2020} and acts as a source function when multiplied with the black body radiation $B(\nu,T)$, given by Planck`s law.
Within the stratosphere the number density $p(T) / (k_{\text{B}} T(r))$ of molecules is dependent on the pressure $p(T)$, the temperature $T(r)$, and the Boltzmann constant $k_{\text{B}}$.
For fundamentals on the Radiative transfer equation we recommend 79BOOKRadiativeProcess.

%The path integral over the line $\Gamma_j$ from the satellite to the end of the atmosphere defines the kernel $\bm{A_{j}}$ that is independent of ozone concentration $x$. 
We parametrize the ozone profile as a function of height, discretized into the $n$ values in each of $n$ layers of the discretized stratosphere where the $i^\text{th}$ layer is defined by two spheres of radii  $h_{i-1} < h_{i}$, $i = 1, \dots, n$, with $h_0$ and $h_{n} $.
In between the heights $h_{i-1}$ and $h_{i}$, each of the ozone concentration $x_{i}$, the pressure $p_{i}$, the temperature $T_{i}$, and thermal radiation is assumed to be constant.
Above $h_{n}$ and below $h_0$, the ozone concentration is set to zero, so no signal can be obtained.
Then depending on the parameter of interest, which is either the ozone volume mixing ratio $\bm{x} =\{x_1,x_2,\ldots,x_n\} \in \mathbb{R}^{n}$ or the fraction of pressure and temperature $\bm{p/T}= \{p_1/T_1,p_2/T_2,\ldots,p_n/T_n\} \in \mathbb{R}^{n} $, we can rewrite the integral in Eq.~\eqref{eq:RTE} as e.g. $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} $, where the absorption $\tau(r)$ induces non-linearity.
Here, the row vector $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{n}$  defines a Kernel for each measurement so that the data vector
\begin{align}
	\bm{y} = \bm{A}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} + \bm{\eta}= \bm{A}(\bm{x},  \bm{p},\bm{T}) \,
	\frac{ \bm{p}}{\bm{T}} + \bm{\eta} \, .
\end{align}
can be written as a matrix vector multiplication, where the matrix $\bm{A}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{m \times n}$  and the noise vector $\bm{\eta} \in \mathbb{R}^{m}$.

Since the absorption $\tau(r)$ reduces measurements by of order $1\%$, or less, making the inverse problem only weakly non-linear. 
We use that to approximate the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T})$ with a map $\bm{M}$ so that $\bm{A}(\bm{x},  \bm{p},\bm{T}) \approx \bm{M} \bm{A}_L $
Where each row $\bm{A}_{L,j} $ of matrix as $\bm{A}_L \in \mathbb{R}^{m \times n}$ is defined by the linear forward model, where absorption is neglected, e.g. $\tau = 1$. 
Then $\bm{A}_{L,j} $ is either defined by $ B(\nu,T) S(\nu, T)   \frac{\bm{p}(T)}{k_{\text{B}} \bm{T}(r)}  \text{d}r$ or $B(\nu,T) S(\nu, T)   \frac{\bm{x}}{k_{\text{B}}}  \text{d}r$, as in Eq..~\eqref{eq:RTE}, depending on the parameter of interest.
This poses a linear inverse problem with the forward map defined by the matrix $\bm{A} = \bm{M} \bm{A}_L$, where $\bm{M}$ is, more specifically, an affine map.


\section{Affine Map}

To approximate the non-linear forward model we use an affine map $ M:\bm{A}_L \bm{x} \rightarrow \bm{A}(\bm{x},  \bm{p},\bm{T})) \bm{x}$, which maps the linear forward model $\bm{A}_L \bm{x}$ onto the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}$.


An affine map is any linear map in between two vector spaces is or affine spaces, where in affine space does not need to have a zero origin. 2.3.1. PROPOSITION AND DEFINITIOn Berge book\cite{}.
In other words an affine map does not need to preserve the origin, or is a linear map on vector spaces including translation, or in the words of my supervisor, C. F., an affine map is a Taylor series of first order.
For more information on affine spaces and maps we refer to \cite{two books}

We generate two affine subspaces spaces \newline $V = \big\{ \bm{A}(\bm{x}^{(1)}, \bm{p,T}), \dots ,\bm{A}(\bm{x}^{(m)}, \bm{p,T})\big\} $ and $W = \big\{ \bm{A}\bm{x}^{(1)}, \dots ,\bm{A}\bm{x}^{(m)}\big\}$ over the same field, with fixed $\bm{p,T}$.
The parameter $\bm{x}$ is distributed as the so-called posterior distribution $\big\{  \bm{x}^{(1)} , \dots, \bm{x}^{(m)} \big\} \sim \pi(\bm{x}|\bm{\theta},\bm{y})$, with hyper-parameters $\bm{\theta}$, according to a Bayesian hierarchical model.



\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[rectnode] at (2,-4) (NL)    {$W$};
		\node[rectnode] at (-2,-4) (L)    {$V$};
		\draw[<-, very thick] (NL.west) -- (L.east); 
		\node[align=center] at (-5.5,-4) (f3) {linear forward model};
		\node[align=center] at (5.5,-4) (f4) {non-linear forward model};
		\node[align=center] at (0,-5) (f5) {$\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}  \approx \bm{M A}_L  \bm{x} = \bm{A x} $ };
		\node[align=center] at (0,-4) (f5) {affine Map \\ $\bm{M}$};
	\end{tikzpicture}
	\caption[Schematics of Affine Map]{Schematics of Affine Map, which approximates the linear forward model to the non-linear forward model.}
\end{figure}

\section{Bayesian Inference}
Given some data $\bm{y}$, see Eq. \ref{}, from the Satellite limb sounder we like to use Bayesian inference to recover the parameter of iterest $\bm{x}$.
In doing so we set up a hierarchially order Bayesian model which naturally inlcudes hyperparameters $\bm{\theta}$, such as the radnom noise added to some obersovation from the space of all measurables $\bm{u}$.
This space is deterministicaly defined through the linear forward model $\bm{A}$.
We can visualize this hierarchial model through a directed acyclic graph (DAG), see Figure \ref{fig:FirstDAG}.

\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[roundnode2] at (0,4) (th)    {$\bm{\theta}$};
		\node[roundnode2] at (0,2) (x)    {$\bm{x}$};
		\node[roundnode2] at (0,0) (A)    {$\bm{A}$};
		\node[roundnode2] at (0,-2) (u)    {$\bm{u}$};
		\node[rectnode] at (0,-4) (y)    {$\bm{y}$};

        \draw[->, very thick] (th.south) -- (x.north); 
        \draw[->, very thick] (x.south) -- (A.north); 
        \draw[->, very thick] (A.south) -- (u.north); 
        \draw[->, very thick, mydotted] (u.south) -- (y.north); 
        \draw[->, very thick, mydotted] (th) edge[bend right=60] (y);  
  
        \node[align=center] at (3,4) (tht) {hyperparameters};
        \node[align=center] at (3,2) (xt) {parameters};
        \node[align=center] at (3,0) (At) {forward model};
        \node[align=center] at (3,-2) (ut) {space of all measurables};
        \node[align=center] at (3,-4) (yt) {data};
        \node[align=center] at (-3,0) (nt) {noise};
	\end{tikzpicture}
	\caption[Bayesian Inference DAG]{The directed acyclic graph (DAG) for a typical linear inverse problem shows deterministic dependecys as solid line arrows and statistical dependencies as dotted arrows.
	Naturally the data $\bm{y}$ has some noise described through some hyperparameters $\bm{\theta}$ which determine the parameters $\bm{x}$. Those determine the space of all measurables $\bm{u}$ according to the forward model matrix $A$, so that $\bm{Ax}$ is a linear operation.
	From the space of all measurables we can observe some data $\bm{y}$, statistically, where as prevoiusly mentioned some random noise is added.
	We set up a more sophisticated Bayesian model in chapter \ref{} explicity including all hyper-parameters and parameters of interest.}
	\label{fig:FirstDAG}
\end{figure}

Within a linear Bayesian hierarchial model we need to define distribution over the unknown parameters and hyper-parameters
\begin{subequations}
	\begin{align}
		\bm{y}|\bm{x}, \bm{\theta}&\sim \mathcal{N}(\bm{A} \bm{x}, \bm{\Sigma}) \label{eq:likelihood}  \\
		\bm{x}| \bm{\theta} & \sim  \mathcal{N}( \bm{\mu}, \bm{Q}^{-1}  ) \label{eq:xPrior} \\
		\bm{\theta} &\sim  \pi(\bm{\theta}) \label{eq:gammaPrior}\, ,
	\end{align}
	\label{eq:BayMode}
\end{subequations}
with the noise covarince matrix $\bm{\Sigma}$, so that $\bm{\eta}  \sim \mathcal{N}(0, \bm{\Sigma}) $ as in Eq. \ref{}, the prior precision matrix $\bm{Q}$, prior mean $\bm{\mu}$
and some distribtion over the hyperparameters $\pi(\bm{\theta})$.
Then this becomes a linear-Gaussian Bayesian hierachial model.

According to Bayes' theorem we focus on the posterior distribution
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) = \frac{ \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta})}{\pi(\bm{y})} \, ,
\end{align}
with the likelihood function $\pi(\bm{y} | \bm{x}, \bm{\theta} )$ and the prior distribution $\pi(\bm{x}, \bm{\theta})$.
If the normalizing contant $\pi(\bm{y})$ is fintie and non-zero we can approximate the posterior distribution
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) \propto \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta}) \, .
\end{align}

Then the expecation of any a function $h(\bm{x})$ can be described as 
\begin{align}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})] =  \int \int   h(\bm{x}) \,  \pi(\bm{x}, \bm{\theta} | \bm{y} ) \, \text{d} \bm{x}  \, \text{d} \bm{\theta}   \label{eq:expPos} \, ,
\end{align}
which is usually a high dimensional integral and computationally not feasible to solve.

One way to work around the high dimensionality is to parameterize $\bm{x}$ using hyperparmaters $\bm{\theta}$ so that $\bm{x}(\bm{\theta})$. 
Another way is to seperate the posterior distribution over latent field $\bm{x}$ and the hyperparameters $\bm{\theta}$.
This is particular benefitial, when $\bm{x}$ is high dimesnional, e.g. $\bm{x} \in \mathbb{R}^n$ with $n = 45$, and can not be parameterized, and $\bm{\theta}$ is low dimensional, e.g. two dimensional.


\subsection{Marginal and then Conditional}
The marginal and then conditional (MTC) method factorizes the full posterior distribution 
\begin{align}
	\pi(\bm{x}, \bm{\theta}|\bm{y}) = \pi(\bm{x}| \bm{\theta}, \bm{y}) \pi(\bm{\theta}|\bm{y})
\end{align}
into the marginal posterior distribtion $ \pi(\bm{\theta}|\bm{y})$ and conditional posterior distribution $\pi(\bm{x}| \bm{\theta}, \bm{y})$.


For the in Eq. \ref{} specified linear-Gaussian Bayesian hierarchial model the marginal posterior distribution is given as
\begin{align}
	\pi(\bm{\theta} | \bm{y}) &= \int \pi(\bm{x}, \bm{\theta} | \bm{y}) \diff \bm{x} \\ 
	\label{eq:condHyper}
	&\propto \sqrt{ \frac{ \det( \bm{\Sigma}^{-1} ) \,  \det( \bm{Q}) }{\det( \bm{Q} + \bm{A}^T \bm{\Sigma}^{-1} \bm{A} ) } } \times \exp \Big[ - \frac{1}{2}(\bm{y} -\bm{A \mu})^T \bm{Q}_{\bm{\theta|y}} (\bm{y} -\bm{A \mu}) \Big] \pi(\bm{\theta}) \, ,
\end{align}
with
\begin{align}
	\bm{Q}_{\bm{\theta|y}} = \bm{\Sigma}^{-1} - \bm{\Sigma}^{-1} \bm{A} (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \bm{A}^T \bm{\Sigma}^{-1} \,  .
\end{align}
See lemma \cite{}.

Then conditioned on the hyperparameters $\bm{\theta}$ we can draw samples of the conditional posterior distribution
\begin{align}
	\bm{x}| \bm{y} , \bm{\theta} \sim \mathcal{N}\big( \bm{\mu} + (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \bm{A}^T \bm{\Sigma}^{-1} (\bm{y} - \bm{A} \bm{\mu}), (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \big) \, ,
\end{align}
see section \ref{} or caculate weighted expections of a function $h(\bm{x})$
\begin{align}
	\label{eq:lte}
	\text{E}_{\bm{x}|\bm{y}} [h(\bm{x})] = \int   \text{E}_{\bm{x}|\bm{\theta},\bm{y}} [h(\bm{x})] \, \pi(\bm{\theta} | \bm{y} )  \, \text{d} \bm{\theta} \,  ,
\end{align}
with weights given by $\pi(\bm{\theta} | \bm{y} )$.
\cite{}

In this thesis we will use sampling and deterministic methods to characterise the posterior distribtion over the hyperparameters and present the basics of those in the following sections.


\section{Sampling Methods -- MCMC}
The aim of sampling methods is to draw enough samples from the distribution of interest so that
the distrnution can characterised with sample based methods
Here we present the sampling methods/algorithms we are using.
Generating a Markov-Chain $ (\bm{x}, \bm{\theta} )^{(0)}, \dots, (\bm{x}, \bm{\theta} )^{(k)} , \dots,  (\bm{x}, \bm{\theta})^{(N)} \sim \pi(\bm{x},\bm{\theta}| \bm{y}) $ we accept and reject proposed samples to accurately calculate the sample mean.
Ergodicity

\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})] \approx \frac{1}{N} \sum_{k=1}^{N} h (\bm{x}^{(k)}) \, ,
\end{align}

\subsection{Metropolis}
Metropolis algorithm is special case of the Metropolis-Hastings algorithm, with a symmetric proposal distribution $q(\cdot|\cdot)$ \cite{}.

\begin{align}
\alpha(\bm{\theta}^{(t)}  | \bm{\theta}^{(t-1)}) = \min \left\{ 1, \frac{\pi(\bm{\theta}^{(t)}  | \bm{y}) \cdot 	q(\bm{\theta}^{(t-1)} | \bm{\theta})}{\pi(\bm{\theta}^{(t-1)}|  \bm{y}) \cdot q(\bm{\theta} | \bm{\theta}^{(t-1)})} \right\}= \min \left\{ 1, \frac{\pi(\bm{\theta}^{(t)}  | \bm{y}) }{\pi(\bm{\theta}^{(t-1)}| \bm{y})} \right\}
\end{align}

\begin{algorithm}
	\caption{Metropolis}
	\begin{algorithmic}[1]
		\STATE Initialize \( \bm{\theta}^{(0)} \).
		\FOR{ \( k = 1, \dots, N \)}
		\STATE Propose \( \bm{\theta}^{(t)} \sim \mathcal{N}(\bm{\theta} | \bm{\theta}^{(t-1)}) \)
		\STATE Compute
			\[ \alpha(\bm{\theta}^{(t)} | \bm{\theta}^{(t-1)}) = \min \left\{ 1, \frac{\pi(\bm{\theta}^{(t)}  | \bm{y}) }{\pi(\bm{\theta}^{(t-1)}| \bm{y}) } \right\} \]
		\STATE Accept and set \( \bm{\theta}^{(t)} = \bm{\theta} \) with probability \( \alpha(\bm{\theta}^{(t)}  | \bm{\theta}^{(t-1)}) \), otherwise set \(\bm{\theta}^{(t)} = \bm{\theta}^{(t-1)} \).
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\subsection{Gibbs}
\begin{algorithm}
	\caption{Gibbs}
	\begin{algorithmic}[1]
		\STATE Initialize \( \bm{\theta}^{(0)} = \{\bm{\theta}^{(0)}_{1}, \dots, \bm{\theta}^{(0)}_{j},\dots,\bm{\theta}^{(0)}_{d} \} \).
		\FOR{ \( k = 1, \dots, N \)}
		\FOR{ \( j = 1, \dots, d \)}
		\STATE Draw \(\bm{\theta}_j^{(t)} \sim  \pi(\bm{\theta}_j | \bm{\theta}_{<j}, \bm{\theta}_{>j} , \bm{y} )\) 

		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

$ \bm{\theta}_{<j} = \{\bm{\theta}_{1}, \dots, \bm{\theta}_{j-1} \} $ 
$ \bm{\theta}_{>j} = \{\bm{\theta}_{j+1}, \dots, \bm{\theta}_{d}\} $which denotes all dimesnions except $ \bm{\theta}_{j}$
\subsection{t-walk}
We use the t-walk sampler as a black box sampling algorithm
\cite{}, so it prodices a ergodic markov chain


\subsection{Draw a sample from the Conditional posterior distribution -- RTO}
In the case of marginalising out the latent field we use a smaple of choice to genrate a amrkov chain.
Then we can draw a sample from that markov chain and condition on it so that we can draw a sample from the 
As the full conditional distribution for $\bm{x}| \bm{y} , \bm{\theta} $ is a normal distribution we can rewrite to:
\begin{align}
	\pi(\bm{x}|\bm{y}, \bm{\theta} ) &\propto \pi(\bm{y} | \bm{x} , \bm{\theta} ) \pi(\bm{x}| \bm{\theta}) \\
	&= \exp  \lVert \hat{\bm{A}} \bm{x} - \hat{\bm{y}} \rVert^2 \, ,
\end{align}
where 
\begin{align}
	\label{eq:minimizer}
	\hat{\bm{A}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta})  \bm{A}\\
		\bm{Q}^{1/2}(\bm{\theta}) 
	\end{bmatrix} \, , \quad \hat{\bm{y}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta})  \bm{y} \\
		\bm{Q}^{1/2}(\bm{\theta}) \bm{\mu}
	\end{bmatrix} \, .
\end{align}
One sample from the posterior can be computed by minimizing the following equation with respect to $\hat{\bm{x}}$ :
\begin{align}
	\bm{x}_i = \arg \min_{\hat{\bm{x}}} \lVert \hat{\bm{A}} \hat{\bm{x}} - ( \hat{\bm{y}} + \bm{\eta} ) \rVert^2 , \quad \bm{\eta} \sim \mathcal{N}(\bm{0}, \mathbf{I}) \, ,
\end{align}
where we add a randomized perturbation $\bm{\eta}$.
Next, we substitute $ - \hat{\bm{A}}^T  \bm{\eta}  = \bm{v}_1 + \bm{v}_2$ we can rewrite the argument of Eq. \ref{eq:minimizer} to 
\begin{align}
	\label{eq:RTO}
	(\bm{A}^T \bm{\Sigma}^{-1} \bm{A}+
	\bm{Q} ) \bm{x}_i &= \bm{A}^T \bm{\Sigma}^{-1} \bm{y} +  \bm{Q} \bm{\mu} + \bm{v}_1 + \bm{v}_2 \,  ,
\end{align}
where $\bm{v}_1 \sim \mathcal{N}(\bm{0}, \bm{A}^T \bm{\Sigma}^{-1} \bm{A}) $ and $\bm{v}_2 \sim \mathcal{N}(\bm{0}, \bm{Q} )$ are independent random variables.
Finally, we can draw an independent sample from the posterior $(\bm{x}_i, \bm{\theta}_i) \sim \pi(\bm{x}, \bm{\theta} | \bm{y})$.


\section{Numerical Approxiamtion Methods}
To approximate multi dimesnioan functions we can use the tensor train format to approximate marginal funcitons



\subsection{Tensor Train}

One of the main contributions of this paper is to show that the conditional distribution method is feasible, and efficient, once a PDF has been put into TT format. This section presents those calculations.

First, we describe the computation of the marginal PDFs \( p_k \), defined in (2), given \( \pi \) in a TT format (3). Note that integrals over the variable \( x_k \) appear in all conditionals (2) with \( k < d \). The TT format allows computing the \( r_{k-1} \times 1 \) vector \( P_k \) required for evaluating the marginal PDF \( p_{k-1} \) by the following algorithm:

\begin{algorithm}
	\caption{Computation of marginal PDFs}
	\begin{algorithmic}[1]
		\STATE Initialize \( P_{d+1} = 1 \).
		\FOR{ \( k = d, d-1, \dots, 2 \)}
		\STATE Compute
		\[
		(P_k)_{\alpha_{k-1}} =
		\sum_{\alpha_k=1}^{r_k} \left( \int_{\mathbb{R}} \pi^{(k)}_{\alpha_{k-1},\alpha_k} (x_k) \, dx_k \right) (P_{k+1})_{\alpha_k}
		\]
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

Since \( \pi^{(k)}(x_k) \in \mathbb{R}^{r_{k-1} \times r_k} \) for each fixed \( x_k \), the integral \( \int \pi^{(k)}(x_k)dx_k \) is a \( r_{k-1} \times r_k \) matrix, where \( \alpha_{k-1} \) is the row index and \( \alpha_k \) is the column index. Hence, we can write Line 3 as the matrix–vector product:

\[
P_k = \left( \int_{\mathbb{R}} \pi^{(k)}(x_k)dx_k \right) P_{k+1}.
\]

Assuming \( n \) quadrature points for each \( x_k \), and the uniform rank bound \( r_k \leq r \), the asymptotic complexity of this algorithm is \( O(dnr^2) \).

The first marginal PDF is approximated by \( p^*_1(x_1) = |\pi^{(1)}(x_1) P_2| \). We take the absolute value because the TT approximation \( \pi^* \) (and hence, \( \pi^{(1)}(x_1) P_2 \)) may be negative at some locations. In the \( k \)-th step of the sampling procedure, the marginal PDF also requires the first \( k-1 \) TT blocks, restricted to the components of the sample that are already determined:

\[
p^*_k(x_1, \dots, x_{k-1}|x_k) = \left| \pi^{(1)}(x_1) \cdots \pi^{(k-1)}(x_{k-1}) \pi^{(k)}(x_k) P_{k+1} \right|.
\]

However, since the loop goes sequentially from \( k = 1 \) to \( k = d \), the sampled TT blocks can be accumulated in the same fashion as the integrals \( P_k \). Again, we take the absolute value to ensure positivity.

The \textbf{full} marginals are then defined as:
\[
p^*_k(x_k) = \left|  \left( \int_{\mathbb{R}} \pi^{(1)}(x_1)dx_{1} \right) \cdots \left( \int_{\mathbb{R}} \pi^{(k-1)}(x_{k-1})dx_{k-1} \right)  \pi^{(k)}(x_k) \left( \int_{\mathbb{R}} \pi^{(k+1)}(x_{k+1})dx_{k+1} \right) \cdots  \left( \int_{\mathbb{R}} \pi^{(d)}(x_d)dx_{d} \right) \right|.
\]

\section{SIRT - Marginal Functions and Conditional PDFs}
We represent each TT core of the decomposition in (18) as
\begin{equation}
	G^{(\alpha_{k-1},\alpha_k)}_k(x_k) = \sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{A}_k[\alpha_{k-1}, i, \alpha_k], \quad k = 1, ..., d,
\end{equation}
where $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ is the set of basis functions for the $k$-th coordinate and $\bm{A}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ is the associated $k$-th coefficient tensor. For the $k$-th set of basis functions, we define the mass matrix $\bm{M}_k \in \mathbb{R}^{n_k \times n_k}$ by
\begin{equation}
	\bm{M}_k[i, j] = \int_{X_k} \phi^{(i)}_k(x_k) \phi^{(j)}_k(x_k) \lambda(x_k) \,dx_k, \quad i = 1, ..., n_k, \quad j = 1, ..., n_k.
\end{equation}


\section{right to left}

\begin{prop}
	Starting with the last coordinate $k = d$, we set $\bm{B}_d = \bm{A}_d$. Suppose for the first $k$ dimensions ($d > k\geq  1$), we have a coefficient tensor $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ that defines a marginal function $\pi_{\leq k}(x_{\leq k})$ as in (??). The following procedure \textbf{can} be used to obtain the coefficient tensor $\bm{B}_{k-1} \in \mathbb{R}^{r_{k-2} \times n_{k-1} \times r_{k-1}}$ for defining the next marginal function $\pi_{<k}(x_{<k})$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_k[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{B}_k[\alpha_{k-1}, i, l_k] \bm{L}_k[i, \tau].
		\end{equation}
		\item Unfold $\bm{C}_k$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_k^{(R)} \in \mathbb{R}^{r_{k-1} \times (n_k r_k)}$:
		\begin{equation}
			\bm{Q}_k \bm{R}_k = {(\bm{C}_k^{(R)})}^{\top}.
		\end{equation}
		\item Compute the new coefficient tensor:
		\begin{equation}
			\bm{B}_{k-1}[\alpha_{k-2}, i, l_{k-1}] = \sum_{\alpha_{k-1}=1}^{r_{k-1}} \bm{A}_{k-1}[\alpha_{k-2}, i, \alpha_{k-1}] \bm{R}_k[l_{k-1}, \alpha_{k-1}].
		\end{equation}
	\end{enumerate}
\end{prop}





\section{left to right}

\begin{prop}
	Starting with the first coordinate $k = 1$, we set $\bm{B}_{pre,1} = \bm{A}_1$. Suppose for the last $k$ dimensions ($k < d$), we have a coefficient tensor $\bm{B}_{pre,k} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ that defines a marginal function $\pi_{\leq k}(x_{\leq k})$ as in (??). The following procedure \textbf{can} be used to obtain the coefficient tensor $\bm{B}_{pre,k+1} \in \mathbb{R}^{r_{k} \times n_{k+1} \times r_{k+1}}$ for defining the next marginal function $\pi_{>k}(x_{>k})$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_{pre,k}[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{L}_k[i, \tau] \bm{B}_{pre,k}[\alpha_{k-1}, i, l_k] .
		\end{equation}
		\item Unfold $\bm{C}_{pre,k}$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_{pre,k}^{(R)} \in \mathbb{R}^{(r_{k-1} n_k ) \times r_k}$:
		\begin{equation}
			\bm{Q}_{pre,k}\bm{R}_{pre,k} = {(\bm{C}_{pre,k}^{(R)})}.
		\end{equation}
		\item Compute the new coefficient tensor $\bm{B}_{pre, k+1} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k} $:
		\begin{equation}
			\bm{B}_{pre, k+1}[l_{k+1}, i, \alpha_{k+1}] = \sum_{\alpha_{k}=1}^{r_{k}} \bm{R}_{pre,k}[l_{k+1}, \alpha_{k}] \bm{A}_{k+1}[\alpha_{k}, i, \alpha_{k+1}] .
		\end{equation}
	\end{enumerate}
\end{prop}
\section{Calc Marginals}
\begin{prop}
	The marginal PDF of $X_1$ can be expressed as
	\begin{equation}
		f_{X_1}(x_1) = \frac{1}{z} \left(\gamma \prod_{i=2}^{d} \lambda_i(X_i) + \sum_{l_1=1}^{r_1} \left(\sum_{i=1}^{n_1} \phi^{(i)}_1(x_1) \bm{D}_1[i, l_1] \right)^2 \right) \lambda_1(x_1),
	\end{equation}
	where $\bm{D}_1[i, l_1] = \bm{B}_1[\alpha_0, i, l_1]$ and $\alpha_0 = 1$.\\
	The marginal PDF of $X_n$ can be expressed as
	\begin{equation}
		f_{X_n}(x_n) = \frac{1}{z} \left(\gamma \prod_{i=1}^{d-1} \lambda_i(X_i) + \sum_{l_{n-1}=1}^{r_{n-1}} \left(\sum_{i=1}^{n_1} \phi^{(i)}_1(x_1) \bm{D}_n[l_{n-1},i] \right)^2 \right) \lambda_n(x_n),
	\end{equation}
	where $\bm{D}_n[l_{n-1},i] = \bm{B}_{pre,n}[l_{n-1}, i, \alpha_n]$ and $\alpha_n = 1$.
	
	The marginal PDF of $X_{k}$ can be expressed as
	\begin{equation}
		f_{X_k}(x_k) = \frac{1}{\pi_{<k}(x_{<k}) \pi_{>k}(x_{>k})} \left(\gamma \prod_{i=1}^{k-1} \lambda_i(X_i) \prod_{i=k+1}^{d} \lambda_i(X_i) + \sum_{l_{k-1}=1}^{r_{k-1}} \sum_{l_k=1}^{r_k} \left(\sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{D}_k[l_{k-1},i, l_k] \right)^2 \right) \lambda_k(x_k),
	\end{equation}
	where $\bm{D}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ and $\bm{R}_{pre,k-1}\in \mathbb{R}^{r_{k-1} \times r_{k-1}}$ and $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$
	\begin{equation}
		\bm{D}_k[l_{k-1},i,l_k] = \sum_{\alpha_{k-1}=1}^{r_{k-1}}  \bm{R}_{pre,k-1}[l_{k-1}, \alpha_{k-1}] \bm{B}_k[\alpha_{k-1}, i, l_k].
	\end{equation}
\end{prop}


