\chapter{Theoretical and Technical Background}

\section{Forward Model}

\begin{figure}[ht!]
	\centering
	\scalebox{0.9}{\input{FirstLIMB.pdf_tex}}
	\label{fig:FirstLIMB}
\end{figure}

In this section we describe the forward model which we use to simulate data and base the Bayesian inference on.

As shown in Figure~\ref{fig:forModel}, one measurement of a stationary satellite can be describes as the path integral along the line of sight $\Gamma_j$ for $j=1,2,\ldots,m$.
For each measurement we can define a tangent height $h_{\ell_j}$ as the shortest distance along the line of sight to the earth.
%and call that the limb height.

%A stationary satellite takes $m$ measurements in pointing directions that define the lines of sight from the satellite $\Gamma_j$ for $j=1,2,\ldots,m$. Each line of sight $\Gamma_j$ is tangent to a sphere around Earth called a limb, and the tangent height $\ell_j$ is called the limb height. Hence the limb height $\ell_j$ is the lowest height to Earth along the line-of-sight $\Gamma_j$.

The $j^\text{th}$ measurement, taken on line of sight $\Gamma_j$  is modelled by the the radiative transfer equation (RTE)~\cite{united2006handbook}
% \begin{align}
	%    y_j =   \int_{\Gamma_j} \underbrace{  S(\nu, T)   \frac{p(T)}{k_{\text{B}} T(r)} B(\nu,T)}_{ \bm{A_{j}} }  x(r) \tau(r) \text{d}r \, 
	%    \label{eq:RTE}
	% \end{align}
\begin{align}
	y_j =   \int_{\Gamma_j}  B(\nu,T) k(\nu, T)   \frac{\bm{p}(T)}{k_{\text{B}} \bm{T}(r)}  \bm{x}(r)  \tau(r) \text{d}r + \eta_j \, \\
	\tau(r) = \exp{ \Bigl\{ - \int^{r}_{r_\text{obs}}  k(\nu, T)   \frac{\bm{p}(T)}{k_B \bm{T}(r^{\prime})}  \bm{x}(r^{\prime}) \text{d}r^{\prime} \Bigr\} }
	\label{eq:RTE}
\end{align}
where the path from the satellite along the line-of-sight of the $j^\text{th}$ pointing direction is $\Gamma_j$ and the ozone concentration$\bm{x}(r)$ at distance $r$ from the radiometer.
The factor $\tau(r)\leq 1$ accounts for re-absorption of the radiation along the line-of-sight, which makes the RTE non linear.
The noise $\eta_j$ is added to each path integral, where the noise vector $ \bm{\eta} \sim \mathcal{N}(0, \gamma^{-1} \mathbf{I} )$ is normally distributed around zero with the noise precision $\gamma$.
The absorption constant $k(\nu, T)$ for a single gas molecule at a specific wavenumber $\nu$ is given by the HITRAN database \cite{gordon2022hitran2020} and acts as a source function when multiplied with the black body radiation $B(\nu,T)$, given by Planck`s law.
Within the stratosphere the number density $p(T) / (k_{\text{B}} T(r))$ of molecules is dependent on the pressure $p(T)$, the temperature $T(r)$, and the Boltzmann constant $k_{\text{B}}$.
For fundamentals on the Radiative transfer equation we recommend 79BOOKRadiativeProcess.

%The path integral over the line $\Gamma_j$ from the satellite to the end of the atmosphere defines the kernel $\bm{A_{j}}$ that is independent of ozone concentration $x$. 
We parametrize the ozone profile as a function of height, discretized into the $n$ values in each of $n$ layers of the discretized stratosphere where the $i^\text{th}$ layer is defined by two spheres of radii  $h_{i-1} < h_{i}$, $i = 1, \dots, n$, with $h_0$ and $h_{n} $.
In between the heights $h_{i-1}$ and $h_{i}$, each of the ozone concentration $x_{i}$, the pressure $p_{i}$, the temperature $T_{i}$, and thermal radiation is assumed to be constant.
Above $h_{n}$ and below $h_0$, the ozone concentration is set to zero, so no signal can be obtained.
Then depending on the parameter of interest, which is either the ozone volume mixing ratio $\bm{x} =\{x_1,x_2,\ldots,x_n\} \in \mathbb{R}^{n}$ or the fraction of pressure and temperature $\bm{p/T}= \{p_1/T_1,p_2/T_2,\ldots,p_n/T_n\} \in \mathbb{R}^{n} $, we can rewrite the integral in Eq.~\eqref{eq:RTE} as e.g. $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} $, where the absorption $\tau(r)$ induces non-linearity.
Here, the row vector $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{n}$  defines a Kernel for each measurement so that the data vector
\begin{align}
	\bm{y} = \bm{A}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} + \bm{\eta}= \bm{A}(\bm{x},  \bm{p},\bm{T}) \,
	\frac{ \bm{p}}{\bm{T}} + \bm{\eta} \, .
\end{align}
can be written as a matrix vector multiplication, where the matrix $\bm{A}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{m \times n}$  and the noise vector $\bm{\eta} \in \mathbb{R}^{m}$.

Since the absorption $\tau(r)$ reduces measurements by of order $1\%$, or less, making the inverse problem only weakly non-linear. 
We use that to approximate the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T})$ with a map $\bm{M}$ so that $\bm{A}(\bm{x},  \bm{p},\bm{T}) \approx \bm{M} \bm{A}_L $
Where each row $\bm{A}_{L,j} $ of matrix as $\bm{A}_L \in \mathbb{R}^{m \times n}$ is defined by the linear forward model, where absorption is neglected, e.g. $\tau = 1$. 
Then $\bm{A}_{L,j} $ is either defined by $ B(\nu,T) S(\nu, T)   \frac{\bm{p}(T)}{k_{\text{B}} \bm{T}(r)}  \text{d}r$ or $B(\nu,T) S(\nu, T)   \frac{\bm{x}}{k_{\text{B}}}  \text{d}r$, as in Eq..~\eqref{eq:RTE}, depending on the parameter of interest.
This poses a linear inverse problem with the forward map defined by the matrix $\bm{A} = \bm{M} \bm{A}_L$, where $\bm{M}$ is, more specifically, an affine map.


\section{Affine Map}

To approximate the non-linear forward model we use an affine map $ M:\bm{A}_L \bm{x} \rightarrow \bm{A}(\bm{x},  \bm{p},\bm{T})) \bm{x}$, which maps the linear forward model $\bm{A}_L \bm{x}$ onto the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}$.


An affine map is any linear map in between two vector spaces is or affine spaces, where in affine space does not need to have a zero origin. 2.3.1. PROPOSITION AND DEFINITIOn Berge book\cite{}.
In other words an affine map does not need to preserve the origin, or is a linear map on vector spaces including translation, or in the words of my supervisor, C. F., an affine map is a Taylor series of first order.
For more information on affine spaces and maps we refer to \cite{two books}

We generate two affine subspaces spaces \newline $V = \big\{ \bm{A}(\bm{x}^{(1)}, \bm{p,T}), \dots ,\bm{A}(\bm{x}^{(m)}, \bm{p,T})\big\} $ and $W = \big\{ \bm{A}\bm{x}^{(1)}, \dots ,\bm{A}\bm{x}^{(m)}\big\}$ over the same field, with fixed $\bm{p,T}$.
The parameter $\bm{x}$ is distributed as the so-called posterior distribution $\big\{  \bm{x}^{(1)} , \dots, \bm{x}^{(m)} \big\} \sim \pi(\bm{x}|\bm{\theta},\bm{y})$, with hyper-parameters $\bm{\theta}$, according to a Bayesian hierarchical model.



\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[rectnode] at (2,-4) (NL)    {$W$};
		\node[rectnode] at (-2,-4) (L)    {$V$};
		\draw[<-, very thick] (NL.west) -- (L.east); 
		\node[align=center] at (-5.5,-4) (f3) {linear forward model};
		\node[align=center] at (5.5,-4) (f4) {non-linear forward model};
		\node[align=center] at (0,-5) (f5) {$\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}  \approx \bm{M A}_L  \bm{x} = \bm{A x} $ };
		\node[align=center] at (0,-4) (f5) {affine Map \\ $\bm{M}$};
	\end{tikzpicture}
	\caption[Schematics of Affine Map]{Schematics of Affine Map, which approximates the linear forward model to the non-linear forward model.}
\end{figure}

\section{Bayesian Inference}
In this this section we give a short introduction to Bayesian inference for a general parameter $\bm{x}$ given some data $\bm{y}$, later in section \ref{} we set up a more sophisticated Bayesian framework applied to the forward model in section \ref{}.

We can visualise the correlation structure of a measurement process through a hierarchiallly ordered directed acyclic graph (DAG), see Figure \ref{fig:FirstDAG}.
As an observatory process naturally includes some random noise we include that in our DAG and classify the noise as a hyper-parameter in $\bm{\theta}$.
Other hyper-parameters influence the parmeters $\bm{x}$ detemernistaclly, which are then mapped through the forward model onto the space of all measurables $\bm{u}$, from which we observe some data $\bm{y}$ including noise as previously mentioned.
Drawing a DAG can help us to dependences within the measurement and modelling process.
Given some data we inferer the distribution of the underling parameters and hyper-parameters by following the arrows in Figure \ref{} backwards and set up a Bayesain hierachlly ordered model.


\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[roundnode2] at (0,4) (th)    {$\bm{\theta}$};
		\node[roundnode2] at (0,2) (x)    {$\bm{x}$};
		\node[roundnode2] at (0,0) (A)    {$\bm{A}$};
		\node[roundnode2] at (0,-2) (u)    {$\bm{u}$};
		\node[rectnode] at (0,-4) (y)    {$\bm{y}$};

        \draw[->, very thick] (th.south) -- (x.north); 
        \draw[->, very thick] (x.south) -- (A.north); 
        \draw[->, very thick] (A.south) -- (u.north); 
        \draw[->, very thick, mydotted] (u.south) -- (y.north); 
        \draw[->, very thick, mydotted] (th) edge[bend right=60] (y);  
  
        \node[align=center] at (3,4) (tht) {hyper-parameters};
        \node[align=center] at (3,2) (xt) {parameters};
        \node[align=center] at (3,0) (At) {forward model};
        \node[align=center] at (3,-2) (ut) {space of all measurables};
        \node[align=center] at (3,-4) (yt) {data};
        \node[align=center] at (-3,0) (nt) {noise};
	\end{tikzpicture}
	\caption[Bayesian Inference DAG]{The directed acyclic graph (DAG) for a typical linear inverse problem visualises forward dependencies as solid line arrows for deterministic dependencies and dotted arrows for statistical dependencies.
	Naturally the data $\bm{y}$ has some noise described through included in some hyper-parameters $\bm{\theta}$.
	The parameters $\bm{x}$ have some dependency of those hyper-parameters $\bm{\theta}$. The parameter $\bm{x}$ is mapped onto the space of all measurables $\bm{u}$ through the linear forward model $\bm{A}$, so that $\bm{Ax}$ is a linear operation.
	From the space of all measurables we can observe some data $\bm{y}$, statistically, where as prevoiusly mentioned some random noise is added.
	We set up a more sophisticated Bayesian model in chapter \ref{} explicitly including all hyper-parameters and parameters of interest according to the forward model in section \ref{}.}
	\label{fig:FirstDAG}
\end{figure}

\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[roundnode2] at (0,3.5) (th)    {$\bm{\theta}$};
		\node[roundnode2] at (0,1.5) (x)    {$\bm{x}$};
		\node[roundnode2] at (0,-1.5) (u)    {$\bm{u}$};
		\node[rectnode] at (0,-3.5) (y)    {$\bm{y}$};
		
		\draw[->, very thick] (th.south) -- (x.north); 
		\draw[->, very thick] (x.south) -- (u.north); 
		\draw[->, very thick, mydotted] (u.south) -- (y.north); 
		\draw[->, very thick, mydotted] (th) edge[bend right=60] (y);  
		
		\node[align=center] at (3,3.5) (tht) {hyper-parameters};
		\node[align=center] at (3,1.5) (xt) {parameters};
		\node[align=center] at (1.1,0) (At) {$\bm{A}$ \quad forward model};
		\node[align=center] at (3,-1.5) (ut) {space of all measurables};
		\node[align=center] at (3,-3.5) (yt) {data};
		\node[align=center] at (-3,0) (nt) {noise};
	\end{tikzpicture}

\end{figure}


Within a linear Bayesian hierarchial model we need to define a likelihood function as well as distribution over the unknown parameters $\bm{x}$ and hyper-parameters $\bm{\theta}$.
\begin{subequations}
	\begin{align}
		\bm{y}|\bm{x}, \bm{\theta}&\sim \mathcal{N}(\bm{A} \bm{x}, \bm{\Sigma}(\bm{\theta})) \label{eq:likelihood}  \\
		\bm{x}| \bm{\theta} & \sim  \mathcal{N}( \bm{\mu}, \bm{Q}^{-1}(\bm{\theta})  ) \label{eq:xPrior} \\
		\bm{\theta} &\sim  \pi(\bm{\theta}) \label{eq:gammaPrior}\, ,
	\end{align}
	\label{eq:BayMode}
\end{subequations}
with the noise covariance matrix $\bm{\Sigma}(\bm{\theta})$, so that $\bm{\eta}  \sim \mathcal{N}(0, \bm{\Sigma}(\bm{\theta})) $ as in Eq. \ref{}, the prior precision matrix $\bm{Q}(\bm{\theta})$, prior mean $\bm{\mu}$
and some prior distribution over the hyper-parameters $\pi(\bm{\theta})$.
Through sensibly choosing the prior distributions  $\pi(\bm{x}|\bm{y})$ as well as the hyper-parameters $\bm{\theta}$ and their prior distribution  $\pi(\bm{\theta})$, we can incorporate functional dependencies as well physical properties of the parameters.
The likelhood function $\pi(\bm{y}|\bm{x}, \bm{\theta})$ is a measure of how the parameters and hyper-parameters fit to the data according to our forward model, including information of the measurement process.

With a normally distributed prior and likelihood function this becomes a linear-Gaussian Bayesian hierarchical model.
For more detailed Bayesian analysis we recommend \cite{}.

The posterior distribution, the function of interest,
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) = \frac{ \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta})}{\pi(\bm{y})} \, ,
\end{align}
is given according to Bayes' theorem \cite{}, with the prior distribution $\pi(\bm{x}, \bm{\theta})$ and the normalising constant $\pi(\bm{y})$.
If the normalising constant is finite and non-zero we can approximate the posterior distribution
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) \propto \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta}) \, .
\end{align}

Then the expectation of any a function $h(\bm{x},\bm{\theta})$ can be described as 
\begin{align}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})] =  \int \int   h(\bm{x},\bm{\theta}) \,  \pi(\bm{x}, \bm{\theta} | \bm{y} ) \, \text{d} \bm{x}  \, \text{d} \bm{\theta}   \label{eq:expPos} \, ,
\end{align}
which is usually a high dimensional integral and computationally not feasible to solve.

One way to work around the high dimensionality is to parameterise $\bm{x}$ using hyper-parameters $\bm{\theta}$ so that $\bm{x}(\bm{\theta})$. 
Another way is to seperate the posterior distribution over latent field $\bm{x}$ and the hyper-parameters $\bm{\theta}$.
This is particular benefitial, when $\bm{x}$ is high dimensional, e.g. $\bm{x} \in \mathbb{R}^n$ with $n = 45$ and can not be parametrised, and $\bm{\theta}$ is low dimensional, e.g. two dimensional.


\subsection{Marginal and then Conditional}
The marginal and then conditional (MTC) method factorises the full posterior distribution 
\begin{align}
	\pi(\bm{x}, \bm{\theta}|\bm{y}) = \pi(\bm{x}| \bm{\theta}, \bm{y}) \pi(\bm{\theta}|\bm{y})
\end{align}
into the marginal posterior distribtion $ \pi(\bm{\theta}|\bm{y})$ and conditional posterior distribution $\pi(\bm{x}| \bm{\theta}, \bm{y})$.


For the in Eq. \ref{} specified linear-Gaussian Bayesian hierarchical model the marginal posterior distribution is given as
\begin{align}
	\pi(\bm{\theta} | \bm{y}) &= \int \pi(\bm{x}, \bm{\theta} | \bm{y}) \diff \bm{x} \\ 
	\label{eq:condHyper}
	&\propto \sqrt{ \frac{ \det( \bm{\Sigma}^{-1} ) \,  \det( \bm{Q}) }{\det( \bm{Q} + \bm{A}^T \bm{\Sigma}^{-1} \bm{A} ) } } \times \exp \Big[ - \frac{1}{2}(\bm{y} -\bm{A \mu})^T \bm{Q}_{\bm{\theta|y}} (\bm{y} -\bm{A \mu}) \Big] \pi(\bm{\theta}) \, ,
\end{align}
with
\begin{align}
	\bm{Q}_{\bm{\theta|y}} = \bm{\Sigma}^{-1} - \bm{\Sigma}^{-1} \bm{A} (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \bm{A}^T \bm{\Sigma}^{-1} \,  .
\end{align}

See lemma \cite{}.

Then conditioned on the hyper-parameters $\bm{\theta}$ we can draw samples of the conditional posterior distribution
\begin{align}
	\bm{x}| \bm{y} , \bm{\theta} \sim \mathcal{N}\big( \bm{\mu} + (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \bm{A}^T \bm{\Sigma}^{-1} (\bm{y} - \bm{A} \bm{\mu}), (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \big) \, ,
\end{align}
see section \ref{} or calculate weighted expectations of a function $h(\bm{x})$
\begin{align}
	\label{eq:lte}
	\text{E}_{\bm{x}|\bm{y}} [h(\bm{x})] = \int   \text{E}_{\bm{x}|\bm{\theta},\bm{y}} [h(\bm{x})] \, \pi(\bm{\theta} | \bm{y} )  \, \text{d} \bm{\theta} \,  ,
\end{align}
with weights given by $\pi(\bm{\theta} | \bm{y} )$.
\cite{}
Note that the noise covariance $\bm{\Sigma}^{-1} = \bm{\Sigma}^{-1}( \bm{\theta}) $ and the prior precision $\bm{Q} = \bm{Q}( \bm{\theta})$ are depending on hyper-parameters $\bm{\theta}$.

In this thesis we will use sampling and deterministic methods to characterise the posterior distribution over the hyper-parameters and present the basics of those in the following sections.

\section{Regularisation}

Data

linear problem
\begin{align}
	\bm{y} = \bm{A}\bm{x} + \bm{\eta}
\end{align}

\begin{align}
	\left\lVert \bm{y} - \bm{A}\bm{x}  \right\rVert^2
\end{align}
data misfit norm

solution semi norm

\begin{align}
	\lambda \left\lVert \bm{x}\bm{L}\bm{x}  \right\rVert^2
\end{align}
$\lambda \geq 0$

\begin{align}
	\text{arg min} \left\lVert \bm{y} - \bm{A}\bm{x}  \right\rVert^2 + \lambda \left\lVert \bm{T}\bm{x}  \right\rVert^2
\end{align}
Derivative of solution
With $   \bm{T}^T \bm{T} = \bm{L}$
\begin{align}
	\text{arg min} \left\lVert \bm{y} - \bm{A}\bm{x}  \right\rVert^2 + \lambda \bm{x}^T  \bm{L}\bm{x} 
\end{align}

minizer function

L graph lapacian for example see late what we choose

Typically, L is the identity matrix or
a banded matrix approximation to the (n  p)th derivative.

to the 2nd derivative




The primary difficulty with linear ill-posed problems is that the inverse image is undetermined
due to small (or zero) singular values of A. Actually the situation is a little worse in practice
because A depends on our model of the measurement process and that is typically not
precisely known, leading to a slight imprecision in the singular values. Usually that is not
significant for the large singular values, but may lead to ambiguity in the small singular values
so that we do not know if they are small or zero.
As an introduction to regularization, which is one method for surmounting the problems
associated with small singular vectors, we consider a framework for describing the quality of
a reconstruction fin an inverse problem.
3.1
The data misfit and the solution semi-norms
In the last chapter, we considered the linear problem
d = Af  n
and focused on the structure of the operator A
 . As far as the data are concerned, areconstructed image is good provided that it gives rise to ‘mock data’ A which are closeto the observed data. Thus, one of the quantities for measuring the quality of fis the data
misfit function which is usually the square of the residual norm


Since the data do not give us any information about some aspects of f , it is necessary to
include additional information which allows us to select from among several feasible recon-
structions. Analytical solutions are available if we choose sufficiently simple criteria. One way
of doing this is to introduce a second function representing our aversion to a particular
49reconstruction. For example, we may decide that the solution having minimum norm should
be chosen from among the feasible set. This can be done by choosing



\section{Sampling Methods}
In this section we present the sampling based methods used in this thesis to generate an ergodic Markov-Chain $ (\bm{x}, \bm{\theta} )^{(0)}, \dots, (\bm{x}, \bm{\theta} )^{(k)} , \dots,  (\bm{x}, \bm{\theta})^{(N)} \sim \pi(\bm{x},\bm{\theta}| \bm{y}) $, where the samples $(\bm{x}, \bm{\theta} )^{(k)}$ are distributed as $\pi(\bm{x},\bm{\theta}| \bm{y}) $.
Ergodicity is implied if an aperiodic and irreduce chain \ref{} proves to be reversible, then the chain converges and has a unique  equilibrium distribution.
In other words if from a state in the chain we can reach every other state in the sampling space and the previous state, and we do not get stuck in periodic loop, then it converges.
Instead of proving ergodicity we can in practise look e.g. at the output samples and their trace $\pi(\bm{x}^{(k)},\bm{\theta}^{(k)}| \bm{y})$ to see convergence towards a stationary distribtuion.


Ergodicity important so that we can from the sample based estimate

if the pi(f 2) is samller than inf
General state space Markov chains and
MCMC algorithms

This gives an unbiased estimate, having standard deviation of order O(1/ sqrt N ).
Furthermore, if pi(f 2) < inf, then by the classical Central Limit Theorem, the
error pi(f ) - pi(f ) 
practical markov chain methods

if egodic then converges
then CLT holds for
anc coverges in distribution to


Then for large enough $N$ the samples based estimate of Eq. \ref{} and of any function $h(\bm{x}, \bm{\theta})$ is
\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}, \bm{\theta})] \approx \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)},\bm{\theta}^{(k)}) \, .
\end{align}

In practise of this thesis we use Markoc-chain Monte-Carlo (MCMC) methods on target distributions such as $\pi(\bm{\theta}| \bm{y})$ and so we will illustrate the sampling procedures for the following three subsections on that distribution.


convergence of 
MTC

\subsection{Metropolis- within Gibbs sampling}
Ergodic staatsion ary distributon 
\cite{}
ARRIS RECURRENCE OF METROPOLIS-WITHIN-GIBBS AND
TRANS-DIMENSIONAL MARKOV CHAINS
By Gareth O. Roberts and Jeffrey S. Rosentha 2006


for 2 D


\begin{algorithm}
	\caption{Metropolis within Gibbs}
	\begin{algorithmic}[1]
		\STATE Initialize \( \bm{\theta}^{(0)} \)
		\FOR{ \( k = 1, \dots, N \)}
		\STATE Propose \( \bm{\theta} \sim q(\bm{\theta} | \bm{\theta}^{(t-1)}) = q(\bm{\theta}^{(t-1)} | \bm{\theta}) \)
		\STATE Compute
		\[ \alpha(\bm{\theta} | \bm{\theta}^{(t-1)}) = \min \left\{ 1, \frac{\pi(\bm{\theta}  | \bm{y}) \cancel{q(\bm{\theta}^{(t-1)} | \bm{\theta}) } }{\pi(\bm{\theta}^{(t-1)}| \bm{y}) \cancel{q(\bm{\theta} | \bm{\theta}^{(t-1)})} } \right\} \]
		\STATE Draw $u \sim \mathcal{U}(0,1)$
		\IF{$\alpha \geq u$ }
		\STATE Accept and set \( \bm{\theta}^{(t)} = \bm{\theta} \)
		\ELSE  
		\STATE Reject and keep \(\bm{\theta}^{(t)} = \bm{\theta}^{(t-1)} \)
		\ENDIF
		\STATE Draw \(\bm{\theta}_j^{(t)} \sim  \pi(\bm{\theta}_j | \bm{\theta}_{<j}, \bm{\theta}_{>j} , \bm{y} )\) 
		\ENDFOR
		\STATE Output: $ \bm{\theta}^{(0)}, \dots,  \bm{\theta}^{(k)} , \dots,   \bm{\theta}^{(N)} \sim \pi(\bm{\theta}| \bm{y}) $
	\end{algorithmic}
\end{algorithm}


\subsection{Draw a sample from a multivariate normal distribution}
for Linear Gaussian Bayesian hierarchical model We need to draw a sample from the multivariate normal distribution.
We can do this using the radomize then optimise (RTO) method \cite{}, with a perturb exponential.

In this thesis we use the RTO method to draw a sample from the full conditional distribution $\pi( \bm{x}| \bm{y} , \bm{\theta} )$, see section \ref{}, which is a normal distribution we can rewrite to:
\begin{align}
	\pi(\bm{x}|\bm{y}, \bm{\theta} ) &\propto \pi(\bm{y} | \bm{x} , \bm{\theta} ) \pi(\bm{x}| \bm{\theta}) \\
	&= \exp  \lVert \hat{\bm{A}} \bm{x} - \hat{\bm{y}} \rVert^2 \, ,
\end{align}
where 
\begin{align}
	\label{eq:minimizer}
	\hat{\bm{A}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta})  \bm{A}\\
		\bm{Q}^{1/2}(\bm{\theta}) 
	\end{bmatrix} \, , \quad \hat{\bm{y}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta})  \bm{y} \\
		\bm{Q}^{1/2}(\bm{\theta}) \bm{\mu}
	\end{bmatrix} \, \text{\cite{}}.
\end{align}

Then one sample can be computed by minimising the following equation with respect to $\hat{\bm{x}}$ :
\begin{align}
	\bm{x}_i = \arg \min_{\hat{\bm{x}}} \lVert \hat{\bm{A}} \hat{\bm{x}} - ( \hat{\bm{y}} + \bm{\eta} ) \rVert^2 , \quad \bm{\eta} \sim \mathcal{N}(\bm{0}, \mathbf{I}) \, ,
\end{align}
where we add a randomised perturbation $\bm{\eta}$.
Next, we substitute $ - \hat{\bm{A}}^T  \bm{\eta}  = \bm{v}_1 + \bm{v}_2$ we can rewrite the argument of Eq. \ref{eq:minimizer} to 
\begin{align}
	\label{eq:RTO}
	(\bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A}+
	\bm{Q}(\bm{\theta}) ) \bm{x}_i &= \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{y} +  \bm{Q}(\bm{\theta}) \bm{\mu} + \bm{v}_1 + \bm{v}_2 \,  ,
\end{align}
where $\bm{v}_1 \sim \mathcal{N}(\bm{0}, \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A}) $ and $\bm{v}_2 \sim \mathcal{N}(\bm{0}, \bm{Q}(\bm{\theta}) )$ are independent random variables \cite{}.

this converges due to acosta and ...

\subsection{Metropolis}

The Metropolis algorithm is special case of the Metropolis-Hastings algorithm, with a symmetric proposal distribution $q(i|j) =q(j|i) $ \cite{}.

The Metropolis-Hastings algorithm starts with a initial sample $\bm{\theta}^{(t)}$ at  at $t=0$.
We propose a new sample $\bm{\theta}\sim q(\bm{\theta} | \bm{\theta}^{(t-1)})$ according to the proposal distribution $q(\bm{\theta} | \bm{\theta}^{(t-1)})$ given the previous state.
Then accept and set $ \bm{\theta}^{(t)} = \bm{\theta}$ with
\begin{align}
\alpha(\bm{\theta} | \bm{\theta}^{(t-1)}) = \min \left\{ 1, \frac{\pi(\bm{\theta}  | \bm{y}) q(\bm{\theta}^{(t-1)} | \bm{\theta})  }{\pi(\bm{\theta}^{(t-1)}| \bm{y}) q(\bm{\theta} | \bm{\theta}^{(t-1)}) } \right\}
\end{align}
or reject and keep $\bm{\theta}^{(t)} = \bm{\theta}^{(t-1)}$, which we do by comparing $\alpha$ to a uniform random number $u \sim \mathcal{U}(0,1)$.
Note that symmetrical proposal distribution $q(\bm{\theta} | \bm{\theta}^{(t-1)}) = q(\bm{\theta}^{(t-1)} | \bm{\theta}) $ cancel, then we call that a Metropolis-algorithm \cite{}.
To assure convergence independent of the intial sample we discard samples after the so-called burn-in period and effectivally generate a Markov-Chain of length $N - N_{\text{burn-in}}$.



Since the proposal distribution is symmetric, Metropolis chains are reversible and as you can see in $\alpha(\bm{\theta} | \bm{\theta}^{(t-1)})$ converge towards $\pi(\bm{\theta}| \bm{y})$.
A more mathematical prove of ergodicity for the general Metropolis-Hastings algorithm can be found in \cite{}.

\subsection{Gibbs Sampling}
Gibb sampling is a special case of the metropolis hastings algorihtm with the acceptance probaility of 1.
In Gibbs sampling according to \cite{} we move one directional by fixing all other direction.

Assume $\bm{\theta} \in \mathbb{R}^d$ is d-dimesnional and $\{\bm{\theta}_{<j}, \bm{\theta}_{>j} \}  = \{\bm{\theta}_{1}, \dots, \bm{\theta}_{j-1}, \bm{\theta}_{j+1}, \dots, \bm{\theta}_{d} \} $ denotes all dimesnions except $ \bm{\theta}_{j}$.
Then to draw a new sample $\bm{\theta}^{(t)}$, we iterate through each dimension to draw a sample $\bm{\theta}_j^{(t)} \sim  \pi(\bm{\theta}_j | \bm{\theta}_{<j}, \bm{\theta}_{>j} , \bm{y} )$ from the conditional distribution.

\begin{algorithm}
	\caption{Gibbs}
	\begin{algorithmic}[1]
		\STATE Initialize \( \bm{\theta}^{(0)} = \{\bm{\theta}^{(0)}_{1}, \dots, \bm{\theta}^{(0)}_{j},\dots,\bm{\theta}^{(0)}_{d} \} \).
		\FOR{ \( k = 1, \dots, N \)}
		\FOR{ \( j = 1, \dots, d \)}
		\STATE Draw \(\bm{\theta}_j^{(t)} \sim  \pi(\bm{\theta}_j | \bm{\theta}_{<j}, \bm{\theta}_{>j} , \bm{y} )\) 
		\ENDFOR
		\ENDFOR
		\STATE Output: $ \bm{\theta}^{(0)}, \dots,  \bm{\theta}^{(k)} , \dots,   \bm{\theta}^{(N)} \sim \pi(\bm{\theta}| \bm{y}) $
	\end{algorithmic}
\end{algorithm}

If the target distribution is well behaved $\pi(\bm{\theta}_j | \bm{\theta}_{<j}, \bm{\theta}_{>j} , \bm{y} ) > 0$ in the sampling space the chain produced by the gibbs sampling algorithm is aperiodic and ireducabile.
We can also show that the criterium, the detailed balance condition, for reveribility is met \cite{}.
Then a Gibbs sampler prodices an ergodic markov chain.


\subsection{t-walk}
We use the t-walk sampler developed By Christens and Fox as a black box sampler\cite{}.
Within the t-walk there are three differnt moves in the sampling space availbe and by construction of the t-walk is ergodic, see section \cite{}.




\section{Numerical Approxiamtion Methods - Tensor Train}
Using the tensor train format to approximate a $d$-dimensional function $\pi(x)$ enables us to compute marginal posterior probability distribution cheaply.
As the name suggest the tensor train format is a train of tensors, more specifically two and three dimensional tensors which we call cores $\pi_{k} \in \mathbb{R}^{r_{k-1} \times n \times r_{k}}$  and which are connected through a ranks $r_{k}$ and $r_{k-1}$ for the $k$th dimension and defined by the number of gridpoints $n$, as in Figure \ref{} displayed.
For the first and last dimensional core the outer ranks are $r_0  = r_d = 1$.




\begin{figure}[ht!]
	\centering
\begin{tikzpicture} 
	\node[rectnode] at (-5,0) (T1)    {$1 \times n \times r_1$};
	\node[rectnode] at (-2,0) (T2)    {$r_1 \times n \times r_{2}$};
	
	\node[rectnode] at (3,0) (Tn1)    {$r_{d-2} \times n \times r_{d-1} $};
	\node[rectnode] at (6.75,0) (Tn)    {$r_{d-1} \times n \times 1$};
	\draw[-, very thick] (T1.east) -- (T2.west); 
	\draw[-, very thick] (Tn.west) -- (Tn1.east); 
	\draw[-, mydotted, very thick] (T2.east) -- (Tn1.west);
	
	\node[align=center] at (-3.5,0.25) (R) {$r_1$};
	\node[align=center] at (5,0.25) (R) {$r_{d-1}$};
	
\end{tikzpicture} 
\caption{text}
\label{key}
	
\end{figure}


\begin{figure}[ht!]
	\centering

	\caption{nice matries picture}
	\label{key}
	
\end{figure}


The approximated marginal target function
\begin{align}
	\begin{split}
	f_{X_k}(x_k) = \frac{1}{z}
	\Big|  &\left( \int_{\mathbb{R}} \pi_{1}(x_1) \uplambda_1(x_1)\text{d}x_{1} \right) \cdots \left( \int_{\mathbb{R}} \pi_{k-1}(x_{k-1}) \uplambda_{k-1}(x_{k-1}) \text{d}x_{k-1} \right) \\ & \qquad \pi_{k}(x_k)\uplambda_k(x_{k}) \\ &\quad \left( \int_{\mathbb{R}} \pi_{k+1}(x_{k+1})\uplambda_{k+1}(x_{k+1})\text{d}x_{k+1} \right) \cdots  \left( \int_{\mathbb{R}} \pi_{d}(x_d)\uplambda_d(x_{d})\text{d}x_d \right) \Big| \, ,
	\end{split} 
\end{align}
is given by integration over each core, where $k$th is in $\mathbb{R}^{r_{k-1} \times r_{k}}$ and z is some normalising constant.
Here we introduce some Lebesgue measurable weight function  $\uplambda(x) = \prod^d_{i=1} \uplambda_i(x_i)$.
Why? \cite{}.
\\


%%%%%% Squared and Basis Function %%%%%
From here the notation and procedure is taken mostly from \cite{}.
For numerical stability we can approximate the sqaure root of
\begin{align}
	\sqrt{\pi(x)} \approx \tilde{g}(x) = \bm{G}_1(x_1), \dots ,  \bm{G}_k(x_k), \dots ,  \bm{G}_d(x_d)
\end{align}
where the TT-core
\begin{equation}
	G^{(\alpha_{k-1},\alpha_k)}_k(x_k) = \sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{A}_k[\alpha_{k-1}, i, \alpha_k], \quad k = 1, ..., d,
\end{equation}
with the associated $k$th coefficient tensor $\bm{A}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ and the $k$-th  basis functions $\phi^{(i)}_k(x_k)$.

We assume the function
\begin{align}
	\pi(x) \approx \gamma^{\prime} + g^2(x) \, ,
\end{align} 
where $g(x)$ is defined through the tensor train decomposition plus an error $\gamma^{\prime}$  according to the l2 norm.
Then the normalised target function is 
\begin{align}
	f_X(x) = \frac{1}{z} \pi(x) \uplambda(x) = \frac{1}{z} ( \gamma^{\prime} \uplambda(x) + g^2(x) \uplambda(x))
\end{align} 
with a normalisation constant $z$.
Consquently the approximated marginal functions can be expressed as
\begin{align}
	\begin{split}
		f_{X_k}(x_k) = \frac{1}{z}  \Bigg( \gamma^{\prime}& \prod_{i=1}^{k-1} \uplambda_i(\mathcal{X}_i) \prod_{i=k+1}^{d} \uplambda_i(\mathcal{X}_i) \\&+  \left( \int_{\mathbb{R}} \bm{G}^2_{1}(x_1) \uplambda_1(x_1)\text{d}x_{1} \right) \cdots \left( \int_{\mathbb{R}} \bm{G}^2_{k-1}(x_{k-1}) \uplambda_{k-1}(x_{k-1}) \text{d}x_{k-1} \right) \\ & \bm{G}^2_{k}(x_k)\uplambda_k(x_{k})\\ & \left( \int_{\mathbb{R}}  \bm{G}^2_{k+1}(x_{k+1})\uplambda_{k+1}(x_{k+1})\text{d}x_{k+1} \right) \cdots  \left( \int_{\mathbb{R}} \bm{G}^2_{d}(x_d)\uplambda_d(x_{d})\text{d}x_d \right) \Bigg) \, ,
	\end{split} 
\end{align}
where $\uplambda_k( \mathcal{X}_k) = \int_{ \mathcal{X}_k} \uplambda_k (x_k) \text{d}x_k$.

To effeciently calculate these marginals on can us a procedure which is called left and right orthogonalization of cores \cite{} [32].
To do so we define the mass matrix $\bm{M}_k \in \mathbb{R}^{n_k \times n_k}$ by
\begin{equation}
	\bm{M}_k[i, j] = \int_{X_k} \phi^{(i)}_k(x_k) \phi^{(j)}_k(x_k)  \uplambda(x_k) \,dx_k, \quad i = 1, ..., n_k, \quad j = 1, ..., n_k,
\end{equation}
where $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ is the set of basis functions for the $k$-th coordinate.
%In the case of cartesian basis with $\uplambda= 1$ M is the identity matrix, which we need to compute the marginals.
%For a boundend Parameter space we consider the weihtin funcitn $\uplambda(x) = 1$




\subsection{Marginal Functions}

We calculate the marginal functions through procedures, which we call backward marginalisation \cite{} and forward marginalisation.
We gain the coefficient matrices $\bm{B}_k$ through backward marginalisation and the coefficient matrices $\bm{B}_{pre,n}$ through forward marginalisation, which enables us to calculate marginal fuunction similar to \cite{}.




The proposition \ref{prob:backMarg} to caculte $\bm{B}_k$  is taken from \cite{}.

\begin{prop}[Backward Marginalisation]
	Starting with the last coordinate $k = d$, we set $\bm{B}_d = \bm{A}_d$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{k-1} \in \mathbb{R}^{r_{k-2} \times n_{k-1} \times r_{k-1}}$, which we need for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_k[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{B}_k[\alpha_{k-1}, i, l_k] \bm{L}_k[i, \tau].
		\end{equation}
		\item Unfold $\bm{C}_k$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_k^{(R)} \in \mathbb{R}^{r_{k-1} \times (n_k r_k)}$:
		\begin{equation}
			\bm{Q}_k \bm{R}_k = {(\bm{C}_k^{(R)})}^{\top}.
		\end{equation}
		\item Compute the new coefficient tensor:
		\begin{equation}
			\bm{B}_{k-1}[\alpha_{k-2}, i, l_{k-1}] = \sum_{\alpha_{k-1}=1}^{r_{k-1}} \bm{A}_{k-1}[\alpha_{k-2}, i, \alpha_{k-1}] \bm{R}_k[l_{k-1}, \alpha_{k-1}].
		\end{equation}
	\end{enumerate}
\label{prob:backMarg}
\end{prop}

Then we need to do this the other way as well.


In addiotn we also have to  do forward marginalistion starig with the first dimension
\begin{prop}[Forward Marginalistaion]
	Starting with the first coordinate $k = 1$, we set $\bm{B}_{pre,1} = \bm{A}_1$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{pre,k+1} \in \mathbb{R}^{r_{k} \times n_{k+1} \times r_{k+1}}$ for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_{pre,k}[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{L}_k[i, \tau] \bm{B}_{pre,k}[\alpha_{k-1}, i, l_k] .
		\end{equation}
		\item Unfold $\bm{C}_{pre,k}$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_{pre,k}^{(R)} \in \mathbb{R}^{(r_{k-1} n_k ) \times r_k}$:
		\begin{equation}
			\bm{Q}_{pre,k}\bm{R}_{pre,k} = {(\bm{C}_{pre,k}^{(R)})}.
		\end{equation}
		\item Compute the new coefficient tensor $\bm{B}_{pre, k+1} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k} $:
		\begin{equation}
			\bm{B}_{pre, k+1}[l_{k+1}, i, \alpha_{k+1}] = \sum_{\alpha_{k}=1}^{r_{k}} \bm{R}_{pre,k}[l_{k+1}, \alpha_{k}] \bm{A}_{k+1}[\alpha_{k}, i, \alpha_{k+1}] .
		\end{equation}
	\end{enumerate}
\end{prop}



The marginal PDF of $X_{k}$ can be expressed as
\begin{equation}
	f_{X_k}(x_k) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=1}^{k-1} \lambda_i(X_i) \prod_{i=k+1}^{d} \lambda_i(X_i) + \sum_{l_{k-1}=1}^{r_{k-1}} \sum_{l_k=1}^{r_k} \left(\sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{D}_k[l_{k-1},i, l_k] \right)^2 \right) \lambda_k(x_k),
\end{equation}
where $\bm{D}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ and $\bm{R}_{pre,k-1}\in \mathbb{R}^{r_{k-1} \times r_{k-1}}$ and $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$
\begin{equation}
	\bm{D}_k[l_{k-1},i,l_k] = \sum_{\alpha_{k-1}=1}^{r_{k-1}}  \bm{R}_{pre,k-1}[l_{k-1}, \alpha_{k-1}] \bm{B}_k[\alpha_{k-1}, i, l_k].
\end{equation}


Special Cases
The marginal PDF of $X_1$ can be expressed as
\begin{equation}
	f_{X_1}(x_1) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=2}^{d} \uplambda_i(\mathcal{X}_i) + \sum_{l_1=1}^{r_1} \left(\sum_{i=1}^{n_1} \phi^{(i)}_1(x_1) \bm{D}_1[i, l_1] \right)^2 \right) \uplambda_1(x_1),
\end{equation}
where $\bm{D}_1[i, l_1] = \bm{B}_1[\alpha_0, i, l_1]$ and $\alpha_0 = 1$.\\
The marginal PDF of $X_n$ can be expressed as
\begin{equation}
	f_{X_n}(x_n) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=1}^{d-1} \uplambda_i(\mathcal{X}_i) + \sum_{l_{n-1}=1}^{r_{n-1}} \left(\sum_{i=1}^{n_1} \phi^{(i)}_1(x_1) \bm{D}_n[l_{n-1},i] \right)^2 \right) \uplambda_n(x_n),
\end{equation}
where $\bm{D}_n[l_{n-1},i] = \bm{B}_{pre,n}[l_{n-1}, i, \alpha_n]$ and $\alpha_n = 1$.
