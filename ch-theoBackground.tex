\chapter{Theoretical and Technical Background}

In this chapter we give a brief introduction into the methods, that we use in this thesis.
We try to keep it as general as possible, as we get more specific in the results, section \ref{}.

\section{Forward Model}

\begin{figure}[ht!]
	\centering
	\scalebox{0.9}{\input{FirstLIMB.pdf_tex}}
	\caption{This figure vsiualises the line of sight of a sattelite at height $h_{obs}$ in a discretiesed atmosphere. The atmosphere has $n$ layers so the line of sight can be disctreized in $\Delta r_i$ for $i = \ell_j , \dots, n$.
	Where $\ell_j \in \mathbb{N}$ denotes a tagent hieght $h_{\ell_j}$  with respect to $R_E$ the erth radius. We use this set up to solve the radiative transfer equation numerically.}
	\label{fig:FirstLIMB}
\end{figure}

In this section we describe the forward model which we use to simulate data, as in section \ref{} and base the Bayesian inference, see section \ref{} and \ref{}.


As shown in Figure~\ref{fig:FirstLIMB}, one measurement of a stationary satellite can be describes as the path integral along the line of sight $\Gamma_j$ for $j=1,2,\ldots,m$.
For each measurement we can define a tangent height $h_{\ell_j}$ as the shortest distance along the line of sight to the earth.

The $j^\text{th}$ measurement, taken on line of sight $\Gamma_j$  is modelled by the radiative transfer equation (RTE)~\cite{mipas2000handbook}
\begin{align}
	\label{eq:RTE} 
	y_j =   \int_{\Gamma_j}  B(\nu,T) k(\nu, T)   \frac{p(T)}{k_{\text{B}} T(r)}  x(r)  \tau(r) \text{d}r + \eta_j \, \\
	\tau(r) = \exp{ \Bigl\{ - \int^{r}_{r_\text{obs}}  k(\nu, T)   \frac{p(T)}{k_B T(r^{\prime})}  x(r^{\prime}) \text{d}r^{\prime} \Bigr\} }
\end{align}
where the path from the satellite along the line-of-sight of the $j^\text{th}$ pointing direction is $\Gamma_j$ and the ozone concentration at distance $r$ from the radiometer is $x(r)$ plus some noise $\eta_j$.
Within the stratosphere the number density $p(T) / (k_{\text{B}} T(r))$ of molecules is dependent on the pressure $p(T)$, the temperature $T(r)$, and the Boltzmann constant $k_{\text{B}}$.
The factor $\tau(r)\leq 1$ accounts for re-absorption of the radiation along the line-of-sight, which makes the RTE non linear.
The absorption constant $k(\nu, T)$ for a single gas molecule at a specific wavenumber $\nu$ is given by the HITRAN database \cite{gordon2022hitran2020} and acts as a source function when multiplied with the black body radiation $B(\nu,T)$, given by Planck`s law \cite{rybicki2000rte}.
For fundamentals on the Radiative transfer equation we recommend Chapter one in \cite{rybicki2000rte}.

%The path integral over the line $\Gamma_j$ from the satellite to the end of the atmosphere defines the kernel $\bm{A_{j}}$ that is independent of ozone concentration $x$. 
To enable Matrix-Vector multiplication, we parametrise the ozone profile as a function of height, discretised into the $n$ values in each of $n$ layers of the discretised stratosphere where the $i^\text{th}$ layer is defined by two spheres of radii  $h_{i-1} < h_{i}$, $i = 1, \dots, n$, with $h_0$ and $h_{n} $.
In between the heights $h_{i-1}$ and $h_{i}$, each of the ozone concentration $x_{i}$, the pressure $p_{i}$, the temperature $T_{i}$, and thermal radiation is assumed to be constant.
Above $h_{n}$ and below $h_0$, the ozone concentration is set to zero, so no signal can be obtained.
Then depending on the parameter of interest, which is either the ozone volume mixing ratio $\bm{x} =\{x_1,x_2,\ldots,x_n\} \in \mathbb{R}^{n}$ or the fraction of pressure and temperature $\bm{p/T}= \{p_1/T_1,p_2/T_2,\ldots,p_n/T_n\} \in \mathbb{R}^{n} $, we can rewrite the integral in Eq.~\eqref{eq:RTE} as e.g. as a vector multiplication $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} $, where the non-linear absorption $\tau(r)$ is included in $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T})$.
Here, the row vector $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{n}$  defines a Kernel for each measurement so that the data vector
\begin{align}
	\bm{y} = \bm{A}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} + \bm{\eta}= \bm{A}(\bm{x},  \bm{p},\bm{T}) \,
	\frac{ \bm{p}}{\bm{T}} + \bm{\eta} \, .
\end{align}
can be written as a matrix vector multiplication, where the matrix $\bm{A}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{m \times n}$  and the noise vector $\bm{\eta} \in \mathbb{R}^{m}$.

Since the measurement process includes absorption $\tau(r)$ reducing measurements slighty and making the inverse problem only weakly non-linear. 
We use that to approximate the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T})$ with a map $\bm{M}$ so that $\bm{A}(\bm{x},  \bm{p},\bm{T}) \approx \bm{M} \bm{A}_L $
Where each row $\bm{A}_{L,j} $ of matrix as $\bm{A}_L \in \mathbb{R}^{m \times n}$ is defined by the linear forward model, where absorption is neglected, e.g. $\tau = 1$. 
Then each entry in the row vector $\bm{A}_{L,j} $ is either defined by $ B(\nu,T) S(\nu, T)   \frac{\bm{p}(T)}{k_{\text{B}} \bm{T}(r)}  \text{d}r$ or $B(\nu,T) S(\nu, T)   \frac{\bm{x}}{k_{\text{B}}}  \text{d}r$, as in Eq..~\eqref{eq:RTE}, depending on the parameter of interest.
This poses a linear inverse problem with the forward map defined by the matrix $\bm{A} = \bm{M} \bm{A}_L$, where $\bm{M}$ is, more specifically, an affine map.


\section{Affine Map}

To approximate the non-linear forward model we use an affine map $ M:\bm{A}_L \bm{x} \rightarrow \bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}$, which maps the linear forward model $\bm{A}_L \bm{x}$ onto the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}$.


An affine map is any linear map in between two vector spaces or affine spaces, where an affine space does not need to preserve a zero origin, see Def. 2.3.1. in \cite{berger2009geometry}.
In other words an affine map does not need to map to the origin of the associated vector space, or is a linear map on vector spaces including translation, or in the words of my supervisor, C. F., an affine map is a Taylor series of first order.
For more information on affine spaces and maps we refer to the books \cite{berger2009geometry, katsumi1994affine}

To find the affine map, we generate two affine subspaces spaces \newline $V = \big\{ \bm{A}(\bm{x}^{(1)}, \bm{p,T}), \dots ,\bm{A}(\bm{x}^{(m)}, \bm{p,T})\big\} $ and $W = \big\{ \bm{A}_L\bm{x}^{(1)}, \dots ,\bm{A}_L\bm{x}^{(m)}\big\}$ over the same field, with fixed $\bm{p,T}$.
Then we can use a linear solver to find the affine map $\bm{M}$ so that we can approximate the non linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T}) \approx \bm{M} \bm{A}_L$ with a linear forward model, we go into further detail in section \ref{}.
The parameter $\bm{x}$ is distributed as the so-called posterior distribution $\big\{  \bm{x}^{(1)} , \dots, \bm{x}^{(m)} \big\} \sim \pi(\bm{x}|\bm{\theta},\bm{y})$ conditioned on the hyper-parameters $\bm{\theta}$, according to a Bayesian hierarchical model.`


\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[rectnode] at (2,-4) (NL)    {$W$};
		\node[rectnode] at (-2,-4) (L)    {$V$};
		\draw[<-, very thick] (NL.west) -- (L.east); 
		\node[align=center] at (-5.5,-4) (f3) {linear forward model};
		\node[align=center] at (5.5,-4) (f4) {non-linear forward model};
		\node[align=center] at (0,-5) (f5) {$\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}  \approx \bm{M A}_L  \bm{x} = \bm{A x} $ };
		\node[align=center] at (0,-4) (f5) {affine Map \\ $\bm{M}$};
	\end{tikzpicture}
	\caption[Schematics of Affine Map]{Schematics of Affine Map, which approximates the linear forward model to the non-linear forward model.}
\end{figure}

\section{Bayesian Inference}
In this this section we give a short introduction to Bayesian inference for a general parameter $\bm{x}$ given some data 
\begin{align}
	\bm{y} = \bm{Ax} + \bm{\eta} \, 
	\label{eq:LinDat}
\end{align}
based on a linear forward model $\bm{A}$ and some noise $ \bm{\eta}$.
Later in section \ref{} we set up a more sophisticated Bayesian framework applied to the forward model in section \ref{}.

We can visualise the correlation structure of a measurement process through a hierarchiallly ordered directed acyclic graph (DAG), see Figure \ref{fig:FirstDAG}.
As an observatory process naturally includes some random noise we include that in our DAG and classify the noise as a hyper-parameter in $\bm{\theta}$ \cite{fox2016fast}.
Other hyper-parameters influence the parmeters $\bm{x}$ detemernistaclly, which are then mapped through the forward model onto the space of all measurables $\bm{u}$, from which we observe some data $\bm{y}$ including noise as previously mentioned.
Drawing a DAG can help us to dependences within the measurement and modelling process.
Given some data we inferer the distribution of the underling parameters and hyper-parameters by following the arrows in Figure \ref{fig:FirstDAG} backwards and set up a Bayesian model, hierarchially ordered.
\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
	\node[roundnode2] at (0,3.5) (th)    {$\bm{\theta}$};
	\node[roundnode2] at (0,1.5) (x)    {$\bm{x}$};
	\node[roundnode2] at (0,-1.5) (u)    {$\bm{u}$};
	\node[rectnode] at (0,-3.5) (y)    {$\bm{y}$};
	
	\draw[->, very thick] (th.south) -- (x.north); 
	\draw[->, very thick, mydotted] (x.south) -- (u.north); 
	\draw[->, very thick] (u.south) -- (y.north); 
	\draw[->, very thick] (th) edge[bend right=60] (y);  
	
	\node[align=center] at (2.8,3.5) (tht) {$\sim \pi_{\bm{\theta}}(\cdot) $ hyper-parameters};
	\node[align=center] at (2.4,1.5) (xt) {$\sim \pi_{\bm{x}}(\cdot|\bm{\theta}) $ parameters};
	\node[align=center] at (2.25,0) (At) {$\bm{u} = \bm{Ax}$ noise free data};
	\node[align=center] at (3,-1.5) (ut) {space of all measurables};
	\node[align=center] at (2,-3.5) (yt) {$\sim \pi_{\bm{y}}(\cdot|\bm{\theta},\bm{u})$ data};
	\node[align=center] at (-3,0) (nt) {noise};
\end{tikzpicture}

	\caption[Bayesian Inference DAG]{The directed acyclic graph (DAG) for a linear inverse problem visualises statistical dependencies as solid line arrows and deterministic dependencies as dotted arrows.
	The parameters $\bm{x}$ have some statistical dependency of those hyper-parameters $\bm{\theta}$, which are distributed as $\pi(\bm{\theta})$. Then a parameter $\bm{x} \sim \pi_{\bm{x}}(\cdot|\bm{\theta})$ is mapped onto the space of all measurables $\bm{u}=\bm{Ax}$ deterministically through the linear forward model $\bm{A}$.
	From the space of all measurables we observe some data $\bm{y} = \bm{Ax} + \bm{\eta}$, statistically,  so that $\bm{y}\sim \pi_{\bm{y}}(\cdot|\bm{\theta},\bm{u})$ , with naturally some random noise $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$.}
	\label{fig:FirstDAG}
\end{figure}



Within a linear Bayesian hierarchial model we need to define a likelihood function as well as a distribution over the unknown parameters $\bm{x}$ and hyper-parameters $\bm{\theta}$.
\begin{subequations}
	\begin{align}
		\bm{y}|\bm{x}, \bm{\theta}&\sim \mathcal{N}(\bm{A} \bm{x}, \bm{\Sigma}(\bm{\theta})) \label{eq:likelihood}  \\
		\bm{x}| \bm{\theta} & \sim  \mathcal{N}( \bm{\mu}, \bm{Q}^{-1}(\bm{\theta})  ) \label{eq:xPrior} \\
		\bm{\theta} &\sim  \pi(\bm{\theta}) \label{eq:gammaPrior}\, .
	\end{align}
	\label{eq:BayMode}
\end{subequations}
Due to the assumption of Gaussian noise $\bm{\eta} \sim \mathcal{N}(0, \bm{\Sigma}(\bm{\theta})) $ the likelhood function $\pi(\bm{y}|\bm{x}, \bm{\theta})$, which includes information about the measurement process and gives a measure of how well parametersand  hyperparemters fit given some data, is normally distributed with the noise covariance matrix $\bm{\Sigma}(\bm{\theta})$. 
More specifically because we choose a normally distributed prior $\pi(\bm{x}| \bm{\theta})$ with the prior precision matrix $\bm{Q}(\bm{\theta})$ and prior mean $\bm{\mu}$ this is a linear-Gaussian Bayesian hierarchical model as in \cite{fox2016fast}, including some distribution over the hyper-parameters $\pi(\bm{\theta})$.
One of the main strengths of a Bayesian framework is that through those prior and hyper-prior distributions we can incorporate functional dependencies as well model physical properties of the parameters $\bm{x}$. 

Then Bayes Theorem gives the posterior distribution, the function of interest,
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) = \frac{ \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta})}{\pi(\bm{y})} \, ,
\end{align}
with the prior distribution $\pi(\bm{x}, \bm{\theta}) = \pi(\bm{x}| \bm{\theta}) \pi(\bm{\theta}) $ and the normalising constant $\pi(\bm{y})$.
If the normalising constant is finite and non-zero we can approximate the posterior distribution
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) \propto \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta}) \, .
\end{align}
and the expectation of any a function $h(\bm{x}_{\bm{\theta}})$, where $\bm{x}$ may be depending on $\bm{\theta}$, can be described as 
\begin{align}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}_{\bm{\theta}})] =  \int \int   h(\bm{x}_{\bm{\theta}}) \,  \pi(\bm{x}, \bm{\theta} | \bm{y} ) \, \text{d} \bm{x}  \, \text{d} \bm{\theta}   \label{eq:expPos} \, ,
\end{align}
which is usually a high dimensional integral and computationally not feasible to solve.


One way to work around the high dimensionality is to parameterise $\bm{x}$ using hyper-parameters $\bm{\theta}$ so that $\bm{x}_{\bm{\theta}}$. 
Another way is to seperate the posterior distribution over latent field $\bm{x}$ and the hyper-parameters $\bm{\theta}$.
This is particular benefitial, when $\bm{x}$ is high dimensional, e.g. $\bm{x} \in \mathbb{R}^n$ with $n = 45$ and can not be parametrised, and $\bm{\theta}$ is low dimensional, e.g. two dimensional.
Then by the law of total expectation \cite{champ2022generalizedlawtotalcovariance} eq. \ref{eq:expPos} becomes
\begin{align}
	\text{E}_{\bm{x}|\bm{y}} [h(\bm{x})] = \text{E}_{\bm{\theta}|\bm{y}}   \big[ \text{E}_{\bm{x}|\bm{\theta},\bm{y}} [h(\bm{x}_{\bm{\theta}})] \big]=  \int 	\text{E}_{\bm{x}| \bm{\theta},\bm{y}}  [h(\bm{x}_{\bm{\theta}})] \,  \pi( \bm{\theta} | \bm{y} )  \, \text{d} \bm{\theta}   \label{eq:MargExpPos} \, ,
\end{align}
where in the case of a linear-Gaussian Bayesian hierarchical model $\text{E}_{\bm{x}| \bm{\theta},\bm{y}}  [h(\bm{x}_{\bm{\theta}})]$ is well defined.
The marginal and then condtional method, does exactly that.
Why MTC sperate hiughky correlated \cite{}

Figure 4.1 in RUE and Held  see appemdix
Hight correlation bewten $\bm{\theta}$ and $\bm{x}$

\subsection{Marginal and then Conditional}
\label{subsec:MTC}
The marginal and then conditional (MTC) method factorises the full posterior distribution 
\begin{align}
	\pi(\bm{x}, \bm{\theta}|\bm{y}) = \pi(\bm{x}| \bm{\theta}, \bm{y}) \pi(\bm{\theta}|\bm{y})
\end{align}
into the marginal posterior distribution $ \pi(\bm{\theta}|\bm{y})$ and conditional posterior distribution $\pi(\bm{x}| \bm{\theta}, \bm{y})$.


For the in Eq. \ref{eq:BayMode} specified linear-Gaussian Bayesian hierarchical model the marginal posterior distribution is given as
\begin{align}
	\pi(\bm{\theta} | \bm{y}) &= \int \pi(\bm{x}, \bm{\theta} | \bm{y}) \diff \bm{x} \\ 
	\label{eq:condHyper}
	&\propto \sqrt{ \frac{ \det( \bm{\Sigma}^{-1} ) \,  \det( \bm{Q}) }{\det( \bm{Q} + \bm{A}^T \bm{\Sigma}^{-1} \bm{A} ) } } \times \exp \Big[ - \frac{1}{2}(\bm{y} -\bm{A \mu})^T \bm{Q}_{\bm{\theta|y}} (\bm{y} -\bm{A \mu}) \Big] \pi(\bm{\theta}) \, ,
\end{align}
with
\begin{align}
	\bm{Q}_{\bm{\theta|y}} = \bm{\Sigma}^{-1} - \bm{\Sigma}^{-1} \bm{A} (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \bm{A}^T \bm{\Sigma}^{-1} \,  ,
\end{align}
see Lemma 2 in \cite{fox2016fast}.
Conditioned on the hyper-parameters $\bm{\theta}$ we can draw samples from the conditional posterior distribution
\begin{align}
	\bm{x}| \bm{\theta}, \bm{y}  \sim \mathcal{N}\big( \underbrace{\bm{\mu} + (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1} \bm{A}^T \bm{\Sigma}^{-1} (\bm{y} - \bm{A} \bm{\mu})}_{\bm{\mu}_{\bm{x}|  \bm{\theta}, \bm{y}}} , \underbrace{ (\bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} )^{-1}}_{\bm{\Sigma}_{\bm{x}| \bm{y} , \bm{\theta}}} \big) \, 
\end{align}
using the Randomis then optimize (RTO) method, see section \ref{subsec:RTO},
or calculate weighted expectations e.g. $ \text{E}_{\bm{x}|\bm{\theta},\bm{y}} [\bm{x}| \bm{\theta}, \bm{y}] = \bm{\mu}_{\bm{x}| \bm{\theta}, \bm{y} }$ or use Eq. \ref{eq:MargExpPos} to calculate weighted expectations of $\text{E}_{\bm{x}|\bm{\theta},\bm{y}} [\text{Var}(\bm{x}| \bm{\theta}, \bm{y})]  = \bm{\Sigma}_{\bm{x}| \bm{y} , \bm{\theta}}$ with weights given by $\pi(\bm{\theta} | \bm{y} )$.
Note that the noise covariance $\bm{\Sigma}= \bm{\Sigma}( \bm{\theta}) $ and the prior precision $\bm{Q} = \bm{Q}( \bm{\theta})$ are depending on hyper-parameters $\bm{\theta}$.

\section{Regularisation}
\label{sec:reg}
Another method to find a solution to a linear inverse problem as in Eq. \ref{eq:LinDat} is to find a solution $\bm{x}_{\lambda}$ according to a data misfit norm and a regularisation semi-norm as in \cite{fox2016fast}.
We will discuss the case of Tikhonov regularisation \cite{kaipio2005statinv,tan2016LecNot} as this is the most similar to a linear-Gaussian hierarchial Bayesian model.

For a parameter $\bm{x}$ a linear forward model matrix $\bm{A}$ and some data $\bm{y}$ the data misfit norm
\begin{align}
	\left\lVert \bm{y} - \bm{A}\bm{x} \right\rVert \, 
\end{align}
gives a measure of how good data fits to a mapped parameter $\bm{Ax}$ and the regularisation semi norm 
\begin{align}
	\lambda \left\lVert \bm{T}\bm{x}  \right\rVert
\end{align}
penalises $\bm{x}$ according to $\bm{T}$ and the regularisation parameter $\lambda > 0$.
Given $\lambda$ a regularised solution
\begin{align}
	\bm{x}_{\lambda} = \underset{\bm{x}}{\text{arg min}} \left\lVert \bm{y} - \bm{A}\bm{x}  \right\rVert^2 + \lambda \left\lVert \bm{T}\bm{x} \right\rVert^2
\end{align}
can be found by the derivativ
\begin{align}
&  &\nabla_{\bm{x}} \big\{  (\bm{y} - \bm{A}\bm{x}_{\lambda} )^T  (\bm{y} - \bm{A}\bm{x}_{\lambda})  + \lambda \bm{x}_{\lambda}^T  \bm{T}^T \bm{T} \bm{x}_{\lambda} \big\} &= 0 \\
&\iff &\nabla_{\bm{x}} \big\{ \bm{y}^T \bm{y} + \bm{x}_{\lambda}^T \bm{A}^T  \bm{A}\bm{x}_{\lambda} - \bm{y}^T \bm{A}\bm{x}_{\lambda} - \bm{x}_{\lambda}^T \bm{A}^T   \bm{y}   + \lambda \bm{x}_{\lambda}^T  \bm{T}^T \bm{T} \bm{x}_{\lambda} \big\} & = 0  \\
& \iff& 2\bm{A}^T  \bm{A}\bm{x}_{\lambda} -2 \bm{A}^T   \bm{y}  + 2 \lambda   \bm{T}^T \bm{T} \bm{x}_{\lambda}  & = 0.
\end{align}
Then a regularised solution is given as:
\begin{align}
	(\bm{A}^T  \bm{A} + \lambda   \bm{L} )^{-1} \bm{A}^T   \bm{y}   = \bm{x}_{\lambda}, 
\end{align}
where we can set $\bm{T}^T \bm{T} = \bm{L}$, which is typically matrix approximation of the nth derivative \cite{tan2016LecNot}.
In practise $\bm{x}_{\lambda}$ is calculated for a range of $\lambda$, and is evaluated by the data-misfit norm with respect to the regularised semi-norm so that the best $\bm{x}_{\lambda}$ lays on the point of maximum curvature of a so-called L-Curve \cite{hansen1993use}, which we will show in section \ref{}.


\section{Sampling Methods}
In this chapter we present the sampling methods used in this thesis and also argue why we can use sampling methods to calculate the expectation
\begin{align}
	\label{eq:intMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}_{\bm{\theta}})] = \underbrace{ \int \int h(\bm{x}_{\bm{\theta}}) \pi(\bm{x},\bm{\theta}|\bm{y}) \, \text{d} \bm{\theta} \, \text{d} \bm{x} }_{\bm{\mu}_{\text{int}}} 
\end{align}
of a function  $h(\bm{x}_{\bm{\theta}})$ with respect to a probability density $\pi( \bm{x},\bm{\theta}|\bm{y})$.
In the case where the calculation of the integral is not feasible we can approximate Eq. \ref{eq:intMean} with a sample based estimate
\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}_{\bm{\theta}})] \approx \underbrace{ \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)}_{\bm{\theta}})  }_{\bm{\mu}_{\text{samp}}} \, ,
\end{align}
for large enough samples size $N$ of a sample set $\mathcal{M} = \{  (\bm{x}, \bm{\theta} )^{(1)}, \dots, (\bm{x}, \bm{\theta} )^{(k)} , \dots,  (\bm{x}, \bm{\theta})^{(N)}  \} \sim \pi(\bm{x},\bm{\theta}| \bm{y}) $.
The variance of this unbiased estimator is $\mathcal{O}(1/N)$ so large samples sizes can reduce the uncertainty of $\bm{\mu}_{samp}$ \cite{roberts2004general}.
%chapter 5 Ridley
We can do this as the central limit theorem states that the samples means $ \bm{\mu}^{(i)}_{samp} $, of samples sets $\mathcal{M}_i$ for $i = 1, \dots, n$ of any distribution , converge in distribution to a normal distribution so that
\begin{align}
	 \sqrt{n} (\bm{\mu}^{(i)}_{\text{samp}} -  \bm{\mu}_{\text{int}} ) \overset{\mathcal{D}}{\longrightarrow} \mathcal{N} (0,\sigma^2) \text{\cite{geyer1992practical}},
\end{align}
and if $\sigma^2 < \infty$ the error $\bm{\mu}^{(i)}_{\text{samp}} -  \bm{\mu}_{\text{int}} $ is bound.

Now, we have to show that the methods we use actually draw samples $ \mathcal{M} = \{ (\bm{x}, \bm{\theta} )^{(1)}, \dots, (\bm{x}, \bm{\theta} )^{(k)} , \dots,  (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x},\bm{\theta}| \bm{y}) $ from the desired target distribution so that we can use sample based estimates as in Eq. \ref{eq:sampMean}.
Here $ \mathcal{M}$ is a Markov-Chain, where each sample $ (\bm{x}, \bm{\theta})^{(k)}$ is only depending on the previous sample  $ (\bm{x}, \bm{\theta})^{(k-1)}$ \cite{}.
Markov-chain Monte Carlo methods generates such a chain $\mathcal{M}$ with random (Monte Carlo) proposals $(\bm{x}, \bm{\theta})^{(k)} \sim q( \cdot |(\bm{x}, \bm{\theta})^{(k-1)})$ according to a proposal distribution, where ergodicity of $\mathcal{M}$ is a sufficient criterium to use samples based estimates \cite{tan2016LecNot, roberts2004general}.
The ergodicy theorem in \cite{tan2016LecNot} states that, if an aperiodic and irreducible Markov chain $\mathcal{M}$ is reversible then it converges towards a stationary unique equilibirum distribution $\pi(\bm{x},\bm{\theta}| \bm{y}) $.
In other words if from any state in the chain we can reach any other state in the sampling space and the previous state, and we do not get stuck in periodic loop, then the chain converges towards a stationary distribution.
In practise one can look at the trace $\pi(\bm{x}^{(k)},\bm{\theta}^{(k)}| \bm{y}) $ for $k = 1, \dots, N$ of the samples and eyeball ergodicity.

The sampling methods used in this thesis have proven ergodic properties, so we will cite and refer the reader to the respective documents.

\subsection{Metropolis- within Gibbs sampling}

As introduced in section \ref{subsec:MTC} when using the MTC method will sample seperatly from $\pi(\bm{\theta}| \bm{y})$ and $\pi(\bm{x}|\bm{\theta}, \bm{y}) $.
To sample from $\pi(\bm{\theta}| \bm{y})$ we use a Metropolis-within-Gibbs sampler as in \cite{fox2016fast} and discuss the 2 dimesnional case, as used in this thesis, with $\bm{\theta}  =( \theta_1 , \theta_2) $, where we do a metroplis step in $\theta_1$ deirection and a gibbs stepp in $\theta_2$ direction.
Ergodicity is proven here \cite{roberts2006harris}.

The Metropolis-within-Gibbs algorithm starts with a initial guess $\bm{\theta}^{(t)}$ at $t=0$.
Then, we propose a new sample $\theta_1 \sim q( \theta_1 | \theta^{(t-1)}_1 )$ conditioned on the previous state according to a symmetric proposal distribution $q(\theta_1 | \theta^{(t-1)}_1 ) = q( \theta^{(t-1)}_1|\theta_1 ) $ , which is a special case of the Metropolis-Hastings algorithm \cite{roberts2006harris} and cancels when computing the acceptance probability $\alpha$. We accept and set $ \theta^{(t)}_1 = \theta_1$ with
\begin{align}
\alpha( \theta_1  | \theta_1^{(t-1)}) = \min \left\{ 1, \frac{\pi(\theta_1  | \theta^{(t-1)}_2, \bm{y}) q(\theta_1^{(t-1)} | \theta_1 )  }{\pi(\theta_1^{(t-1)}| \theta_2^{(t-1)}, \bm{y}) q(\theta_1 | \theta_1^{(t-1)})}  \right\}
\end{align}
or reject and keep $\theta^{(t)}_1 = \theta^{(t-1)}_1$, which we do by comparing $\alpha$ to a uniform random number $u \sim \mathcal{U}(0,1)$.
Next, we do a Gibbs step in $\theta_2$ direction, where Gibbs sampling is a special case of the Metropolis-Hastings algorithm with acceptance probaility of 1, and draw the next sample $\theta_2^{(t)} \sim  \pi( \cdot | \theta_1^{(t)} , \bm{y} ) $ conditioned on $\theta_1^{(t)}$ at step $t$.
We reapet this $N$ times and assure convergence independent of the intial sample (irreducibility) as we discard samples after the so-called burn-in period so that we produce a Markov-Chain of length $N - N_{\text{burn-in}}$.


\begin{algorithm}[!ht]
	\caption{Metropolis within Gibbs}
	\begin{algorithmic}[1]
		\STATE Initialize and suppose two dimensional vector \( \bm{\theta}^{(0)}  =( \theta_1^{(0)} , \theta_2^{(0)}  ) \)
		\FOR{ \( k = 1, \dots, N \)}
		\STATE Propose \( \theta_1 \sim q(\cdot   | \theta_1 ^{(t-1)}) = q(\theta_1 ^{(t-1)} |\cdot  ) \)
		\STATE Compute
		\[ \alpha( \theta_1  | \theta_1^{(t-1)}) = \min \left\{ 1, \frac{\pi(\theta_1  | \theta^{(t-1)}_2, \bm{y}) \cancel{q(\theta_1^{(t-1)} | \theta_1 ) } }{\pi(\theta_1^{(t-1)}| \theta_2^{(t-1)}, \bm{y}) \cancel{q(\theta_1 | \theta_1^{(t-1)})} } \right\} \]
		\STATE Draw $u \sim \mathcal{U}(0,1)$
		\IF{$\alpha \geq u$ }
		\STATE Accept and set \( \theta_1^{(t)} = \theta_1 \)
		\ELSE  
		\STATE Reject and keep \(\theta_1^{(t)} = \theta_1^{(t-1)} \)
		\ENDIF
		\STATE Draw \(\theta_2^{(t)} \sim  \pi( \cdot | \theta_1^{(t)} , \bm{y} )\) 
		\ENDFOR
		\STATE Output: $ \bm{\theta}^{(0)}, \dots,  \bm{\theta}^{(k)} , \dots,   \bm{\theta}^{(N)} \sim \pi(\bm{\theta}| \bm{y}) $
	\end{algorithmic}
\end{algorithm}




\subsection{Draw a sample from a multivariate normal distribution \label{subsec:RTO}}

after sampling from $\pi(\bm{\theta}| \bm{y})$ we draw samples from $\pi(\bm{x}|\bm{\theta}, \bm{y}) $ within the MTC scheme.
For Linear Gaussian Bayesian hierarchical model we can draw a sample $\bm{x}$ from the multivariate normal distribution$\pi(\bm{x}|\bm{\theta}, \bm{y})$  using the radomize then optimise (RTO) method \cite{bardsley2012mcmc}.

In doing so we can rewrite the full conditional normal distribution $\pi( \bm{x}| \bm{y} , \bm{\theta} )$ to:
\begin{align}
	\pi(\bm{x}|\bm{y}, \bm{\theta} ) &\propto \pi(\bm{y} | \bm{x} , \bm{\theta} ) \pi(\bm{x}| \bm{\theta}) \\
	&= \exp  \lVert \hat{\bm{A}} \bm{x} - \hat{\bm{y}} \rVert^2 \, ,
\end{align}
where 
\begin{align}
	\label{eq:minimizer}
	\hat{\bm{A}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta})  \bm{A}\\
		\bm{Q}^{1/2}(\bm{\theta}) 
	\end{bmatrix} \, , \quad \hat{\bm{y}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta})  \bm{y} \\
		\bm{Q}^{1/2}(\bm{\theta}) \bm{\mu}
	\end{bmatrix} \, \text{\cite{bardsley2014randomize}}.
\end{align}

Then one sample can be computed by minimising the following equation with respect to $\hat{\bm{x}}$ :
\begin{align}
	\bm{x}_i = \arg \min_{\hat{\bm{x}}} \lVert \hat{\bm{A}} \hat{\bm{x}} - ( \hat{\bm{y}} + \bm{b} ) \rVert^2 , \quad \bm{b} \sim \mathcal{N}(\bm{0}, \mathbf{I}) \, ,
\end{align}
where we add a randomised perturbation $\bm{b}$.
Similarly as in section \ref{sec:reg} we can rewrite the argument of Eq. \ref{eq:minimizer} to 
\begin{align}
	\label{eq:RTO}
	(\bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A}+
	\bm{Q}(\bm{\theta}) ) \bm{x}_i &= \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{y} +  \bm{Q}(\bm{\theta}) \bm{\mu} + \bm{v}_1 + \bm{v}_2 \,  ,
\end{align}
where we substitute $ - \hat{\bm{A}}^T  \bm{b}  = \bm{v}_1 + \bm{v}_2$ so that $\bm{v}_1 \sim \mathcal{N}(\bm{0}, \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A}) $ and $\bm{v}_2 \sim \mathcal{N}(\bm{0}, \bm{Q}(\bm{\theta}) )$ are independent random variables \cite{bardsley2012mcmc, fox2016fast}.


If within the MTC scheme the Markov chain from the marginal posterior is ergodic then with independent samples $\bm{x}^{(k)}$ from the full conditional $\pi(\bm{x}|\bm{\theta}, \bm{y})$ the combined chain $\{  (\bm{x}, \bm{\theta} )^{(1)}, \dots, (\bm{x}, \bm{\theta} )^{(k)} , \dots,  (\bm{x}, \bm{\theta})^{(N)}  \} \sim \pi(\bm{x},\bm{\theta}| \bm{y})$ is ergodic as well \cite{acosta2014markov}.

\subsection{t-walk}
If there is a functional dependency of the parameters $\bm{x}$ and the hyper-parameters $\bm{\theta}$ so that $\bm{x}(\bm{\theta})$ we can use the t-walk algorithm by Christens and Fox on $\pi(\bm{\theta}|\bm{y} )$.
We use the t-walk as a black box sampler, where convergence is guaranteed by construction \cite{christen2010general}.



\section{Numerical Approxiamtion Methods - Tensor Train}
First we will derive how we find marginal probality distribution funcitons and then give a brief introduction into tensor train format.
Asumme that the triple $(\Omega, \mathcal{F} , \mathbb{P})$ is called a probability space over the whole sample (or parameter) space $\Omega$ with the collection $\mathcal{F}$ of very countable subset $\{ A _n \}_{n\in \mathbb{N}}$ in $\Omega$, so that $A_n \subseteq  \Omega$, and $\mathbb{P}$ is a measure on $\mathcal{F}$. 
We assume $\mathbb{P}$ fullfils the requirements of a probality measure and $\mathcal{F}$ is a $\sigma $-algebra see Appendix \ref{ch:Mesure}.
We call $\mathbb{P}(A)$ the probaility of an event $A \subseteq \mathcal{F}$ 
\begin{align}
	\mathbb{P}(A) = \int_A \diff \mathbb{P} \text{\cite{}}.
\end{align}
We change variables using the Radon-Nikodym theroem \cite{kopp2004measintprob}
\begin{align}
	\mathbb{P}(A) = \int_{A}  \frac{\diff \mathbb{P}}{\diff x} \diff x
\end{align}
where $\diff x$ is a measure on the same probality space, also known as the lebesgue measure and $\frac{\diff \mathbb{P}}{\diff x}$ is often called Radon-Nikodym derivativ of $\mathbb{P}$ with repect to $x$.
We can say that $\mathbb{P}$ has a density $ \pi(x)$, with repect to $x$ \cite[Chapter 10]{simonnet1996measprob}.
Next, we can define $x: \Omega \longrightarrow X$ as a random variable connecting to the measurable space $(X,\mathcal{X})$ through the probability density function $ \pi(x)$ \cite{kopp2004measintprob}.
For a $d$-dimensional $x$ we call the cartesioan product $\mathcal{X} = \mathcal{X}_1 \times \mathcal{X}_2 \times \dots \times \mathcal{X}_d$ the parameter space in $X$, which we set to $\mathbb{R}^d$, so that ervey subset $\mathcal{X}_k \subseteq \mathbb{R}$ for $k = 1, \dots , d$.
Next we intrdouce a weight function $\uplambda(x) $ \cite{davis2007methods}, which can be helpful for qraudrature rules, so that
\begin{align}
	\mathbb{P}(A) = \int_{A} \uplambda(x)   \pi(x) \diff x \, ,
\end{align}
where Cui et al. \cite{cui2022deep} call $\uplambda(x)$ the ' product-from Lebsegue measurable weighting functions" and define it as $\uplambda( \mathcal{X}) = \prod_{i = 1}^{d}  \uplambda_i( \mathcal{X}_i) $ and  $\uplambda_i( \mathcal{X}_i) = \int_{ \mathcal{X}_i} \uplambda_i (x_i) \text{d}x_i$.
The marginal function
\begin{align}
	f_{X_k}(x_k) = \int_{\mathcal{X}_1} \cdots \int_{\mathcal{X}_d} \uplambda(x) \pi(x) \diff x_1 \cdots \diff x_{k-1} \diff x_{k+1} \cdots \diff x_d \, ,
\end{align}
of $x_k \in \mathcal{X}_k$, as a realization/event in $X_k \subseteq X$ is calculated by integrating the probility desnity funciton $\pi(x)$ over the parameter space in all other dimensions.

Using the tensor train (TT) format we can approximate $d$-dimensional function $\pi(x)$ and compute marginal probailty functions distributions cheaply.
In doing so we have to define a d-dimesnional discrete univariate grid in the parameter space $\mathcal{X}$ with $n$ grid points in each direction.
Then, as the name suggest the tensor train format is a train of tensors which represent this d-dimesnional grid.
More specifically each tensor is a two and three dimensional matrix, which we call core, $\pi_{k} \in \mathbb{R}^{r_{k-1} \times n \times r_{k}}$ and connected with the neighbouring tensor through ranks $r_{k}$ and $r_{k-1}$ for each $k = 1, \dots , d$, as in Figure \ref{fig:TTfig} displayed.
For the first and last dimensional the core outer ranks are $r_0  = r_d = 1$, so that for $x = {x_1, \dots, x_d}$ the function value $\pi(x) = \big( \pi_1(x_1) \dots \pi_d(x_d) \big)\in \mathbb{R}$ is a vector-matrix-vector multiplication and each core $\pi_k(x_k)$ at a fixed $x_k$ on the approximated grid has dimensions $r_{k-1} \times 1 \times r_{k} =r_{k-1} \times r_{k}$.


\begin{figure}[ht!]
	\centering
\begin{subfigure}{\textwidth}
	\input{TTSchem.pdf_tex}
	%\includegraphics[width = \textwidth]{Figures/TTSchem.png}
	\caption{}
\end{subfigure}
	\centering
\begin{subfigure}{\textwidth}
\begin{tikzpicture} 
	\node[rectnode] at (-5,0) (T1)    {$1 \times n \times r_1$};
	\node[rectnode] at (-2,0) (T2)    {$r_1 \times n \times r_{2}$};
	
	\node[rectnode] at (3,0) (Tn1)    {$r_{d-2} \times n \times r_{d-1} $};
	\node[rectnode] at (6.75,0) (Tn)    {$r_{d-1} \times n \times 1$};
	\draw[-, very thick] (T1.east) -- (T2.west); 
	\draw[-, very thick] (Tn.west) -- (Tn1.east); 
	\draw[-, mydotted, very thick] (T2.east) -- (Tn1.west);
	
	\node[align=center] at (-3.5,0.25) (R) {$r_1$};
	\node[align=center] at (5,0.25) (R) {$r_{d-1}$};	
\end{tikzpicture} 
	\caption{}
\end{subfigure}
\caption[Visualization of Tensor Train cores]{Here we visualize the tensor train cores as two and three dimesnional matrices. Each matrix has a length $n$ according to the gridsize and ther cores are connected through ranks $r$. More specifiaclly a core $\pi_k$ has dimesnions $r_{k-1} \times  n \times r_k$, where $r_0 = r_d = 1$. Figure (a) is taken from \cite{fox2021grid}.}
\label{fig:TTfig}
\end{figure}


Consequently, using a tensor train approxiamtion, the marginal target function
\begin{align}
	\begin{split}
	f_{X_k}(x_k) = \frac{1}{z}
	\Big|  &\left( \int_{\mathbb{R}} \uplambda_1(x_1) \pi_{1}(x_1)\text{d}x_{1} \right) \cdots \left( \int_{\mathbb{R}} \uplambda_{k-1}(x_{k-1}) \pi_{k-1}(x_{k-1}) \text{d}x_{k-1} \right) \\ & \qquad \uplambda_k(x_{k}) \pi_{k}(x_k) \\ &\quad \left( \int_{\mathbb{R}} \uplambda_{k+1}(x_{k+1}) \pi_{k+1}(x_{k+1})\text{d}x_{k+1} \right) \cdots  \left( \int_{\mathbb{R}} \uplambda_d(x_{d}) \pi_{d}(x_d)\text{d}x_d \right) \Big| \, 
	\end{split} 
\end{align}  
is given by integration over each core \cite{dolgov2020approximation} including some normalising constant $z$ \cite{cui2022deep}.
\\


%%%%%% Squared and Basis Function %%%%%
From here we follow the notation and procedure mostly from Cui et al. \cite{cui2022deep} and approximate the square root
\begin{align}
	\sqrt{\pi(x)} \approx g(x) = \bm{G}_1(x_1), \dots ,  \bm{G}_k(x_k), \dots ,  \bm{G}_d(x_d)
\end{align}
for numerical stability, where each TT-core
\begin{equation}
	G^{(\alpha_{k-1},\alpha_k)}_k(x_k) = \sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{A}_k[\alpha_{k-1}, i, \alpha_k], \quad k = 1, ..., d,
\end{equation}
is associated $k$th coefficient tensor $\bm{A}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ and the $k$-th  basis functions $\phi^{(i)}_k(x_k)$.
We assume the function
\begin{align}
	\pi(x) \approx \gamma^{\prime} + g^2(x) \, ,
\end{align} 
is approximated through the TT decomposition $g(x)$, where the error $\gamma^{\prime}$ assures positivity and is chosen according to the $L_2$ norm 
\begin{align}
	\gamma^{\prime} \leq \frac{1}{\uplambda( \mathcal{X})} \lVert g - \sqrt{\pi}\rVert^2_2 \, .
\end{align}
Then the normalised target function is
\begin{align}
	f_X(x) = \frac{1}{z} \pi(x) \uplambda(x) = \frac{1}{z} ( \gamma^{\prime} \uplambda(x) + g^2(x) \uplambda(x)) \, .
\end{align} 
Given the tensor train approximation of the sqared rooted function $\sqrt{\pi}$ can be expressed as
\begin{align}
	\begin{split}
		f_{X_k}(x_k) = \frac{1}{z}  \Bigg( \gamma^{\prime}& \prod_{i=1}^{k-1} \uplambda_i(\mathcal{X}_i) \prod_{i=k+1}^{d} \uplambda_i(\mathcal{X}_i) \\&+  \left( \int_{\mathbb{R}} \bm{G}^2_{1}(x_1) \uplambda_1(x_1)\text{d}x_{1} \right) \cdots \left( \int_{\mathbb{R}} \bm{G}^2_{k-1}(x_{k-1}) \uplambda_{k-1}(x_{k-1}) \text{d}x_{k-1} \right) \\ & \bm{G}^2_{k}(x_k)\uplambda_k(x_{k})\\ & \left( \int_{\mathbb{R}}  \bm{G}^2_{k+1}(x_{k+1})\uplambda_{k+1}(x_{k+1})\text{d}x_{k+1} \right) \cdots  \left( \int_{\mathbb{R}} \bm{G}^2_{d}(x_d)\uplambda_d(x_{d})\text{d}x_d \right) \Bigg) \, .
	\end{split} 
\end{align}

To efficiently calculate these marginals on can use a procedure similar to something that is called left and right orthogonalisation of cores \cite{oseledets2011tensor}.
To do so we define the mass matrix $\bm{M}_k \in \mathbb{R}^{n_k \times n_k}$ by
\begin{equation}
	\bm{M}_k[i, j] = \int_{X_k} \phi^{(i)}_k(x_k) \phi^{(j)}_k(x_k)  \uplambda(x_k) \,dx_k, \quad i = 1, ..., n_k, \quad j = 1, ..., n_k,
\end{equation}
where $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ is the set of basis functions for the $k$-th coordinate.
%In the case of cartesian basis with $\uplambda= 1$ M is the identity matrix, which we need to compute the marginals.
%For a boundend Parameter space we consider the weihtin funcitn $\uplambda(x) = 1$




\subsection{Marginal Functions}
We calculate the marginal functions through procedures, which we call backward marginalisation \cite{cui2022deep} and forward marginalisation.
We gain the coefficient matrices $\bm{B}_k$ through backward marginalisation and the coefficient matrices $\bm{B}_{pre,n}$ through forward marginalisation, which enables us to calculate marginal function similar to \cite{cui2022deep}.
The proposition \ref{prob:backMarg} to caculte $\bm{B}_k$  is taken from \cite{cui2022deep}.
\begin{prop}[Backward Marginalisation]
	\label{prob:backMarg}
	Starting with the last coordinate $k = d$, we set $\bm{B}_d = \bm{A}_d$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{k-1} \in \mathbb{R}^{r_{k-2} \times n_{k-1} \times r_{k-1}}$, which we need for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_k[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{B}_k[\alpha_{k-1}, i, l_k] \bm{L}_k[i, \tau].
		\end{equation}
		\item Unfold $\bm{C}_k$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_k^{(R)} \in \mathbb{R}^{r_{k-1} \times (n_k r_k)}$:
		\begin{equation}
			\bm{Q}_k \bm{R}_k = {(\bm{C}_k^{(R)})}^{\top}.
		\end{equation}
		\item Compute the new coefficient tensor:
		\begin{equation}
			\bm{B}_{k-1}[\alpha_{k-2}, i, l_{k-1}] = \sum_{\alpha_{k-1}=1}^{r_{k-1}} \bm{A}_{k-1}[\alpha_{k-2}, i, \alpha_{k-1}] \bm{R}_k[l_{k-1}, \alpha_{k-1}].
		\end{equation}
	\end{enumerate}
\end{prop}
We start the forward marginalisation with the first dimension as in Proposition \ref{prob:ForMarg}.
\begin{prop}[Forward Marginalistaion]
	\label{prob:ForMarg}
	Starting with the first coordinate $k = 1$, we set $\bm{B}_{pre,1} = \bm{A}_1$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{pre,k+1} \in \mathbb{R}^{r_{k} \times n_{k+1} \times r_{k+1}}$ for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_{pre,k}[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{L}_k[i, \tau] \bm{B}_{pre,k}[\alpha_{k-1}, i, l_k] .
		\end{equation}
		\item Unfold $\bm{C}_{pre,k}$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_{pre,k}^{(R)} \in \mathbb{R}^{(r_{k-1} n_k ) \times r_k}$:
		\begin{equation}
			\bm{Q}_{pre,k}\bm{R}_{pre,k} = {(\bm{C}_{pre,k}^{(R)})}.
		\end{equation}
		\item Compute the new coefficient tensor $\bm{B}_{pre, k+1} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k} $:
		\begin{equation}
			\bm{B}_{pre, k+1}[l_{k+1}, i, \alpha_{k+1}] = \sum_{\alpha_{k}=1}^{r_{k}} \bm{R}_{pre,k}[l_{k+1}, \alpha_{k}] \bm{A}_{k+1}[\alpha_{k}, i, \alpha_{k+1}] .
		\end{equation}
	\end{enumerate}
\end{prop}

The marginal PDF of $X_{k}$ can be expressed as
\begin{equation}
	f_{X_k}(x_k) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=1}^{k-1} \uplambda_i(X_i) \prod_{i=k+1}^{d} \uplambda_i(X_i) + \sum_{l_{k-1}=1}^{r_{k-1}} \sum_{l_k=1}^{r_k} \left(\sum_{i=1}^{n} \phi^{(i)}_k(x_k) \bm{D}_k[l_{k-1},i, l_k] \right)^2 \right) \uplambda_k(x_k),
\end{equation}
where $\bm{D}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ and $\bm{R}_{pre,k-1}\in \mathbb{R}^{r_{k-1} \times r_{k-1}}$ and $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$
\begin{equation}
	\bm{D}_k[l_{k-1},i,l_k] = \sum_{\alpha_{k-1}=1}^{r_{k-1}}  \bm{R}_{pre,k-1}[l_{k-1}, \alpha_{k-1}] \bm{B}_k[\alpha_{k-1}, i, l_k].
\end{equation}



In the special case for the first dimension, $f_{X_1}(x_1)$ can be expressed as
\begin{equation}
	f_{X_1}(x_1) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=2}^{d} \uplambda_i(\mathcal{X}_i) + \sum_{l_1=1}^{r_1} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_1[i, l_1] \right)^2 \right) \uplambda_1(x_1),
\end{equation}
where $\bm{D}_1[i, l_1] = \bm{B}_1[\alpha_0, i, l_1]$ and $\alpha_0 = 1$,
and similiarly in the last dimesnion
\begin{equation}
	f_{X_d}(x_d) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=1}^{d-1} \uplambda_i(\mathcal{X}_i) + \sum_{l_{n-1}=1}^{r_{d-1}} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_d[l_{n-1},i] \right)^2 \right) \uplambda_d(x_d),
\end{equation}
where $\bm{D}_d[l_{n-1},i] = \bm{B}_{pre,d}[l_{n-1}, i, \alpha_{n+1}]$ and $\alpha_{d+1} = 1$.


%he dx refers to integration with respect to Lebesgue measure, which is “uniform measure”, λ((a, b)) = b−a (here λ(A) is the Lebesgue measure of A, applied to A = (a, b)).