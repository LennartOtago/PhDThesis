%\the\columnwidth
\chapter{Theoretical and Technical Background}
\label{ch:background}
In this Chapter, we give an introduction into the hierarchical Bayesian approach to inverse problems, some fundamentals of Markov chain Monte Carlo (MCMC) methods, and tensor-trains approximations of high-dimensional probability distributions.
We keep it as general as possible and will not introduce specific sampling algorithms, as those are tailored towards the forward map and the applied problem and hence will be presented when used.


\section{Hierarchical Bayesian Inference}
\label{sec:bayes}
\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[roundnode2] at (0,3.5) (th)    {$\bm{\theta}$};
		\node[roundnode2] at (0,1.5) (x)    {$\bm{x}$};
		\node[roundnode2] at (0,-1.5) (u)    {$\Omega$};
		\node[rectnode] at (0,-3.5) (y)    {$\bm{y}$};
		
		\draw[->, very thick] (th.south) -- (x.north); 
		\draw[->, very thick, mydotted] (x.south) -- (u.north); 
		\draw[->, very thick] (u.south) -- (y.north); 
		\draw[->, very thick] (th) edge[bend right=60] (y);  
		
		\node[align=center] at (2.8,3.5) (tht) {$\sim \pi_{\bm{\theta}}(\cdot) $ hyper-parameters};
		\node[align=center] at (2.4,1.5) (xt) {$\sim \pi_{\bm{x}}(\cdot|\bm{\theta}) $ parameters};
		\node[align=center] at (2.5,0) (At) {$\{\bm{A}(\bm{x})\}$ ``noise-free data''};
		\node[align=center] at (3,-1.5) (ut) {space of all measurables};
		\node[align=center] at (2,-3.5) (yt) {$\sim \pi_{\bm{y}}(\cdot|\bm{x},\bm{\theta})$ data};
		\node[align=center] at (-3.25,0) (nt) {noise\\$\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$};
	\end{tikzpicture}
	\caption[Hierarchical Bayesian Model]{A directed acyclic graph (DAG) for an inverse problem visualises statistical dependencies as solid line arrows and deterministic dependencies as dotted arrows.
		The hyper-parameters $\bm{\theta}$ are distributed as ($\sim$) the hyper-prior distribution $\pi(\bm{\theta})$.
		The prior distribution $ \pi_{\bm{x}}(\cdot|\bm{\theta})$ for the parameter $\bm{x}$ and the noise  $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$ are statistically dependent on some of those hyper-parameters.
		Then a parameter $\bm{x} \sim \pi_{\bm{x}}(\cdot|\bm{\theta})$ is deterministically mapped onto the space of all measurables $\Omega=\bm{A}(\bm{x})$ through the forward model.
		From the space of all measurable noise-free data we observe (square box) a data set $\bm{y} = \bm{A}(\bm{x}) + \bm{\eta}$ with some additive random noise, which determines the likelihood function $\pi(\bm{y}|\bm{x},\bm{\theta})$.}
	\label{fig:FirstDAG}
\end{figure}

Assume we observe some data
\begin{align}
	\bm{y} = \bm{A} (\bm{x}) + \bm{\eta},
	\label{eq:NonLinDat}
\end{align}
based on a forward model $\bm{A}(\bm{x})$, which may be non-linear, an unknown parameter vector $\bm{x}$ and some additive random noise $\bm{\eta}$.

Naturally, due to the noise, the observation process in Eq. \ref{eq:NonLinDat} is a random process.
Hence, in Bayesian modelling, the aim is to determine a probability distribution over the parameter $\bm{x}$ given some data $\bm{y}$.
Further, a hierarchical Bayesian model incorporates unknown (auxiliary) hyper-parameters $\bm{\theta}$, and treats hyper-parameters and parameters as random variables \cite[Chapter 3]{kaipio2005statinv}.

According to Bayes' theorem, the joint posterior distribution over the parameters $\bm{x}$ and the hyper-parameter $\bm{\theta}$ is given as
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) = \frac{ \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta})}{\pi(\bm{y})} \propto \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta}) \, ,
\end{align}
with finite and non-zero $\pi(\bm{y})$.
The likelihood function $\pi(\bm{y}|\bm{x},\bm{\theta})$ is defined by the nature of the noise and the noise-free data $\bm{A}(\bm{x})$, which we read as the distribution over $\bm{y}$ conditioned on $\bm{x}$ and $\bm{\theta}$.
Here $\bm{\theta}$ may account for multiple hyper-parameters, e.g. modelling the noise vector $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$, where $\sim$ reads as ``is distributed as'', and describing physical properties or functional dependencies of $\bm{x}$ such as the smoothness of $\bm{x}$.
Consequently $\bm{\theta}$ is described by the hyper-prior distribution $\pi(\bm{\theta})$, where $\pi(\bm{x}, \bm{\theta}) = \pi(\bm{x}|\bm{\theta}) \pi(\bm{\theta})$ with the prior distribution $\pi(\bm{x}|\bm{\theta})$.
Choosing these prior distributions is ultimately a modeller's choice and is crucial, as those shall be as uninformative as possible for regions in hyper-parameter and parameter space where the data is informative.
If the data is uninformative, the prior distributions can be informative and represent a rather restrictive range of (physically) feasible hyper-parameters and parameters.

Figure~\ref{fig:FirstDAG} visualises the conditional dependencies between hyper-parameters and parameters as well as how distributions progress through to a observation (square box) using a directed acyclic graph (DAG).
We plot statistical dependencies as solid arrows and deterministic dependencies as dotted arrows.
%\textcolor{red}{conditional dependencies and sqaure box is observation}

% not affect the posterior distribution \textcolor{red}{what, of course the prior affects the posterior. What are you trying to say?}

%$\pi(\bm{x},\bm{\theta}|\bm{y}) $ reads as the distribution of $\bm{x}$ and $\bm{\theta}$ given (conditioned on) the data $\bm{y}$. \textcolor{red}{two sentences, talk anbout posterior and then model }

%Here $\bm{\theta}$ models the random noise $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$, 
%The h
%This already incorporates hierarchically ordered modelling as we model the noise thought the hyper-parameters $\bm{\theta}$.
%which we classify through a hyper-parameter, we deal with a random process. \textcolor{red}{Maybe more clear is to say that the observation process (2.1) is a random process.}
%We incorporate that in our hierarchically ordered modelling through the likelihood function $\pi(\bm{y}|\bm{x},\bm{\theta})$, which includes all relevant information captured by the model $\bm{A}(\bm{x})$, and is defined by the measurement process and the nature of the noise. \textcolor{red}{how does the model "capture information"? Oh, do you mean the observation process? Might pay to not use 'model' for lots of things.}
%
%Consequently we define hyper-prior distributions $\pi(\bm{\theta})$ and prior distributions $\pi(\bm{x}|\bm{\theta})$.


%Note that here we include the hyper-parameters within the posterior distribution, which is the key idea of hierarchical Bayesian modelling. \textcolor{red}{"the" key idea? Maybe say that it is intrinsic to Bayes models, or something. You could quote Kaipio and Somersalo that all unknowns are treated as random variables.} \cite[Chapter 3]{kaipio2005statinv}
%


Usually, the objective is to calculate the expectation of a function $h(\bm{x})$, which is defined as
\begin{align}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})] =  \underbrace{\int \int   h(\bm{x}) \,  \pi(\bm{x}, \bm{\theta} | \bm{y} ) \, \diff \bm{x}  \, \diff \bm{\theta}}_{\bar{h}}   \label{eq:expPos} \, .
\end{align}
If that is a high-dimensional integral and computationally not feasible to solve, we approximate 
\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})] \approx \underbrace{ \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)})  }_{\bar{h}_N} \, ,
\end{align}
with an the unbiased sample-based Monte-Carlo estimate \cite{roberts2004general} for large enough $N$ (law of large numbers \cite[Chapter 17]{tweedie2009measprob}).
Here, the posterior samples $\{\bm{x}^{(k)},\bm{\theta}^{(k)} \}\sim \pi_{\bm{x}, \bm{\theta}}(\cdot|\bm{y})$, for $k = 1, \dots, N$, form a sample set $\mathcal{M} =\{ (\bm{x},\bm{\theta})^{(1)}, \dots ,  (\bm{x},\bm{\theta})^{(N)} \}$.
The central limit theorem states that the sample mean $\bar{h}^{(i)}_N $ of independent sample sets $\mathcal{M}^{(i)}\sim\pi (\bm{x}, \bm{\theta}| \bm{y})$, for $i = 1, \dots, n$ from a distribution, converges to be normally distributed, so that
\begin{align}
	\sqrt{n} (\bar{h}^{(i)}_N -  \bar{h} ) \overset{\mathcal{D}}{\longrightarrow} \mathcal{N} (0,\sigma^2) \text{\cite{geyer1992practical}},
\end{align}
and if $\sigma^2 < \infty$ the Monte-Carlo error $\bar{h}^{(i)}_N -  \bar{h} $ is bounded.
In practice, we approximate the Monte-Carlo error from a sample set $\mathcal{M}^{(i)}$ as
\begin{align}
	(\sigma^{(i)})^2  =  \text{Var}(\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})]) 
	\approx \frac{\text{Var}(h(\bm{x}) )}{N} \Bigg( \underbrace{  1 + 2 \sum_{t = 1}^{W} \frac{\Gamma(t)}{\Gamma(0)}  }_{ \coloneqq 	\tau_{\text{int}} }\Bigg) = \text{Var}(h(\bm{x})) \frac{ \tau_{\text{int}} }{N} \, , \label{eq:MCerr}
\end{align}
where we have to take into account that the samples generated by any system or algorithm are correlated.
We define the integrated autocorrelation time (IACT) $\tau_{\text{int}}$ as in \cite{fox2016fast}, which is twice the value of the IACT in~\cite[pp. 103-105]{wolff2002LecNot}.
Here the autocorrelation coefficient $\Gamma(t) \propto \exp\{ - |t| /\tau \} \longrightarrow 0$ for $t \rightarrow \infty$ at lag $t$ decays exponentially and $\Gamma(0) =\text{Var}(h(\bm{x}) ) $.
Choosing the summation window $W$ is crucial because it has to be large compared to the decay time $\tau$, but for too large $t$ the autocorrelation coefficient $\Gamma(t)$ is noise-dominated.
U. Wolff \cite{wolff2004monte} (and the Python implementation by D. Hesse \cite{drikHesse}) provide a way to not only calculate the IACT safely but also to quantify the errors of the estimated IACT.

The IACT provides a good estimate of the number of steps the sampling algorithm needs to take to produce one independent sample.
According The IACT, we define the effective sample size as $ \tau_{\text{int}} /N$.
We point out that for uncorrelated samples $\tau_{\text{int}} = 1$ the error $(\sigma^{(i)})^2$ is a typical Monte-Carlo estimate.
See Appendix~\ref{ap:IATC} and~\cite{Sokal1997, wolff2004monte, wolff2002LecNot} for a more detailed derivation.

\subsubsection{Ergodicity}

In this section we present the methods that draw samples from the marginal posterior $ \mathcal{M} = \{  \bm{\theta}^{(1)}, \dots,  \bm{\theta}^{(k)}, \dots, \bm{\theta}^{(N)} \} \sim \pi(\bm{\theta} |  \bm{y})$ \textcolor{red}{this reads as if M is the marginal posterior. Rewrite.} as well as the RTO method to draw samples from a normally distributed full conditional posterior $\pi(\bm{x}|\bm{\theta} , \bm{y})$. \textcolor{red}{what is the "full posterior" as opposed to the "posterior"?}
Here, $\mathcal{M}$ denotes a Markov chain, where each new sample $\bm{\theta}^{(k)}$ is only affected by the previous one $\bm{\theta}^{(k-1)}$.
MCMC methods generate such a chain $\mathcal{M}$ using random (Monte-Carlo) proposals $(\bm{x}, \bm{\theta})^{(k)} \sim q( \cdot |  \bm{\theta}^{(k-1)})$ \textcolor{red}{This is defining "Markov". . Be more explicit.} according to a proposal distribution conditioned on the previous sample (Markov), where ergodicity of the chain $\mathcal{M}$ is a sufficient criterion for using sample-based estimates~\cite{tan2016LecNot, roberts2004general}. \textcolor{red}{you are a little unclear about a chain of random variables, and an instance of such a chain.}

The ergodicity theorem in~\cite{tan2016LecNot} states that, if a Markov chain $\mathcal{M}$ is aperiodic, irreducible, and reversible, then it converges to a unique stationary equilibrium distribution.
In other words, the chain can reach any state from any other state (irreducibility), is not stuck in periodic cycles (aperiodicity), and satisfies the detailed balance condition~\cite{tan2016LecNot} (reversibility).
Then the samples in that chain $ \mathcal{M} \sim \pi( \bm{\theta} |  \bm{y})$ are samples from the desired target distribution.
In practice, one can inspect the trace $\pi(\bm{\theta}^{(k)} |  \bm{y})$ for $k = 1, \dots, N$ and visually assess convergence and mixing properties of the chain to evaluate ergodicity. \textcolor{red}{converge to, so after 'burn in'}
The sampling methods used in this thesis possess proven ergodic properties, and we therefore refer the reader to the corresponding literature for further details. \textcolor{red}{this is a bit loose. really, one evaluates if the behaviour is consistent with ergodicity.}

If the Markov chain over the marginal posterior $\pi(\bm{\theta} |  \bm{y})$ is ergodic \textcolor{red}{reference Jun Liu, or something.}, and the full conditional samples $\bm{x}^{(k)} \sim \pi(\bm{x}|   \bm{\theta}^{(k)}, \bm{y})$ are drawn independently, as e.g. in Sec.~\ref{subsec:RTO}, then the resulting joint chain $\{ (\bm{x}, \bm{\theta})^{(1)}, \dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y}) =  \pi(\bm{x} |  \bm{\theta} , \bm{y}) \pi( \bm{\theta} | \bm{y})$ is also ergodic \textcolor{red}{for the posterior}~\cite{acosta2022markov}. \textcolor{red}{strictly, ergodic for [theta|y] -- ergodicity is with respect to a particular distribution.}





\subsection{Marginal and then Conditional Method}
\label{subsec:TheoMTC}
Quickly generating a representative sample set from the posterior distribution often presents a significant challenge. This is mainly \textcolor{red}{this word is misplaced (for English). Better is Quickly generating (it's an adverb)} due to the strong correlations that usually exist between the parameters and hyper-parameters, as discussed by Rue and Held in~\cite{rue2005gaussian} and illustrated in Appendix~\ref{ap:Correlation}.
If $\bm{x}$ cannot be parametrised directly in terms of the hyper-parameters $\bm{\theta}$, so that $\bm{x}(\bm{\theta})$ is function of $\bm{\theta}$, it is beneficial to factorise the posterior distribution as \textcolor{red}{this is unclear. Of course x(theta) is a function of theta. What are you trying to say?}
\begin{align}
	\pi(\bm{x}, \bm{\theta} |  \bm{y}) = \pi(\bm{x} |  \bm{\theta}, \bm{y}) \, \pi(\bm{\theta} |   \bm{y}), \label{eq:MTC}
\end{align}
into the full conditional posterior $\pi(\bm{x} |  \bm{\theta}, \bm{y})$ over the latent field $\bm{x}$ and the marginal posterior over the hyper-parameter $\bm{\theta}$, that can be calculated by,
\begin{align}
	\pi(\bm{\theta} |   \bm{y}) =  \frac{ \pi(   \bm{y} | \bm{x},\bm{\theta})  \pi( \bm{x} | \bm{\theta} )  \pi(\bm{\theta}) }{ \pi(\bm{x} | \bm{\theta} ,   \bm{y})   \pi( \bm{y})} \propto \frac{ \pi(   \bm{y} | \bm{x},\bm{\theta})  \pi( \bm{x} | \bm{\theta} )  \pi(\bm{\theta}) }{ \pi(\bm{x} | \bm{\theta} ,   \bm{y}) } \label{eq:margGen}\, ,
\end{align}
as in~\cite[Lemma 2]{fox2016fast}.
%In \cite{norton2018sampling}, they classify inverse problems into problems with known or unknown conditional posterior distributions, and conclude that if $\pi(\bm{x} | \bm{\theta} ,   \bm{y}) = \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}| \bm{\theta})  / \pi(   \bm{y}| \bm{\theta})$ has a known form, the normalising constant of $\pi(\bm{\theta} |   \bm{y})$ is available \newline $\int \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}| \bm{\theta}) \text{d} \bm{x} = \pi(   \bm{y}| \bm{\theta})  \propto \pi( \bm{\theta}|\bm{y}) / \pi(\bm{\theta})$ and one can almost surely determine the $\bm{\theta}$-dependence of the marginal posterior $\pi(\bm{\theta} |   \bm{y})$.

This approach, known as the MTC method, is particularly advantageous when $\bm{x}\in \mathbb{R}^n$ is high-dimensional, while $\bm{\theta}\in \mathbb{R}^{n_{\bm{\theta}}}$ is low-dimensional, so that $n_{\bm{\theta}} \ll n$ and the evaluation of $\pi(\bm{\theta}| \bm{y})$ is relatively cheap. \textcolor{red}{horrible notation, try better.}
Applying the law of total expectation~\cite{champ2022generalizedlawtotalcovariance}, Eq.~\eqref{eq:expPos} becomes
\begin{align}
	\mathbb{E}_{\bm{x} ,\bm{\theta}  |\bm{y}} [h(\bm{x})] &= \int \int   h(\bm{x}) \pi(\bm{x} |  \bm{\theta}, \bm{y}) \, \diff \bm{x} \,  \pi(\bm{\theta} |   \bm{y}) \, \diff \bm{\theta} \\
	&= \int \mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} \left[ h(\bm{x}) \right] \, \pi(\bm{\theta} |  \bm{y}) \, \diff \bm{\theta}\label{eq:2fullCond} \\
		&= \mathbb{E}_{\bm{\theta} |  \bm{y}} \left[ \mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} [h(\bm{x})] \right] \, .
	\label{eq:fullCond}
\end{align}
In the case of a linear-Gaussian hierarchical Bayesian model, both the marginal distribution $\pi (\bm{\theta}| \bm{y})$ %\textcolor{red}{separate sentence (You have a tendency to let sentences wander to different ideas. Split into single topics.)} 
and the inner expectation $\mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} \left[ h(\bm{x}) \right]$ are well defined (see next subsection).
If the integral in Eq.~\ref{eq:2fullCond} is expensive to calculate, we use sample-based methods to produce a Markov chain $\{ (\bm{x}, \bm{\theta})^{(1)}, \dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y}) $ and sample from $\pi(\bm{\theta} |  \bm{y})$ first and then draw samples from the full conditional posterior $\pi(\bm{x} | \bm{\theta} , \bm{y})$, e.g via the RTO method (see Sec.~\ref{subsec:RTO}). 


\subsubsection{Linear-Gaussian hierarchical Bayesian model}
\label{subsec:LinBay}
In case of normally distributed noise $\bm{\eta} \sim \mathcal{N}(\bm{0},\bm{\Sigma}(\bm{\theta}))$ with zero mean and covariance $\bm{\Sigma}(\bm{\theta})$ and a linear forward model matrix $\bm{A}$ the Eq.~\ref{eq:NonLinDat} simplifies to
\begin{align}
	\bm{y} = \bm{A} \bm{x} + \bm{\eta} \, .
	\label{eq:LinDat}
\end{align}
Then we can obtain the marginal and full conditional posterior distribution explicitly.
Our hierarchical linear-Gaussian Bayesian model is defined as
\begin{subequations}
	\label{eq:GenBayMode}
	\begin{align}
		\bm{y} |  \bm{x}, \bm{\theta} &\sim \mathcal{N}(\bm{A} \bm{x}, \bm{\Sigma}(\bm{\theta}) ) \\
		\bm{x} |  \bm{\theta} &\sim \mathcal{N}(\bm{\mu}, \bm{Q}(\bm{\theta})^{-1} ) \\
		\bm{\theta} &\sim \pi(\bm{\theta}) \,  ,
	\end{align}
\end{subequations}
with a Gaussian likelihood function $\pi(\bm{y} | \bm{x}, \bm{\theta} )$, a normally distributed prior $\pi(\bm{x}|\bm{\theta})$, with prior mean $\bm{\mu}$ and prior precision $\bm{Q}(\bm{\theta})$, and a hyper-prior distribution $\pi(\bm{\theta})$.
For derivation of the marginal posterior and the full conditional posterior distribution, consider the joint multivariate Gaussian distribution
\begin{align}
	\begin{pmatrix}
		\bm{x} \\
		\bm{y}
	\end{pmatrix}\sim \mathcal{N}\left[  \begin{pmatrix}
		\bm{\mu} \\
		\bm{A}\bm{\mu}
	\end{pmatrix},\begin{pmatrix}
		\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A} & - \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \\
		\bm{\Sigma}(\bm{\theta})^{-1} \bm{A} & \bm{\Sigma}(\bm{\theta})^{-1} 
	\end{pmatrix}^{-1} \right] \, , 	\label{eq:jointMultiGaus}
\end{align}
with the joint precision matrix as in~\cite{SIMPSON201216} (see also~\cite{rue2005gaussian, fox2016fast}).
Immediately, the full conditional posterior distribution can be formulated as 
\begin{align}
	\bm{x} | \bm{\theta} , \bm{y}\sim \mathcal{N}\big(\underbrace{ \bm{\mu} + (\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})^{-1}(\bm{y} - \bm{A}\bm{\mu})}_{\bm{\mu}_{\bm{x} | \bm{\theta} , \bm{y}}},\underbrace{ (\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})^{-1}}_{\bm{\Sigma}_{\bm{x} | \bm{\theta} , \bm{y}}} \big)\, \label{eq:CondPostLin}.
\end{align}
Then the marginal posterior distribution over the hyper-parameters is derived as in Eq. \ref{eq:margGen}, where, as noted by Fox and Norton~\cite{fox2016fast}, the parameter $\bm{x}$ cancels and we arrive at
\begin{align}\begin{split}
		\pi(\bm{\theta} | \bm{y}) \propto & \sqrt{\frac{\det{(\bm{\Sigma}(\bm{\theta})^{-1})} \det{(\bm{Q}(\bm{\theta}))} }{\det{(\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})} } }  \exp \Bigg\{  -\frac{1}{2} (\bm{y} - \bm{A} \bm{\mu})^T \\ &\big[ \bm{\Sigma}(\bm{\theta})^{-1} - \bm{\Sigma}(\bm{\theta})^{-1} \bm{A}  (\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})^{-1} \bm{A}^T \bm{\Sigma} (\bm{\theta})^{-1} \big] (\bm{y} - \bm{A} \bm{\mu}) \Bigg\} \pi(\bm{\theta}) \, .
	\end{split} 
\end{align} 
Having the marginal posterior distribution $\pi (\bm{\theta}| \bm{y})$ (independent of $\bm{x}$) available breaks up the correlation structure between $\bm{x}$ and $\bm{\theta}$ and makes the MTC approach very efficient~\cite{fox2016fast} (see Appendix~\ref{ap:Correlation}).
Within this scheme, we evaluate the marginal posterior first and then either condition on hyper-parameters to draw full conditional posterior samples $\bm{x} \sim \pi (\bm{x} | \bm{\theta}, \bm{y})$ (see Sec.~\ref{subsec:RTO}) or evaluate the mean
\begin{align}
	\mu_{\bm{x}|\bm{y}} = \int \bm{\mu}_{\bm{x} | \bm{\theta} , \bm{y}} \pi(\bm{\theta}| \bm{y}) \diff \bm{\theta} \label{eq:MeanGenInt}
\end{align} and covariance matrix
\begin{align}
	\Sigma_{\bm{x}|\bm{y}} = \int \bm{\Sigma}_{\bm{x} | \bm{\theta} , \bm{y}} \pi(\bm{\theta}| \bm{y}) \diff \bm{\theta}\label{eq:CoVarGenInt}
\end{align}
of the posterior $\pi(\bm{x}| \bm{y})$ by some quadrature rule.


\section{Numerical Approximation -- Tensor-Train (TT)}
\label{sec:tensortrain}
%\textcolor{red}{Explain how to find normalisation constant and say that due to aproxiamting the sqaure root we ensure posivyt later when squaring it}
%First, we provide a short overview of probability spaces and their associated measures. 
%I am not claiming this overview to be complete but consider it helpful to understand the notation in \cite{cui2022deep}, which we follow to approximate functions in the TT format.
Instead of relying on sampling-based methods to explore a target distribution $\pi(\bm{x})$, we can approximate this distribution on a $d$-dimensional grid using a tensor-train (TT) approximation $\tilde{\pi}(\bm{x}) \approx \pi(\bm{x})$, where $\bm{x} \in \mathbb{R}^d$, with far fewer function evaluation compared to conventional sampling methods.
In the following, we describe how to compute marginal distributions from a probability density approximated in TT \textcolor{red}{ (tensor train)} format and how to generate samples using the inverse Rosenblatt transform (IRT) as in \cite{dolgov2020approximation}, following the notation and procedure introduced in~\cite{cui2022deep}.
\textcolor{red}{We represent a probability distribution on a d-dimensional grid with grid points n in each dimension.
I'm surprised that you never cite the Approximation an Sampling paper that introduced the idea of TT approximation of PDFs. Oh, you do, but the author list is abbreviated. Don't abbreviate the author list fro a thesis.}
As in~\cite{cui2022deep}, we can define the parameter space as the product space $\mathcal{X} = \mathcal{X}_1 \times \mathcal{X}_2 \times \dots \times \mathcal{X}_d$ with $ x_k \in \mathcal{X}_k \subseteq \mathbb{R}$.
% and $\bm{x} = ( x_1,\dots ,x_k,\dots,x_d )$.
The marginal density function for the $k$-th component is then given by
\begin{align}
	f_{X_k}(x_k) = \frac{1}{z} \int_{\mathcal{X}_1} \cdots \int_{\mathcal{X}_{k-1}} \int_{\mathcal{X}_{k+1}} \cdots  \int_{\mathcal{X}_d} \uplambda(\bm{x}) \, \pi(\bm{x}) \, \diff x_1 \cdots \diff x_{k-1} \, \diff x_{k+1} \cdots \diff x_d, \label{eq:margInt}
\end{align}
where we integrate over all dimensions except the $k$-th, and $z$ is a normalisation constant.
Here, Cui and Dolgov~\cite{cui2022deep} refer to $\uplambda(x)$ as the ``product-form Lebesgue-measurable weighting function'', which can be useful for quadrature rules~\cite{davis2007methods}, and define it as
\begin{align}
	\uplambda(\mathcal{X}) = \prod_{i = 1}^{d} \uplambda_i(\mathcal{X}_i), \quad \text{where} \quad \uplambda_i(\mathcal{X}_i) = \int_{\mathcal{X}_i} \uplambda_i(x_i) \, \diff x_i. \label{eq:lebesgueWeight}
\end{align}

In the TT format, the integral in Eq.~\ref{eq:margInt} for the marginal probability can be computed at a low computational cost, as $\pi(\bm{x})$ is approximated by
\begin{align*}
	\tilde{\pi}(\bm{x}) = 	\tilde{\pi}_1(x_1)  \tilde{\pi}_2(x_2)  \cdots \tilde{\pi}_d(x_d),
\end{align*}
which is a sequence of matrix multiplications with $\tilde{\pi}_k(x_k) \in \mathbb{R}^{r_{k-1} \times r_k}$ for a fixed point $\bm{x} = (x_1, \dots, x_d)$ on a predefined $d$-dimensional discrete univariate grid over the parameter space $\mathcal{X}$. 
A TT core  $\tilde{\pi}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ has ranks $ r_{k-1}$ and $r_k$, which are connecting the TT core to its respective neighbouring dimensions.
A TT train has outer ranks $r_0 = r_d = 1$.  \textcolor{red}{more sentence creep. You should have defined the univariate grids, earlier.}
This enables us to approximate $\pi(\mathcal{X})\approx \tilde{\pi}_1  \tilde{\pi}_2  \cdots \tilde{\pi}_d$ over a discrete parameter space $\mathcal{X}$ using $2nr + (d-2)nr^2$ evaluation points for fixed ranks $r =r_{k-1} = r_k $, as illustrated in Figure~\ref{fig:TTfig}, instead of $n^d$ function evaluation.
Consequently, the marginal target distribution
\begin{align}
	\begin{split}
		f_{X_k}(x_k) \approx \frac{1}{z} \Big|\, 
		&\left( \int_{\mathcal{X}_1} \uplambda_1(x_1) \tilde{\pi}_1(x_1) \, \diff x_1 \right) \cdots 
		\left( \int_{\mathcal{X}_{k-1}} \uplambda_{k-1}(x_{k-1}) \tilde{\pi}_{k-1}(x_{k-1}) \, \diff x_{k-1} \right) \\
		&\quad \uplambda_k(x_k) \tilde{\pi}_k(x_k) \\
		& \left( \int_{\mathcal{X}_{k+1}} \uplambda_{k+1}(x_{k+1}) \tilde{\pi}_{k+1}(x_{k+1}) \,\diff x_{k+1} \right) \cdots 
		\left( \int_{\mathcal{X}_{d}} \uplambda_d(x_d) \tilde{\pi}_d(x_d) \, \diff x_d \right)
		\Big| 
	\end{split}
\end{align}
is computed by integrating over all TT cores except the $k$-th core $\pi_k$, as in~\cite{dolgov2020approximation}, and normalised by the constant $z$~\cite{cui2022deep}.

\begin{figure}[ht!]
	\centering
\begin{subfigure}{\textwidth}
	\input{TTSchem.pdf_tex}
	\caption{}
\end{subfigure}
	\centering
\begin{subfigure}{\textwidth}
\begin{tikzpicture} 
	\node[rectnode] at (-5,0) (T1)    {$1 \times n \times r_1$};
	\node[rectnode] at (-2,0) (T2)    {$r_1 \times n \times r_{2}$};
	
	\node[rectnode] at (3,0) (Tn1)    {$r_{d-2} \times n \times r_{d-1} $};
	\node[rectnode] at (6.75,0) (Tn)    {$r_{d-1} \times n \times 1$};
	\draw[-, very thick] (T1.east) -- (T2.west); 
	\draw[-, very thick] (Tn.west) -- (Tn1.east); 
	\draw[-, mydotted, very thick] (T2.east) -- (Tn1.west);
	
	\node[align=center] at (-3.5,0.25) (R) {$r_1$};
	\node[align=center] at (5,0.25) (R) {$r_{d-1}$};	
\end{tikzpicture} 
	\caption{}
\end{subfigure}
\caption[Visualisation of a tensor train]{Here, we visualise the TT cores as a train of two- and three-dimensional matrices. 
Each core has a length $n$, corresponding to the number of grid points in each dimension, and the cores are connected through ranks $r_k$. 
More specifically, a core $\tilde{\pi}_k$ has dimensions $r_{k-1} \times n \times r_k$, with outer ranks $r_0 = r_d = 1$.
Using the TT format enables us to represent a $d$-dimensional grid with only $2nr + (d-2)nr^2$ evaluation points instead of $n^d$ grid points.
Figure~(a) is adapted from~\cite{fox2021grid}.}
\label{fig:TTfig}
\end{figure}


%%%%%% Squared and Basis Function %%%%%
In practice, TT approximations may suffer from numerical instability, in particular because it is not advantageous yet to approximate the target function $\pi(\bm{x})$ in e.g. the logarithmic space. 
Hence, Cui et al.~\cite{cui2022deep} approximate the square root of the probability density \textcolor{red}{Thsi sentence needs rewriting - -it's a jumble of ideas.
	Hence implies an implication -- is it that or just one possible resolution?}
\begin{align}
	\sqrt{\pi(\bm{x})} \approx \tilde{g}(\bm{x}) = \bm{G}_1(x_1), \dots, \bm{G}_k(x_k), \dots, \bm{G}_d(x_d)\, \,  \text{\cite[Eq. 18]{cui2022deep}},
\end{align}
which ensures positivity.
%and intuitively ``reduces the range of numbers'' (excuse this unmathematical expression).
Here, each TT core is given by
\begin{equation}
	G^{(\alpha_{k-1},\alpha_k)}_k(x_k) = \sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{A}_k[\alpha_{k-1}, i, \alpha_k], \quad k = 1, \dots, d,\, \,  \text{\cite[Eq. 21]{cui2022deep}},
\end{equation}
where $\bm{A}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ is the $k$-th coefficient tensor and $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ are the basis functions corresponding to the $k$-th coordinate.
The approximated unnormalised density is written as: \textcolor{red}{put the reference in the text. Immediately prior is probably best.}
\begin{align}
	\pi(\bm{x}) \approx \xi + \tilde{g}(\bm{x})^2\, \,  \text{\cite[Eq. 19]{cui2022deep}},
\end{align}
where $\xi$ is a positive constant added according to the ratio of the Lebesgue weighted L2-norm error and the Lebesgue weighting (see Eq.~\ref{eq:lebesgueWeight}) such that
\begin{align}
	0 \leq \xi \leq \frac{1}{\uplambda(\mathcal{X})} \lVert \tilde{g} - \sqrt{\pi} \rVert_{L^2_{\uplambda}(\mathcal{X})}^2\, \,  \text{\cite[Eq. 35]{cui2022deep}}. \label{eq:gamErr}
\end{align}
This leads to the normalised target function
\begin{align}
	f_X(\bm{x})  \approx \frac{1}{z} \left( \uplambda(\bm{x}) \xi  + \uplambda(\bm{x}) \tilde{g}(\bm{x})^2 \right)\, \,  \text{\cite[Eq. 19]{cui2022deep}},
\end{align}
with the normalisation constant $z = \int_{\mathcal{X}} f_X(\bm{x}) \diff \bm{x} $.
Given the tensor train approximation of $\sqrt{\pi}$, the marginal function $f_{X_k}(x_k)$ can be expressed as
\begin{align}
	\begin{split}
		f_{X_k}(x_k)  \approx \frac{1}{z} \Bigg(&\xi \prod_{i=1}^{k-1} \uplambda_i(\mathcal{X}_i) \prod_{i=k+1}^{d} \uplambda_i(\mathcal{X}_i) \\
		&+ \left( \int_{\mathcal{X}_1} \uplambda_1(x_1) \bm{G}_1^2(x_1)  \, \diff x_1 \right) \cdots 
		\left( \int_{\mathcal{X}_{k-1}} \uplambda_{k-1}(x_{k-1}) \bm{G}_{k-1}^2(x_{k-1}) \, \diff x_{k-1} \right) \\
		& \uplambda_k(x_k) \bm{G}_k^2(x_k)  \\
		&\left( \int_{\mathcal{X}_{k+1}} \uplambda_{k+1}(x_{k+1}) \bm{G}_{k+1}^2(x_{k+1})  \,\diff x_{k+1} \right) \cdots 
		\left( \int_{\mathcal{X}_d} \uplambda_d(x_d) \bm{G}_d^2(x_d)  \, \diff x_d \right) \Bigg).
	\end{split}
\end{align}




\subsection{Marginal Probability Distributions}
\label{subsec:TTMarg}
We compute the marginal probability distributions by a procedure to which Cui et al.~\cite{cui2022deep} refer to as backwards marginalisation, see Prop.~\ref{prob:ForMarg}, and to which we add the forward marginalisation, see Prop.~\ref{prob:backMarg}. \textcolor{red}{By now you have refered to multiple marginal distributions, so you'll need to be more clear.}
This is similar to the left and right orthogonalisation of TT \textcolor{red}{why abbreviate?} cores~\cite{oseledets2011tensor, Oseledets2011DMRG}.
The backwards marginalisation provides us with the coefficient matrices $\bm{B}_k$, while the forward marginalisation gives the coefficient matrices $\bm{R}_{\text{pre}, k}$. 
These matrices enable the efficient evaluation of marginal functions since they integrate over the coordinates either left or right of the $k$-th dimension, as in~\cite{cui2022deep}. \textcolor{red}{are they marginal functions or marginal distributions. Pick a language and stick to it.}
In doing so, the mass matrix $\bm{M}_k \in \mathbb{R}^{n_k \times n_k}$ is defined as
\begin{equation}
	\bm{M}_k[i, j] = \int_{\mathcal{X}_k} \phi^{(i)}_k(x_k) \phi^{(j)}_k(x_k) \uplambda(x_k) \, \diff x_k, \quad i, j = 1, \dots, n_k, \, \,  \text{\cite[Eq. 22]{cui2022deep}} \label{eq:MassMat},
\end{equation}
where $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ denotes the set of basis functions for the $k$-th coordinate.
The proposition used to compute $\bm{B}_k$, stated in Prop.~\ref{prob:backMarg}, is adapted directly from~\cite{cui2022deep}.

After computing the coefficient tensors $\bm{R}_{\text{pre},k-1}$ as in Prop.~\ref{prob:ForMarg} and $\bm{B}_{k}$ from Prop.~\ref{prob:backMarg}, the marginal PDF of $k$-th dimension can be expressed as
\begin{equation}
	f_{X_k}(x_k)  \approx \frac{1}{z} \left(\xi \prod_{i=1}^{k-1} \uplambda_i(X_i) \prod_{i=k+1}^{d} \uplambda_i(X_i) + \sum_{l_{k-1}=1}^{r_{k-1}} \sum_{l_k=1}^{r_k} \left(\sum_{i=1}^{n} \phi^{(i)}_k(x_k) \bm{D}_k[l_{k-1},i, l_k] \right)^2 \right) \uplambda_k(x_k), \label{eq:MargTT}
\end{equation}
where $\bm{D}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ and given as
\begin{equation}
	\bm{D}_k[l_{k-1},i,l_k] = \sum_{\alpha_{k-1}=1}^{r_{k-1}}  \bm{R}_{\text{pre},k-1}[l_{k-1}, \alpha_{k-1}] \bm{B}_k[\alpha_{k-1}, i, l_k] \, ,
\end{equation}
with $\bm{R}_{\text{pre},k-1}\in \mathbb{R}^{r_{k-1} \times r_{k-1}}$ and $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$.

For the first dimension, $f_{X_1}(x_1)$ can be expressed as
\begin{equation}
	f_{X_1}(x_1)  \approx \frac{1}{z} \left(\xi \prod_{i=2}^{d} \uplambda_i(\mathcal{X}_i) + \sum_{l_1=1}^{r_1} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_1[i, l_1] \right)^2 \right) \uplambda_1(x_1)\, \,  \text{\cite[Eq. 30]{cui2022deep}},
	\label{eq:firstMarg}
\end{equation}
where $\bm{D}_1[i, l_1] = \bm{B}_1[\alpha_0, i, l_1]$ and $\alpha_0 = 1$,
and similarly in the last dimension
\begin{equation}
	f_{X_d}(x_d)  \approx \frac{1}{z} \left(\xi \prod_{i=1}^{d-1} \uplambda_i(\mathcal{X}_i) + \sum_{l_{d-1}=1}^{r_{d-1}} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_d[l_{d-1},i] \right)^2 \right) \uplambda_d(x_d),
\end{equation}
where $\bm{D}_d[l_{d-1},i] = \bm{B}_{\text{pre},d}[l_{d-1}, i, \alpha_{d+1}]$ and $\alpha_{d+1} = 1$.
Note that in practice we calculate $z$ numerically within the process of computing the marginals so that  $\sum f_{X_k}(x_k) =1 $ and for Cartesian basis $\bm{M}_k = \text{diag}(\uplambda_k(\mathcal{X}_k))$ where we set $\uplambda(x) = 1$.

\begin{prop}[Backwards Marginalisation as in~\cite{cui2022deep}]
	\label{prob:backMarg}
	Starting with the last coordinate $k = d$, we set $\bm{B}_d = \bm{A}_d$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{k} \in \mathbb{R}^{r_{k-1} \times n_{k} \times r_{k}}$, which we need for defining the marginal function $f_{X_k}(x_k)$ or to draw samples from $\tilde{\pi}(\bm{x})$ via the squared IRT scheme (see Alg. Box~\ref{alg:SIRT}):
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_k[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{B}_k[\alpha_{k-1}, i, l_k] \bm{L}_k[i, \tau] \text{\cite[Eq. (27)]{cui2022deep}}. \label{eq:constrCBack}
		\end{equation}
		\item Unfold $\bm{C}_k$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_k^{(R)} \in \mathbb{R}^{r_{k-1} \times (n_k r_k)}$:
		\begin{equation}
			\bm{Q}_k \bm{R}_k = {(\bm{C}_k^{(R)})}^{\top} \, \,  \text{\cite[Eq. 28]{cui2022deep}}.\label{eq:thinQRBack}
		\end{equation}
		\item Compute the new coefficient tensor:
		\begin{equation}
			\bm{B}_{k-1}[\alpha_{k-2}, i, l_{k-1}] = \sum_{\alpha_{k-1}=1}^{r_{k-1}} \bm{A}_{k-1}[\alpha_{k-2}, i, \alpha_{k-1}] \bm{R}_k[l_{k-1}, \alpha_{k-1}]\, \,  \text{\cite[Eq. 29]{cui2022deep}}. \label{eq:nextCoeffTBack}
		\end{equation}
	\end{enumerate}
\end{prop}

\begin{prop}[Forward Marginalisation]
	\label{prob:ForMarg}
	Starting with the first coordinate $k = 1$, we set $\bm{B}_{\text{pre},1} = \bm{A}_1$. The following procedure can be used to obtain $\bm{R}_{\text{pre},k-1} \in \mathbb{R}^{r_{k-1}  \times r_{k-1}}$ for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_{\text{pre},k}[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{L}_k[i, \tau] \bm{B}_{\text{pre},k}[\alpha_{k-1}, i, l_k] .\label{eq:constrCForw}
		\end{equation}
		\item Unfold $\bm{C}_{\text{pre},k}$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_{\text{pre},k}^{(R)} \in \mathbb{R}^{(r_{k-1} n_k ) \times r_k}$:
		\begin{equation}
			\bm{Q}_{pre,k}\bm{R}_{\text{pre},k} = {(\bm{C}_{\text{pre},k}^{(R)})}.\label{eq:thinQRForw}
		\end{equation}
		\item Compute the new coefficient tensor $\bm{B}_{\text{pre}, k+1} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k} $:
		\begin{equation}
			\bm{B}_{\text{pre}, k+1}[l_{k+1}, i, \alpha_{k+1}] = \sum_{\alpha_{k}=1}^{r_{k}} \bm{R}_{\text{pre},k}[l_{k+1}, \alpha_{k}] \bm{A}_{k+1}[\alpha_{k}, i, \alpha_{k+1}] .\label{eq:nextCoeffTForw}
		\end{equation}
	\end{enumerate}
\end{prop}


\subsection{Sampling from a TT Approximation}
\label{subsec:SamplTT}
Instead of evaluating marginal functions for quadrature, we can draw samples from the approximated function in the TT format via the inverse Rosenblatt transform, as in \cite{dolgov2020approximation}.
Since the square root of the target function is approximated, Cui and Dolgov~\cite{cui2022deep} call that the squared inverse Rosenblatt transform (SIRT).

\begin{algorithm}[!th]
	\caption{Squared Inverse Rosenblatt Transform (SIRT)}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} seeds $\{ \bm{u}^{(1)},\dots, \bm{u}^{(N)} \} \sim \mathcal{U}(0,1)^d $ and $\bm{B}_1 , \dots,\bm{B}_d$  from Prop.~\ref{prob:backMarg}
		\FOR{ \( s = 1, \dots, N\)}
		\FOR{ \( k = 1, \dots, d\)}
		\STATE compute normalised PDF $ f_{X_k|X_{<k}}(x_k|x^{(s)}_{k-1},\dots,x^{(s)}_1)$, Eq.~\ref{eq:CurrMarg}
		\STATE compute cumulative distribution function $F_{X_k|X_{<k}}(x_k)$, Eq.~\ref{eq:CurrCDF},
		\STATE project sample $x^{(s)}_k = F_{X_k|X_{<k}}^{-1}(u^{(s)}_k)$
		\STATE interpolate $\bm{G}_k(x^{(s)}_k)$, Eq. \ref{eq:LinPol}
		\STATE update $\bm{G}_{\leq k}(x^{(s)}_{\leq k}) = \bm{G}_{<k}(x^{(s)}_{<k}) \bm{G}_k(x^{(s)}_k)$
		\ENDFOR
		\ENDFOR
		\STATE \textbf{Output:} samples $\{ \bm{x}^{(1)},\dots, \bm{x}^{(N)} \} $, where each $\bm{x}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N$
	\end{algorithmic}
	\label{alg:SIRT}
\end{algorithm}
Given the Backward marginals $\bm{B}_1 , \dots,\bm{B}_d$ as in Prop.~\ref{prob:backMarg}, draw $N$ uniformly distributed seeds $\{ \bm{u}^{(1)},\dots, \bm{u}^{(N)} \} \sim \mathcal{U}(0,1)^d $, where each $\bm{u}^{(s)}$ is $d$-dimensional for $s = 1, \dots, N$.
Then calculate the first marginal $f_{X_1}(x_1)$ as in Eq.~\ref{eq:firstMarg} and normalise with $z = \int_{\mathcal{X}_1} f_{X_1}(x_1) d x_1$. \textcolor{red}{OK, another task -- remove all but one 'we' per page, and even then you will have 100, which is too many. FYI, I counted 397 occurrences of 'we' in this text -- so you only have to rewrite 297 sentences! Holy Toledo, here's one to zap. and another. Is this a novel about Lennart's adventures in MCMC for 10 year olds?}
Next, compute the cumulative distribution function (CDF) $F_{X_1}(x_k) = \int^{x_k}_{-\infty} f_{X_1}(\hat{x}_1) d \hat{x}_1$, which for the general case is given as:
\begin{align}
	F_{X_k|X_{<k}}(x_k) = \int_{-\infty}^{x_k} f_{X_k|X_{<k}}(\hat{x}_k|x_{k-1},\dots,x_1) \diff \hat{x}_k  \, \, \text{\cite[Eq. 17]{cui2022deep}} \, ;
	\label{eq:CurrCDF}
\end{align}
Now project the seed $u^{(s)}_k$ on the parameter space, and generate the sample $x^{(s)}_k = F_{X_k|X_{<k}}^{-1}(u^{(s)}_k)$. \textcolor{red}{Argghhhh!}
The ``conditional marginal'' is given as:
\begin{align}\begin{split} 
		f_{X_k|X_{<k}}(x_k|x^{(s)}_{k-1},\dots,x^{(s)}_1) \approx \frac{1}{z}
		\Bigg( 
		\xi \prod_{i=k+1}^{d} \uplambda_i(X_i) +&  \\
		\sum_{l_{k} = 1}^{r_{k}} \Bigg( \sum_{i = 1}^{n}  \phi^{(i)}_k(x^{(s)}_k) \bigg( \sum_{\alpha_{k-1} = 1}^{r_{k-1}} \bm{G}^{(\alpha_{k-1})}_{<k}(x^{(s)}_{<k}) &\bm{B}_k[\alpha_{k-1},i,l_k] \bigg) \Bigg)^2 \Bigg) \uplambda_k(x_k) \, \,  \text{\cite[Eq. 31]{cui2022deep}},
	\end{split} 
	\label{eq:CurrMarg} 
\end{align}
where we marginalise over the dimensions $k+1 , \dots, d$ via $\bm{B}_k$ and condition on the previous $k-1$ samples through the product $\bm{G}_k(x^{(s)}_k)\in \mathbb{R}^{1 \times r_{k-1}}$ to preserve the correlation structure. \textcolor{red}{God dammit, another pesky we.}
Function values between grid points $i$ and $i+1$ are approximated with a piecewise polynomial interpolation
\begin{align}
	\bm{G}_k(x^{(s)}_k) \approx   \frac{x^{(s)}_k - x^{(i)}_k }{x^{(i+1)}_k -x^{(i)}_k } \bm{G}_k(x^{(i+1)}_k) + \frac{ x^{(i+1)}_k - x^{(s)}_k}{x^{(i+1)}_k -x^{(i)}_k } \bm{G}_k(x^{(i)}_k) \, ,
	\label{eq:LinPol}
\end{align}
for $x^{(i)}_k \leq x^{(s)}_k \leq x^{(i+1)}_k$ as in~\cite{dolgov2020approximation} for the next ``conditional marginal''.

The procedure is repeated for each $u^{(s)}_k \in \bm{u}^{(s)}$ to produce the samples $\bm{x}^{(s)} \sim f_{X}(\bm{x})$, as summarised in Alg.~Box~\ref{alg:SIRT}.%\textcolor{red}{The procedure is repeated ... (the 'we' is entirely redundant, and distracting.)}


\textcolor{red}{Function or distribution}

%Note that with Cartesian basis $ \sum \phi^{(i)}_k(x_k) \Bigg( \sum \bm{G}^{(\alpha_{k-1})}_{<k}(x_{<k}) \bm{B}_k[\alpha_{k-1},i,l_k] \Bigg) \Bigg)^2 
%$  and $\bm{G}_{<k}(x^{(s)}_{<k}) = \bm{G}_{1}(x^{(s)}_{1}) \cdots \bm{G}_{k-1}(x^{(s)}_{k-1}) $ are simple matrix multiplications for each grid point $i$ or sample $\bm{x}^{(s)}$.


\subsubsection{Metropolis-Hastings -- correction step}
Since the samples by the SIRT scheme are generated from an approximation, it is sensible to correct those using a Metropolis-Hastings (MH) importance step.
\begin{algorithm}[!ht]
	\caption{MH correction step}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} samples $\{ \bm{x}^{(1)},\dots, \bm{x}^{(N+1)} \} $, where each $\bm{x}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N+1$
		\FOR{ \( s = 1, \dots, N\)}
		\STATE compute MH ratio $\frac{w^{(s+1)}}{w^{(s)} } =\frac{\pi(\bm{x}^{(s+1)})}{\pi(\bm{x}^{(s)})} \frac{f_X(\bm{x}^{(s)})}{f_X(\bm{x}^{(s+1)})}$ 
		\STATE compute acceptance probability $\alpha = \text{min}(w^{(s+1)}/w^{(s)}, 1)$ 
		\STATE Draw $u \sim \mathcal{U}(0,1)$
		\IF{$\alpha \geq u$ }
		\STATE Accept and set $\bm{x}_{\text{MH}}^{(s+1)} = \bm{x}^{(s+1)}$
		\ELSE  
		\STATE Reject and keep $\bm{x}_{\text{MH}}^{(s+1)} = \bm{x}^{(s)}$
		\ENDIF
		\ENDFOR
		\STATE \textbf{Output:} corrected sample chain $\{ \bm{x}_{\text{MH}}^{(1)},\dots, \bm{x}_{\text{MH}}^{(N)} \} $, where each $\bm{x}_{\text{MH}}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N$
	\end{algorithmic}
	\label{alg:MHCorr}
\end{algorithm}
In doing so, we compute the acceptance probability $  \alpha = \text{min}(w^{(s+1)}/w^{(s)}, 1)$, where 
\begin{align}
	w(x) = \frac{\pi(\bm{x})}{f_X(\bm{x})} = \frac{\pi(\bm{x})}{\xi + \tilde{g}(\bm{x})^2} 
\end{align}
is the importance ratio.
Note that since we calculate the ratio $w^{(s+1)}/w^{(s)}$, the normalising constants cancel.
In practice, we calculate the importance ratio in the log space, where $\log f_X(\bm{x})  =  \log f_{X_1}(x_1) + \log f_{X_2|X_1}(x_2|x_1) + \cdots + \log f_{X_k|X_{<k}}(x_k|x_{k-1},\dots,x_1)$ is given as in Eq.~\ref{eq:CurrMarg}, see \cite{dolgov2020approximation}.
We refer to this as the SIRT-MH scheme, which provides the corrected chain $ \{ \bm{x}_{\text{MH}}^{(1)},\dots, \bm{x}_{\text{MH}}^{(N)}  \} \sim \pi(\bm{x}) $.


\subsection{Error of the TT Approximation}
A straightforward way to assess an average error from the TT approximation is to calculate the relative root mean squared (RMS) error 
%\textcolor{red}{explain why this is a sensible thing to do.}
\begin{align}
	\Bigg( \frac{ \int_{\mathcal{X}} (\pi(\bm{x}) - (\xi + \tilde{g}(\bm{x})^2))^2 \uplambda(\bm{x}) \diff \bm{x}}{ \int_{\mathcal{X}} \pi(\bm{x})^2 \uplambda(x)  \diff \bm{x} } \Bigg)^{1/2} =	\frac{\lVert 	\pi(\bm{x}) - (\xi + \tilde{g}(\bm{x})^2)  \rVert_{L^2_{\uplambda}(\mathcal{X})}}{\lVert 	\pi(\bm{x}) \rVert_{L^2_{\uplambda}(\mathcal{X})}  } \, .
\end{align}
The RMS is approximated by
\begin{align}
	\Bigg( \frac{1}{N} \sum^{N}_{i =1} \Big(\pi(\bm{x}^{(i)}) - \big(\xi + \tilde{g}(\bm{x}^{(i)})^2\big)\Big)^2 \uplambda(\bm{x}^{(i)})\Bigg)^{1/2}    \approx \Bigg(  \int_{\mathcal{X}} (\pi(\bm{x}) - \big(\xi + \tilde{g}(\bm{x})^2\big)\Big)^2 \uplambda(\bm{x}) \diff \bm{x} \Bigg)^{1/2} \label{eq:RMSTT}
\end{align}
and similarly $\int_{\mathcal{X}} \pi(\bm{x})^2 \uplambda(\bm{x})  \diff \bm{x}$.


\subsubsection{Absolute error bound}
\label{subsec:wasser}
If large errors occur in regions with low probability the RMS is sensible to those, whereas the Wasserstein distance weights the distance measures according to their respective probability values.

The Wasserstein distance is the infimum over all couplings between two probability distributions with respect to some distance measure.
%\textcolor{red}{why is this sensible, or is it just 'another way'?}
The Kantorovich-Rubinstein duality, as in~\cite{thickstun2019kantorovich, Ambrosio2024Kanta}, says that the 1-Wasserstein distance is equal to the supremum of differences in expectations over all 1-Lipschitz functions $h$ between two probability distributions.
So the 1-Wasserstein distance provides an upper absolute error bound.

The 1-Wasserstein distance is defined as
\begin{align}
	W_1(\pi,\tilde{\pi}) = \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}}\int_{\mathcal{X} \times \mathcal{X}} c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}}) \, \nu(\bm{x},\tilde{\bm{x}}) d\bm{x} d\tilde{\bm{x}}
	\label{eq:wass} \, ,
\end{align}
where $\nu$ couples $\bm{x}$ and $\tilde{\bm{x}}$ so that the integral over the distance $c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}}) $ weighted by the probability measures $\pi$ and $\tilde{\pi}$ is the greatest lower bound of all integrals with respect to $\nu$ in the set of all couplings $ \Pi(\pi,\tilde{\pi})$.
Often $\nu$ is the transport plan, where $c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}})$ is the (ground) cost function, and $\nu(\bm{x}, \tilde{\bm{x}})$ is related to the mass which has to be transported and the 1-Wasserstein distance is the earth mover distance.
On the other hand (Kantorovich-Rubinstein duality) the 1-Wasserstein distance
\begin{align}
	W_1(\pi,\tilde{\pi})  =& \underset{ h(\bm{x}); \, c_{\mathcal{Y}}(h(\bm{x}), h(\tilde{\bm{x}}) ) \leq  c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}}) }{ \text{sup}} \Bigg\{  \int_{\mathcal{X}} h(\bm{x}) d \pi (\bm{x})  - \int_{\mathcal{X}} h(\tilde{\bm{x}}) d \tilde{\pi} (\tilde{\bm{x}}) \Bigg\} \\
	=& \underset{ h(\bm{x});  \, c_{\mathcal{Y}}(h(\bm{x}), h(\tilde{\bm{x}}) )  \leq  c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}}) }{ \text{sup}}  \Bigg\{  \underset{\bm{x} \sim  \pi }{\mathbb{E}} \big[ h(\bm{x}) \big]  -  \underset{\tilde{\bm{x}}\sim \tilde{\pi}}{\mathbb{E}} \big[ h(\tilde{\bm{x}}) \big] \Bigg\} .
\end{align}
is the lowest upper bound of differences in expectations over all 1-Lipschitz function $h(\bm{x}) : \mathcal{X} \rightarrow \mathcal{Y}$ in between the two distributions $\pi$ and $\tilde{\pi}$, with the distance measure $c_{\mathcal{X}}$ on the set $\mathcal{X}$ forming the metric space $( \mathcal{X}, c_{\mathcal{X}} )$ and similarly the metric space $(\mathcal{Y}, c_{\mathcal{Y}})$.

For two sample sets $\{ \bm{x}^{(1)},\dots,\bm{x}^{(N)}\} \sim \pi$ and $\{\tilde{ \bm{x}}^{(1)},\dots,\tilde{\bm{x}}^{(M)}\} \sim \tilde{\pi}$ the calculation of the Wasserstein distance becomes an optimisation problem that is to find the best coupling of samples weighted by their distribution value according to an appropriate distance measure~\cite{feydy2020OT}, which we set to $c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}})= \lVert \bm{x} -\tilde{\bm{x}} \rVert_{L^2} $.
More specifically,
\begin{align}
	W_1(\pi,\tilde{\pi}) = 	\underset{\nu \in \Pi(\pi,\tilde{\pi}) }{\text{min}} \sum^M_{j = 1} \sum^N_{i =1}  \nu_{ij} \lVert\bm{x}^{(i)}  -  \tilde{\bm{x}}^{(j)} \rVert_{L^2} \, , \label{eq:applWasser}
\end{align}
where the transport plan $\nu \in \mathbb{R}^{N \times M}_ {\geq 0}$ defines the coupling $\nu_{ij} \in \nu $ as $ \nu_{ij} \coloneqq \pi(\bm{x}^{(i)}) \tilde{\pi}(\tilde{\bm{x}}^{(j)})$ similar to~\cite[Eq. 3.166]{feydy2020OT}.
Additionally we require that $\sum^N_{i =1} \pi(\bm{x}^{(i)}) = \sum^M_{j = 1} \tilde{\pi}(\tilde{\bm{x}}^{(j)})= 1 $.
This gives us an upper bound of the absolute error between the expected value of any 1-Lipschitz function $h$.
%, e.g the upper bound of absolute differences in means related to the probability measures $\pi$ and $\tilde{\pi}$. 

%Now consider the 2-Wasserstein distance
%\begin{align}
%	W_2(\pi,\tilde{\pi}) = \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}}  \Bigg(\int_{\mathcal{X} \times \mathcal{X}} c(\bm{x},\tilde{\bm{x}})^2 \, \nu(\bm{x},\tilde{\bm{x}}) d\bm{x} d\tilde{\bm{x}} \Bigg)^{1/2}
%	\label{eq:wass} \, ,
%\end{align}
%which for two multivariate normal distributions $\pi \sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$ and $\tilde{\pi} \sim \mathcal{N}(\tilde{\bm{\mu}},  \tilde{\bm{\Sigma}})$ is defined as 
%\begin{align}
%	W_2(\pi,\tilde{\pi}) = \Bigg(  \lVert \bm{\mu} -\tilde{\bm{\mu}} \rVert^2_{L^2} + \text{trace}(\bm{\Sigma}+  \tilde{ \bm{\Sigma}} - 2 (\tilde{ \bm{\Sigma}}^{1/2} \bm{\Sigma} \tilde{ \bm{\Sigma}}^{1/2} )^{1/2} )  \Bigg)^{1/2} \, \text{\cite{Asuka2011Wasser},}
%\end{align}
%with distance measure set to $c(\bm{x},\tilde{\bm{x}})= \lVert \bm{x} - \tilde{\bm{x}}\rVert_{L^2} $.
%To relate that to the 1-Wassertein distance, we write the p-Wassertein distance as the infimum of expectations
%\begin{align}
%	W_p(\pi,\tilde{\pi}) =  \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}} \Bigg( \underset{ \bm{x},\tilde{\bm{x}} \sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^p \Bigg)^{1/p}
%\end{align}
%and one can show that
%\begin{align}
% \Bigg(   \underset{ \bm{x},\tilde{\bm{x}} \sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^p \Bigg)^{1/p} \leq  \Bigg(  \underset{ \bm{x},\tilde{\bm{x}}\sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^q \Bigg)^{1/q}
%\end{align}
%for every $p \leq q$, \cite{Chizat2020LecNot}.
%Then we can bound the absolute difference in expectation of a 1-Lipschitz function $h$ by the 2-Wasserstein distance, so that $W_1(\pi,\tilde{\pi}) \leq W_2(\pi,\tilde{\pi})$.



%Consequently, to map between the linear and non-linear forward map, we generate two affine subspaces $\bm{V}$ and $\bm{W}$ over the same field.
%Assume we draw samples $\{\bm{x}^{(1)}, \dots, \bm{x}^{(j)}, \dots ,\bm{x}^{(m)}\} \sim \pi(\bm{x}|\bm{y})$ and the affine subspace associated with the linear forward model is \begin{align}
%	\bm{W} = \begin{bmatrix}
%		\vert&   &  \vert & & \vert \\
%		\bm{A}_{L} \bm{x}^{(1)} &  \cdots& \bm{A}_{L} \bm{x}^{(j)} &  \cdots & \bm{A}_{L} \bm{x}^{(m)} \\
%		\vert&   &  \vert & & \vert 
%	\end{bmatrix}
%	%	= \begin{bmatrix}
%  %\begin{array}{ccc}
%	%\horzbar & w_{1} & \horzbar \\
%	%	& \vdots    &          \\
%	%\horzbar & w_{j} & \horzbar \\
%	%& \vdots    &          \\
%	%\horzbar &w_{m} & \horzbar
%%end{array}
%%	\end{bmatrix} 
%\in \mathbb{R}^{m \times m}
%\end{align} and with the non-linear forward model is 
%\begin{align}
%	\bm{V} = \begin{bmatrix}
%		\vert&   &  \vert & & \vert \\
%		\bm{A}_{NL}\bm{x}^{(1)} &  \cdots& \bm{A}_{NL}\bm{x}^{(j)} &  \cdots & \bm{A}_{NL} \bm{x}^{(m)}  \\
%		\vert&   &  \vert & & \vert 
%	\end{bmatrix} = 
%		\begin{bmatrix}
%	\begin{array}{ccc}
%		\horzbar & v_{1} & \horzbar \\
%		& \vdots    &          \\
%		\horzbar & v_{j} & \horzbar \\
%		& \vdots    &          \\
%		\horzbar &v_{m} & \horzbar
%	\end{array}
%\end{bmatrix}\in \mathbb{R}^{m \times m} \, .
%\end{align}
%Then the we calculate affine map 
%\begin{align}
%	\bm{V}\bm{W}^{-1} = \bm{M} =
%		\begin{bmatrix}
%	\begin{array}{ccc}
%		\horzbar & r_{1} & \horzbar \\
%		& \vdots    &          \\
%		\horzbar & r_{j} & \horzbar \\
%		& \vdots    &          \\
%		\horzbar &r_{m} & \horzbar
%	\end{array}
%\end{bmatrix}\, \in \mathbb{R}^{m \times m} ,
%\end{align}
%using that, for the forward model in this thesis, each row of $\bm{V}$ is independent of each other, we solve $v_j =r_j \bm{W} $ for each row $ r_j  \in \bm{M}$, where $j = 1, \dots, m$.





%We refer show that the filter factors as in \cite{tan2016LecNot} for a regularisation parameter as shown here affects the solution the generalised singular value decomposition is given in  of $A, T$ as in \cite{hansen1989GSVD}.
%Where on can show the the so-called filter factors are dominated by the default regularisaion solution 
%then the filter factors 
%
%
%
%
%We refer to \cite{hansen1989GSVD} and \cite{tan2016LecNot} for the curious reader, who wants to know how the regularisation parameter affects the reconstruction especially for regions where the forward model does not provide much information (small singular values).
%
%
%
%To show how the regularisation parameter effects the solution one can do a singular value decomposition of $A$
%and the generalised singular value decomposition of $A, T$ as in \cite{hansen1989GSVD}.
%
%Then $\bm{A} = \bm{U} \bm{\Lambda}_A \bm{M}^{-1}$ and $\bm{L} = \bm{V} [\bm{\Lambda}_L,0] \bm{M}^{-1} $
%Where the general singular values $\bm{\Lambda}_A = \text{diag}(\sigma_{A,1},\dots,\sigma_{A,1},1,\dots,1)$ and $\bm{\Lambda}_L = \text{diag}(\sigma_{L,1},\dots,\sigma_{L,1},1,\dots,1)$
%
%
%
%Then one can show that the solution is %$\x_{\lambda} = \bm{M} \bm{F} \bm{U}^T \bm{y} $
%with filter factors
%\begin{align}
%	f_i = \frac{(\sigma_{A,i}/\sigma_{L,i})^2 }{(\sigma_{A,i}/\sigma_{L,i})^2 + \lambda^2} \approx
%	\begin{cases}
%		\frac{(\sigma_{A,i}/\sigma_{L,i})^2}{ \lambda^2},  & \sigma_{A,i}/\sigma_{L,i} \gg \lambda\\
%				1 ,  & \sigma_{A,i}/\sigma_{L,i} \ll \lambda 
%	\end{cases}
%\, \, \text{for}\, i= 1,\dots,p
%\end{align}
%Then small singular values depend on prior information only
%for large $(\sigma_{A,i}/\sigma_{L,i})^2$ singular values the solution is unaffected
%for small $(\sigma_{A,i}/\sigma_{L,i})^2$ singular values the solution is affected by the regularisation parameter




