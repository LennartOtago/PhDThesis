%\the\columnwidth
\chapter{Theoretical and Technical Background}
\label{ch:background}
In this chapter, we provide introductions and brief derivations of the methods used in this thesis, as well as references for more details. We keep it as general as possible, as the expressions specifically tailored towards the forward map will be presented in the results Chapter~\ref{ch:res}.
We begin by introducing a general hierarchical Bayesian approach to an inverse problem.
Next, we provide the basics of Metropolis-Hastings sampling, more specifically, the essentials of Markov-Chain Monte Carlo (MCMC) methods.
Further, we explain how we approximate functions using a Tensor-Train (TT) approach, which enables us to calculate marginals from the posterior distribution cheaply.
Then, we elaborate on the Wasserstein distance for assessing upper error bounds.
Lastly, we provide some background information on affine maps and the Tikhonov regularisation method.


\section{Hierarchical Bayesian Inference}
\label{sec:bayes}
\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[roundnode2] at (0,3.5) (th)    {$\bm{\theta}$};
		\node[roundnode2] at (0,1.5) (x)    {$\bm{x}$};
		\node[roundnode2] at (0,-1.5) (u)    {$\Omega$};
		\node[rectnode] at (0,-3.5) (y)    {$\bm{y}$};
		
		\draw[->, very thick] (th.south) -- (x.north); 
		\draw[->, very thick, mydotted] (x.south) -- (u.north); 
		\draw[->, very thick] (u.south) -- (y.north); 
		\draw[->, very thick] (th) edge[bend right=60] (y);  
		
		\node[align=center] at (2.8,3.5) (tht) {$\sim \pi_{\bm{\theta}}(\cdot) $ hyper-parameters};
		\node[align=center] at (2.4,1.5) (xt) {$\sim \pi_{\bm{x}}(\cdot|\bm{\theta}) $ parameters};
		\node[align=center] at (2.5,0) (At) {$\{\bm{A}(\bm{x})\}$ "noise free data"};
		\node[align=center] at (3,-1.5) (ut) {space of all measurables};
		\node[align=center] at (2,-3.5) (yt) {$\sim \pi_{\bm{y}}(\cdot|\bm{\theta},\bm{x})$ data};
		\node[align=center] at (-3,0) (nt) {noise};
	\end{tikzpicture}
	\caption[Hierarchical Bayesian Inference]{The directed acyclic graph (DAG) for an inverse problem visualises statistical dependencies as solid line arrows and deterministic dependencies as dotted arrows.
		The hyper-parameters $\bm{\theta}$ are distributed as the hyper-prior distribution $\pi(\bm{\theta})$.
		The prior distribution $ \pi_{\bm{x}}(\cdot|\bm{\theta})$ for the parameter $\bm{x}$ and the noise are statistically dependent on some of those hyper-parameters.
		Then a parameter $\bm{x} \sim \pi_{\bm{x}}(\cdot|\bm{\theta})$ is mapped onto the space of all measurables $\bm{\Omega}=\bm{A}(\bm{x})$ deterministically through the forward model.
		From the space of all measurable noise free data we observe a data set $\bm{y} = \bm{A}(\bm{x}) + \bm{\eta}$ with some random noise $ \bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$, which determines the likelihood function $\pi(\bm{y}|\bm{\theta}, \bm{x})$. }
	\label{fig:FirstDAG}
\end{figure}



Assume we observe some data
\begin{align}
	\bm{y} = \bm{A} (\bm{x}) + \bm{\eta},
	\label{eq:NonLinDat}
\end{align}
based on a forward model $\bm{A}(\bm{x})$, which may be non-linear, a unknown parameter $\bm{x}$ and some additive random noise $\bm{\eta}$.
Naturally, due to the noise, which we classify through a hyper-parameter, we deal with a random process, and we wish to include that in our hierarchically ordered modelling.
Then define the likelihood function $\pi(\bm{y}|\bm{\theta},\bm{x})$ according to the nature of the noise as well as all relevant information about the measurement process, captured by the model $\bm{A}(\bm{x})$.
We read $\pi(\bm{y}|\bm{\theta},\bm{x})$ as the distribution over $\bm{y}$ conditioned on $\bm{x}$ and $\bm{\theta}$, and $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$ as $\bm{\eta}$ is distributed as $\pi_{\bm{\eta}}(\cdot|\bm{\theta})$.
Here $\bm{\theta}$ may account for multiple hyper-parameters, e.g. describing the distribution $\pi_{\bm{\eta}}(\cdot|\bm{\theta})$ over the noise vector $\bm{\eta}$, and describing physical properties or functional dependencies of $\bm{x}$, e.g. smoothness, trough the prior distribution $\pi(\bm{x}|\bm{\theta})$.
Consequently we define a hyper-prior distribution $\pi(\bm{\theta})$, where $\pi(\bm{x}, \bm{\theta}) = \pi(\bm{x}|\bm{\theta}) \pi(\bm{\theta}) $.
Choosing these prior distributions is ultimately a modeller's choice and crucial, as it shall not affect the posterior distribution 
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) = \frac{ \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta})}{\pi(\bm{y})} \propto \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta}) \, ,
\end{align}
which according to Bayes theorem gives us a distribution of $\bm{x}$ and $\bm{\theta}$ given (conditioned) on the data.
Note that here we include the hyper-parameters within the posterior distribution, which is the key idea of hierarchical Bayesian modelling, as we do not only aim to quantify the posterior distribution over the parameters $\bm{x}$ but also the posterior distribution over the hyper-parameter $\bm{\theta}$.
We can visualise this hierarchically ordered correlation structure between parameters as well as how distributions progress through a measurement process, using a directed acyclic graph (DAG), see Figure~\ref{fig:FirstDAG}.

The expectation of any function $h(\bm{x}_{\bm{\theta}})$, where $\bm{x}$ may depend on $\bm{\theta}$, is described as 
\begin{align}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}_{\bm{\theta}})] =  \underbrace{\int \int   h(\bm{x}_{\bm{\theta}}) \,  \pi(\bm{x}, \bm{\theta} | \bm{y} ) \, \diff \bm{x}  \, \diff \bm{\theta}}_{\bm{\mu}_{\text{int}}}   \label{eq:expPos} \, .
\end{align}
If that is a high-dimensional integral and computationally not feasible to solve, we approximate 
\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}_{\bm{\theta}})] \approx \underbrace{ \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)}_{\bm{\theta}})  }_{\bm{\mu}_{\text{samp}}} \, ,
\end{align}
with an the unbiased sample-based Monte Carlo estimate \cite{roberts2004general} for large enough $N$ (law of large numbers \cite[Chapter 17]{tweedie2009measprob}).
Here, the samples $\{\bm{x}^{(k)},\bm{\theta}^{(k)} \}\sim \pi_{\bm{x}, \bm{\theta}}(\cdot|\bm{y})$, for $k = 1, \dots, N$, form a sample set $\mathcal{M} =\{ (\bm{x},\bm{\theta})^{(1)}, \dots ,  (\bm{x},\bm{\theta})^{(N)} \}$.

Generating a representative sample set quickly from the posterior distribution often presents a significant challenge. This is mainly due to the strong correlations that usually exist between the parameters and hyper-parameters, as discussed by Rue and Held in \cite{rue2005gaussian} and illustrated in Appendix~\ref{ap:Correlation}.
If $\bm{x}$ can not be parametrised directly in terms of the hyper-parameters $\bm{\theta}$, so that $\bm{x}(\bm{\theta})$ is function of $\bm{\theta}$, it is beneficial to factorise the posterior distribution as
\begin{align}
	\pi(\bm{x}, \bm{\theta} |  \bm{y}) = \pi(\bm{x} |  \bm{\theta}, \bm{y}) \, \pi(\bm{\theta} |   \bm{y}), \label{eq:MTC}
\end{align}
into the conditional posterior $\pi(\bm{x} |  \bm{\theta}, \bm{y})$ over the latent field $\bm{x}$ and the marginal posterior 
\begin{align}
	\pi(\bm{\theta} |   \bm{y}) =  \frac{ \pi(   \bm{y} | \bm{\theta} ,\bm{x})  \pi( \bm{x} | \bm{\theta} )  \pi(\bm{\theta}) }{ \pi(\bm{x} | \bm{\theta} ,   \bm{y})   \pi( \bm{y})} \propto \frac{ \pi(   \bm{y} | \bm{\theta} ,\bm{x})  \pi( \bm{x} | \bm{\theta} )  \pi(\bm{\theta}) }{ \pi(\bm{x} | \bm{\theta} ,   \bm{y}) } \label{eq:margGen}
\end{align}
over the hyper-parameters $\bm{\theta}$.
In \cite{norton2018sampling}, they classify inverse problems into problems with known or unknown conditional posterior distributions, and conclude that if $\pi(\bm{x} | \bm{\theta} ,   \bm{y}) = \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}| \bm{\theta})  / \pi(   \bm{y}| \bm{\theta})$ has a known form, the normalising constant is available \newline $\int \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}| \bm{\theta}) \text{d} \bm{x} = \pi(   \bm{y}| \bm{\theta})  \propto \pi( \bm{\theta}|\bm{y}) / \pi(\bm{\theta})$ and one can almost surely determine the $\bm{\theta}$-dependence of the marginal posterior $\pi(\bm{\theta} |   \bm{y})$.

This approach, known as the marginal and then conditional (MTC) method, is particularly advantageous when $\bm{x}\in \mathbb{R}^n$ is high-dimensional, while $\bm{\theta}\in \mathbb{R}^{n_{\bm{\theta}}}$ is low-dimensional, so that $n_{\bm{\theta}} \ll n$ and evaluation $\pi(\bm{\theta}| \bm{y})$ is cheap.
Applying the law of total expectation~\cite{champ2022generalizedlawtotalcovariance}, Eq.~\eqref{eq:expPos} becomes
\begin{align}
	\mathbb{E}_{\bm{x} |  \bm{y}} [h(\bm{x})] 
	= \mathbb{E}_{\bm{\theta} |  \bm{y}} \left[ \mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} [h(\bm{x}_{\bm{\theta}})] \right] 
	= \int \mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} \left[ h(\bm{x}_{\bm{\theta}}) \right] \, \pi(\bm{\theta} |  \bm{y}) \, \diff \bm{\theta},
	\label{eq:fullCond}
\end{align}
where, in the case of a linear-Gaussian hierarchical Bayesian model, both the marginal distribution and the inner expectation $\mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} \left[ h(\bm{x}_{\bm{\theta}}) \right]$ are well defined see next subsection.
Furthermore, the central limit theorem states that the sample mean $ \bm{\mu}^{(i)}_{\text{samp}} $, of independent sample sets $\mathcal{M}_i$ for $i = 1, \dots, n$ of any distribution, converges in distribution to a normal distribution so that
\begin{align}
	\sqrt{n} (\bm{\mu}^{(i)}_{\text{samp}} -  \bm{\mu}_{\text{int}} ) \overset{\mathcal{D}}{\longrightarrow} \mathcal{N} (0,\sigma^2) \text{\cite{geyer1992practical}},
\end{align}
and if $\sigma^2 < \infty$ the Monte-Carlo error $\bm{\mu}^{(i)}_{\text{samp}} -  \bm{\mu}_{\text{int}} $ is bounded.

\subsubsection{Integrated Autocorrelation time}
To assess the error $\sigma^2$ of chain $\mathcal{M}_i$, we ignore systematic error due to initialisation bias (burn-in period), but we have to take into account that samples produced by any system or algorithm are correlated.
To derive the integrated autocorrelation time (IATC), we follow the lecture notes \cite{wolff2002LecNot}.
In general, the error of a Monte-Carlo-based estimate from a sample set $\mathcal{M}_i = \{\bm{x}^{(1)}, \dots,\bm{x}^{(k)},\dots, \bm{x}^{(s)},\dots, \bm{x}^{(N)}\} \sim \pi(\bm{x}|\bm{y})$ is:
\begin{align}
	(\sigma^{(i)})^2 = \text{var}(\bm{\mu}^{(i)}_{\text{samp}} ) =  \text{var}(\text{E}_{\bm{x}|\bm{y}} [h(\bm{x})]) = \Bigg( \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)}) - \bm{\mu}^{(i)} \Bigg)^2 \, .
\end{align}
Expanding this summation, we see that
\begin{align}
	\Bigg( \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)}) - \bm{\mu}^{(i)} \Bigg)^2  =  \frac{\text{var}(\bm{\mu}^{(i)}_{\text{samp}}) }{N^2} \sum_{k,s=1}^{N}\rho(k-s)\, ,
\end{align}
with the normalised auto correlation coefficient $\rho(k-s) =  \Gamma(k-s)/ \Gamma(0)$ at lag $k-s$, where the auto correlation coefficient $\Gamma(k-s) =  \big( h(\bm{x}^{(k)}) - \bm{\mu}^{(i)} \big) \big(h(\bm{x}^{(s)}) - \bm{\mu}^{(i)} \big)$ and  $\Gamma(0) = \text{var}(\bm{\mu}^{(i)}_{\text{samp}} )$ for $k=s$.
Typically $\Gamma(t)$ decays exponentially so that $\Gamma(t) \overset{t \rightarrow \infty }{ \propto} \exp\{ - t / \tau \}  $ and for positive $\tau$ and $N\gg \tau$ we can approximate
\begin{align}
	(\sigma^{(i)})^2   \approx  \frac{\text{var}(h(\bm{x}) )}{N} \underbrace{\sum_{t = - \infty }^{\infty} \rho(t)}_{ \coloneqq 	2\tau_{\text{int}} } = \text{var}(h(\bm{x})) \frac{ 2 \tau_{\text{int}} }{N} \, , \label{eq:MCerr}
\end{align}
where define the IATC as in \cite[pp. 103-105]{wolff2002LecNot}.
See Appendix \ref{ap:IATC} and \cite{Sokal1997} for a more detailed derivation.
The IACT provides a good estimate of how many steps the sampling algorithm needs to take to produce one independent sample, accordingly we define the effective sample size as $\frac{ 2 \tau_{\text{int}} }{N}$.

\subsubsection{Linear-Gaussian hierarchical Bayesian model}
\label{subsec:LinBay}
In case of normally distributed noise $\bm{\eta} \sim \mathcal{N}(0,\bm{\Sigma}(\bm{\theta}))$, with zero mean and covariance $\bm{\Sigma}(\bm{\theta})$, and a linear model $\bm{A}$, the data is given as 
\begin{align}
	\bm{y} = \bm{A} \bm{x} + \bm{\eta} \, ,
	\label{eq:LinDat}
\end{align}
and we can derive the marginal and conditional posterior distribution explicitly.
We define our hierarchical Bayesian model as
\begin{subequations}
	\begin{align}
		\bm{y} |  \bm{x}, \bm{\theta} &\sim \mathcal{N}(\bm{A} \bm{x}, \bm{\Sigma}(\bm{\theta}) ) \\
		\bm{x} |  \bm{\theta} &\sim \mathcal{N}(\bm{\mu}, \bm{Q}(\bm{\theta})^{-1} ) \\
		\bm{\theta} &\sim \pi(\bm{\theta}) ,
	\end{align}
	\label{eq:GenBayMode}
\end{subequations}
with a Gaussian likelihood function $\pi(\bm{y} |  \bm{x}, \bm{\theta} )$, the prior mean $\bm{\mu}$, prior precision $\bm{Q}(\bm{\theta})$ and a hyper-prior distribution $\pi(\bm{\theta})$.
To derive the marginal posterior and the conditional posterior distribution, we consider the joint multivariate Gaussian distribution
\begin{align}
	\begin{pmatrix}
		\bm{x} \\
		\bm{y}
	\end{pmatrix}\sim \mathcal{N}\left[  \begin{pmatrix}
		\bm{\mu} \\
		\bm{A}\bm{\mu}
	\end{pmatrix},\begin{pmatrix}
		\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A} & - \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \\
		\bm{\Sigma}(\bm{\theta})^{-1} \bm{A} & \bm{\Sigma}(\bm{\theta})^{-1} 
	\end{pmatrix}^{-1} \right] \, ,
\end{align}
where we provide the joint precision matrix as in \cite{SIMPSON201216}, see also \cite{rue2005gaussian, fox2016fast}.
Immediately, we formulate the conditional posterior as 
\begin{align}
	\bm{x} | \bm{y}, \bm{\theta} \sim \mathcal{N}(\bm{\mu} + (\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})^{-1}(\bm{y} - \bm{A}\bm{\mu}),(\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})^{-1}) \, \label{eq:CondPostLin}.
\end{align}
Then the marginal posterior distribution over the hyper-parameters can be derived as in Eq. \ref{eq:margGen}, where, as noted in \cite{fox2016fast}, the parameter $\bm{x}$ cancels and we arrive at
\begin{align}\begin{split}
		\pi(\bm{\theta} | \bm{y}) \propto & \sqrt{\frac{\det{(\bm{\Sigma}(\bm{\theta})^{-1})} \det{(\bm{Q}(\bm{\theta}))} }{\det{(\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})} } }  \exp \Bigg\{  -\frac{1}{2} (\bm{y} - \bm{A} \bm{\mu})^T \\ &\big[ \bm{\Sigma}(\bm{\theta})^{-1} - \bm{\Sigma}(\bm{\theta})^{-1} \bm{A}  (\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})^{-1} \bm{A}^T \bm{\Sigma} (\bm{\theta})^{-1} \big] (\bm{y} - \bm{A} \bm{\mu}) \Bigg\} \pi(\bm{\theta}) \, .
	\end{split} 
\end{align} 
Having the marginal posterior distribution available breaks up the correlation structure between $\bm{x}$ and $\bm{\theta}$, see Appendix \ref{ap:Correlation}, and makes the marginal and then conditional (MTC) approach so efficient \cite{fox2016fast}.
This scheme evaluates the marginal posterior values first and then conditions on hyper-parameters to draw posterior samples $\bm{x} \sim \pi (\bm{x} | \bm{y}, \bm{\theta})$ or to evaluate expectation and variance of $\pi(\bm{x}| \bm{y})$ by integration over the marginal posterior.




\section{Sampling Methods}
\label{sec:sampling}
In this section we present the underlying methodology of the sampling methods used in this thesis and show how these methods draw samples \newline$ \mathcal{M} = \{ (\bm{x}, \bm{\theta} )^{(1)}, \dots, (\bm{x}, \bm{\theta} )^{(k)}, \dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y})$ from the desired target distribution, so that we can calculate sample-based estimates as in Eq.~\ref{eq:sampMean}.
Here, $\mathcal{M}$ denotes a Markov chain, where each new sample $(\bm{x}, \bm{\theta})^{(k)}$ is only affected by the previous one, $(\bm{x}, \bm{\theta})^{(k-1)}$.
Markov chain Monte Carlo (MCMC) methods generate such a chain $\mathcal{M}$ using random (Monte Carlo) proposals $(\bm{x}, \bm{\theta})^{(k)} \sim q( \cdot |  (\bm{x}, \bm{\theta})^{(k-1)})$ according to a proposal distribution conditioned on the previous sample (Markov), where ergodicity of the chain $\mathcal{M}$ is a sufficient criterion for using sample-based estimates~\cite{tan2016LecNot, roberts2004general}.

The ergodicity theorem in~\cite{tan2016LecNot} states that, if a Markov chain $\mathcal{M}$ is aperiodic, irreducible, and reversible, then it converges to a unique stationary equilibrium distribution.
In other words, if the chain can reach any state from any other state (irreducibility), is not stuck in periodic cycles (aperiodicity), and is reversible (detailed balance condition~\cite{tan2016LecNot}).
Then the chain converges to the desired target distribution  with $ \mathcal{M} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y})$.
In practice, one can inspect the trace $\pi(\bm{x}^{(k)}, \bm{\theta}^{(k)} |  \bm{y})$ for $k = 1, \dots, N$ and visually assess convergence and mixing properties of the chain to evaluate ergodicity.
The sampling methods used in this thesis possess proven ergodic properties, and we therefore refer the reader to the corresponding literature for further details.

\subsection{Metropolis within Gibbs}
\label{subsec:MWG}
As in Eq. \ref{eq:MTC}, when using the MTC method we sample from $\pi(\bm{\theta} |  \bm{y})$ first and then determine the full conditional $\pi(\bm{x} | \bm{\theta} , \bm{y})$ as in Eq. \ref{eq:fullCond}. To sample from $\pi(\bm{\theta} |  \bm{y})$, we use a Metropolis-within-Gibbs (MWG) sampler as described in~\cite{fox2016fast}.
We apply the MWG sample for the two-dimensional case only, with $\bm{\theta} = (\theta_1, \theta_2)$, where we perform a Metropolis step in the $\theta_1$ direction and a Gibbs step in the $\theta_2$ direction.
Ergodicity for this approach is proven in~\cite{roberts2006harris}.


The Metropolis-within-Gibbs algorithm begins with an initial guess $\bm{\theta}^{(t)}$ at $t=0$. We then propose a new sample $\theta_1 \sim q(\theta_1 |  \theta_1^{(t-1)})$, conditioned on the previous state, using a symmetric proposal distribution $q(\theta_1 |  \theta_1^{(t-1)}) = q(\theta_1^{(t-1)} |  \theta_1)$, which is a special case of the Metropolis-Hastings algorithm~\cite{roberts2006harris}.
We accept and set $\theta_1^{(t)} = \theta_1$ with the acceptance probability
\begin{align}
	\alpha(\theta_1 |  \theta_1^{(t-1)}) = \min \left\{ 1, \frac{\pi(\theta_1 |  \theta_2^{(t-1)}, \bm{y}) \, \cancel{q(\theta_1^{(t-1)} |  \theta_1)}}{\pi(\theta_1^{(t-1)} |  \theta_2^{(t-1)}, \bm{y}) \, \cancel{q(\theta_1 |  \theta_1^{(t-1)})}} \right\}
\end{align}
or reject and keep $\theta_1^{(t)} = \theta_1^{(t-1)}$, which we do by comparing $\alpha$ to a uniform random number $u \sim \mathcal{U}(0,1)$. 

Next, we perform a Gibbs step in the $\theta_2$ direction, where Gibbs sampling is again a special case of the Metropolis-Hastings algorithm with acceptance probability equal to one, and draw the next sample $\theta_2^{(t)} \sim \pi(\cdot |  \theta_1^{(t)}, \bm{y})$, conditioned on the current value $\theta_1^{(t)}$. 

We repeat this procedure $N^{\prime}$ times and ensure convergence independently of the initial sample (irreducibility) by discarding the initial $N_{\text{burn-in}}$ samples after a so-called burn-in period, resulting in a Markov chain of length $N = N^{\prime} - N_{\text{burn-in}}$.

\begin{algorithm}[!ht]
	\caption{Metropolis within Gibbs}
	\begin{algorithmic}[1]
		\STATE Initialise and suppose two dimensional vector \( \bm{\theta}^{(0)}  =( \theta_1^{(0)} , \theta_2^{(0)}  ) \)
		\FOR{ \( k = 1, \dots, N^{\prime} \)}
		\STATE Propose \( \theta_1 \sim q(\cdot   | \theta_1 ^{(t-1)}) = q(\theta_1 ^{(t-1)} |\cdot  ) \)
		\STATE Compute
		\[ \alpha( \theta_1  | \theta_1^{(t-1)}) = \min \left\{ 1, \frac{\pi(\theta_1  | \theta^{(t-1)}_2, \bm{y}) \cancel{q(\theta_1^{(t-1)} | \theta_1 ) } }{\pi(\theta_1^{(t-1)}| \theta_2^{(t-1)}, \bm{y}) \cancel{q(\theta_1 | \theta_1^{(t-1)})} } \right\} \]
		\STATE Draw $u \sim \mathcal{U}(0,1)$
		\IF{$\alpha \geq u$ }
		\STATE Accept and set \( \theta_1^{(t)} = \theta_1 \)
		\ELSE  
		\STATE Reject and keep \(\theta_1^{(t)} = \theta_1^{(t-1)} \)
		\ENDIF
		\STATE Draw \(\theta_2^{(t)} \sim  \pi( \cdot | \theta_1^{(t)} , \bm{y} )\) 
		\ENDFOR
		\STATE Output: $ \bm{\theta}^{(0)}, \dots,  \bm{\theta}^{(k)} , \dots,   \bm{\theta}^{(N)} \sim \pi(\bm{\theta}| \bm{y}) $
	\end{algorithmic}
	\label{alg:MwG}
\end{algorithm}


%\subsection{Draw a sample from a multivariate normal distribution}
%\label{subsec:RTO}
%As part of the MTC scheme, we only draw samples from the conditional distribution $\pi(\bm{x} |  \bm{\theta}, \bm{y})$ after sampling from the marginal posterior $\pi(\bm{\theta} |  \bm{y})$. For linear-Gaussian Bayesian hierarchical models, samples from the multivariate normal distribution $\pi(\bm{x} |  \bm{\theta}, \bm{y})$ can be efficiently generated using the Randomise-then-Optimise (RTO) method~\cite{bardsley2012mcmc}.
%
%The full conditional distribution can be rewritten as
%\begin{align}
%	\pi(\bm{x} |  \bm{y}, \bm{\theta}) &\propto \pi(\bm{y} |  \bm{x}, \bm{\theta}) \, \pi(\bm{x} |  \bm{\theta}) \\
%	&= \exp \left( -\left\lVert \hat{\bm{A}} \bm{x} - \hat{\bm{y}} \right\rVert^2 \right),
%\end{align}
%where
%\begin{align}
%	\label{eq:minimizer}
%	\hat{\bm{A}} = 
%	\begin{bmatrix}
%		\bm{\Sigma}^{-1/2}(\bm{\theta}) \bm{A} \\
%		\bm{Q}^{1/2}(\bm{\theta})
%	\end{bmatrix}, \quad 
%	\hat{\bm{y}} = 
%	\begin{bmatrix}
%		\bm{\Sigma}^{-1/2}(\bm{\theta}) \bm{y} \\
%		\bm{Q}^{1/2}(\bm{\theta}) \bm{\mu}
%	\end{bmatrix} \quad \text{\cite{bardsley2014randomize}}.
%\end{align}
%A sample $\bm{x}_i $ can be computed by minimising the following equation with respect to $\hat{\bm{x}}$ :
%\begin{align}
%	\bm{x}_i = \arg \min_{\hat{\bm{x}}} \lVert \hat{\bm{A}} \hat{\bm{x}} - ( \hat{\bm{y}} + \bm{b} ) \rVert^2 , \quad \bm{b} \sim \mathcal{N}(\bm{0}, \mathbf{I}) \, ,
%\end{align}
%where we add a randomised perturbation $\bm{b}$.
%Similar to Section~\ref{sec:regularise}, this expression can be rewritten as
%\begin{align}
%	\label{eq:RTO}
%	\left( \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A} + \bm{Q}(\bm{\theta}) \right) \bm{x}_i = \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{y} + \bm{Q}(\bm{\theta}) \bm{\mu} + \bm{v}_1 + \bm{v}_2,
%\end{align}
%where the term $-\hat{\bm{A}}^T \bm{b}$ is decomposed as $\bm{v}_1 + \bm{v}_2$, with $\bm{v}_1 \sim \mathcal{N}(\bm{0}, \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A})$ and $\bm{v}_2 \sim \mathcal{N}(\bm{0}, \bm{Q}(\bm{\theta}))$, representing independent Gaussian random variables~\cite{bardsley2012mcmc, fox2016fast}.
%
%If the Markov chain over the marginal posterior $\pi(\bm{\theta} |  \bm{y})$ is ergodic, and the conditional samples $\bm{x}^{(k)} \sim \pi(\bm{x}|   \bm{\theta}^{(k)}, \bm{y})$ are drawn independently, then the resulting joint chain $\{ (\bm{x}, \bm{\theta})^{(1)}, \dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y})$ is also ergodic~\cite{acosta2014markov}.

\subsection{T-walk Sampler as a Black Box}
If the parameters $\bm{x}$ are functionally dependent on the hyper-parameters $\bm{\theta}$, i.e., $\bm{x} = \bm{x}(\bm{\theta})$, we can sample directly from the marginal posterior $\pi(\bm{\theta} | \bm{y})$ using the t-walk algorithm by Christen and Fox~\cite{christen2010general}. 
The t-walk is employed as a black-box sampler, requiring the specification of the number of samples, burn-in period, support region, and the target distribution. 
Convergence to the target distribution is guaranteed by construction of the algorithm.

\section{Numerical Approximation Methods - Tensor Train (TT)}
\label{sec:tensortrain}
%\textcolor{red}{Explain how to find normalisation constant and say that due to aproxiamting the sqaure root we ensure posivyt later when squaring it}
Instead of sampling from a target distribution $\pi(\bm{x})$ we can approximate that distribution on a d-dimensional grid with far fewer function evaluation compared to sampling methods using a tensor train (TT) approximation $\tilde{\pi}(\bm{x}) \approx \pi(\bm{x})$, with  $\bm{x}\in \mathbb{R}^d$.
First, we provide a short overview of probability spaces and their associated measures, as a foundation for calculating marginal probability distributions from the tensor train format.
Then we explain how we calculate marginal distribution and generate samples via the inverse Rosenblatt transform (IRT).
Note that we follow the notation of Cui et al. \cite{cui2022deep} to introduce this methodology.

Assume that the triple $(\Omega, \mathcal{F}, \mathbb{P})$ defines a probability space, where $\Omega$ denotes the complete sample space, $\mathcal{F}$ is a $\sigma$-algebra consisting of a collection of countable subsets $\{A_n\}_{n \in \mathbb{N}}$ with $A_n \subseteq \Omega$, and $\mathbb{P}$ is a probability measure defined on $\mathcal{F}$. The formal conditions for $\mathbb{P}$ to be a probability measure, and for $\mathcal{F}$ to be a $\sigma$-algebra over $\Omega$, are given in Appendix~\ref{ch:Mesure}.
We denote
\begin{align}
	\mathbb{P}(A) = \int_A \diff  \mathbb{P}
\end{align}
as the probability of an event $A \in \mathcal{F}$.
By applying the Radon-Nikodym theorem~\cite{kopp2004measintprob}, we can change variables
\begin{align}
	\mathbb{P}(A) = \int_A \frac{\diff \mathbb{P}}{\diff \bm{x}} \, \diff \bm{x} = \int_A \pi(\bm{x}) \, \diff \bm{x},
\end{align}
where $\mathrm{d}\bm{x}$ is a reference measure on the same probability space, commonly referred to as the Lebesgue measure. 
The Radon-Nikodym derivative $\frac{\mathrm{d} \mathbb{P}}{\mathrm{d}\bm{x}}$ of $\mathbb{P}$ with respect to $\bm{x}$ is often interpreted as the probability density function (PDF) $\pi(\bm{x})$. Thus, we say that $\mathbb{P}$ has a density $\pi(\bm{x})$ with respect to $\bm{x}$~\cite[Chapter 10]{simonnet1996measprob}.

Now, let $X: \Omega \longrightarrow \mathbb{R}^d$ be a $d$-dimensional random variable mapping from the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ to the measurable space $(\mathbb{R}^d, \mathcal{X})$, where $\mathcal{X}$ is a collection of subsets in $\mathbb{R}^d$.
Then the associated PDF $\pi(\bm{x})$ is a joint density of $X$, induced by the probability measure on $\Omega$~\cite{VesaInvLect, kopp2004measintprob}.
As in~\cite{cui2022deep}, we can define the parameter space as the Cartesian product $\mathcal{X} = \mathcal{X}_1 \times \mathcal{X}_2 \times \dots \times \mathcal{X}_d$ with $ x_k \in \mathcal{X}_k \subseteq \mathbb{R}$ and $\bm{x} = ( x_1,\dots ,x_k,\dots,x_d )$.
The marginal density function for the $k$-th component is then given by
\begin{align}
	f_{X_k}(x_k) = \frac{1}{z} \int_{\mathcal{X}_1} \cdots \int_{\mathcal{X}_d} \uplambda(\bm{x}) \, \pi(\bm{x}) \, \diff x_1 \cdots \diff x_{k-1} \, \diff x_{k+1} \cdots \diff x_d, \label{eq:margInt}
\end{align}
where we integrate over all dimensions except the $k$-th, and $z$ is a normalisation constant.
Here, we introduce a weight function $\uplambda(x)$, which can be useful for quadrature rules~\cite{davis2007methods}, to which~\cite{cui2022deep} refer to as a "product-form Lebesgue-measurable weighting function" and define it as
\begin{align}
	\uplambda(\mathcal{X}) = \prod_{i = 1}^{d} \uplambda_i(\mathcal{X}_i), \quad \text{where} \quad \uplambda_i(\mathcal{X}_i) = \int_{\mathcal{X}_i} \uplambda_i(x_i) \, \diff x_i. \label{eq:lebesgueWeight}
\end{align}

In the tensor train (TT) format, the integral in Eq. \ref{eq:margInt} for the marginal probability can be computed at a low computational cost as $\pi(\bm{x})$ is approximated by
\begin{align*}
	\tilde{\pi}(\bm{x}) = 	\tilde{\pi}_1(x_1)  \tilde{\pi}_2(x_2)  \cdots \tilde{\pi}_d(x_d)  \in \mathbb{R},
\end{align*}
which is a sequence of matrix multiplications, with $\tilde{\pi}_k(x_k) \in \mathbb{R}^{r_{k-1} \times r_k}$ for a fixed point $\bm{x} = (x_1, \dots, x_d)$ on a predefined $d$-dimensional discrete univariate grid over the parameter space $\mathcal{X}$. 
We call $\tilde{\pi}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ a TT-core with ranks $ r_{k-1} = r_k= r$, where the outer ranks are $r_0 = r_d = 1$, representing each dimension on $n$ grid points and connecting to neighbouring dimensions through its ranks.
This enables us to approximate $\pi(\mathcal{X})\approx \tilde{\pi}_1  \tilde{\pi}_2  \cdots \tilde{\pi}_d$ over the whole parameter space $\mathcal{X}$ using $2nr + (d-2)nr^2$ evaluation points, as illustrated in Figure~\ref{fig:TTfig}, instead of $n^d$ function evaluation.
Consequently, the marginal target distribution
\begin{align}
	\begin{split}
		f_{X_k}(x_k) \approx \frac{1}{z} \Big|\, 
		&\left( \int_{\mathbb{R}} \uplambda_1(x_1) \tilde{\pi}_1(x_1) \, \diff x_1 \right) \cdots 
		\left( \int_{\mathbb{R}} \uplambda_{k-1}(x_{k-1}) \tilde{\pi}_{k-1}(x_{k-1}) \, \diff x_{k-1} \right) \\
		&\quad \uplambda_k(x_k) \tilde{\pi}_k(x_k) \\
		& \left( \int_{\mathbb{R}} \uplambda_{k+1}(x_{k+1}) \tilde{\pi}_{k+1}(x_{k+1}) \,\diff x_{k+1} \right) \cdots 
		\left( \int_{\mathbb{R}} \uplambda_d(x_d) \tilde{\pi}_d(x_d) \, \diff x_d \right)
		\Big| 
	\end{split}
\end{align}
is computed by integrating over all TT cores except $\pi_k$, as in~\cite{dolgov2020approximation}, including a normalisation constant $z$~\cite{cui2022deep}.

\begin{figure}[ht!]
	\centering
\begin{subfigure}{\textwidth}
	\input{TTSchem.pdf_tex}
	\caption{}
\end{subfigure}
	\centering
\begin{subfigure}{\textwidth}
\begin{tikzpicture} 
	\node[rectnode] at (-5,0) (T1)    {$1 \times n \times r_1$};
	\node[rectnode] at (-2,0) (T2)    {$r_1 \times n \times r_{2}$};
	
	\node[rectnode] at (3,0) (Tn1)    {$r_{d-2} \times n \times r_{d-1} $};
	\node[rectnode] at (6.75,0) (Tn)    {$r_{d-1} \times n \times 1$};
	\draw[-, very thick] (T1.east) -- (T2.west); 
	\draw[-, very thick] (Tn.west) -- (Tn1.east); 
	\draw[-, mydotted, very thick] (T2.east) -- (Tn1.west);
	
	\node[align=center] at (-3.5,0.25) (R) {$r_1$};
	\node[align=center] at (5,0.25) (R) {$r_{d-1}$};	
\end{tikzpicture} 
	\caption{}
\end{subfigure}
\caption[Visualisation of a tensor train]{Here, we visualise the tensor train cores as two- and three-dimensional matrices. 
Each core has a length $n$, corresponding to the number of grid points in one dimension, and the cores are connected through ranks $r_k$. 
More specifically, a core $\tilde{\pi}_k$ has dimensions $r_{k-1} \times n \times r_k$, with outer ranks $r_0 = r_d = 1$.
Using the TT-format enables us to represent a $d$-dimensional grid with only $2nr + (d-2)nr^2$ evaluation points instead of $n^d$ grid points.
Figure~(a) is adapted from~\cite{fox2021grid}.}
\label{fig:TTfig}
\end{figure}


%%%%%% Squared and Basis Function %%%%%
In practice, tensor train approximations may suffer from numerical instability, in particular because it is not advantageous to approximate the target function $\pi(\bm{x})$ in e.g. the logarithmic space. 
Hence, Cui et al.~\cite{cui2022deep} approximate the square root of the probability density
\begin{align}
	\sqrt{\pi(\bm{x})} \approx \tilde{g}(\bm{x}) = \bm{G}_1(x_1), \dots, \bm{G}_k(x_k), \dots, \bm{G}_d(x_d)\, \,  \text{\cite[Eq. 18]{cui2022deep}},
\end{align}
which ensures positivity.
Here, each TT-core is given by
\begin{equation}
	G^{(\alpha_{k-1},\alpha_k)}_k(x_k) = \sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{A}_k[\alpha_{k-1}, i, \alpha_k], \quad k = 1, \dots, d,\, \,  \text{\cite[Eq. 21]{cui2022deep}},
\end{equation}
where $\bm{A}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ is the $k$-th coefficient tensor and $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ are the basis functions corresponding to the $k$-th coordinate.
The approximated unnormalised density is written as:
\begin{align}
	\pi(\bm{x}) \approx \xi + \tilde{g}(\bm{x})^2\, \,  \text{\cite[Eq. 19]{cui2022deep}},
\end{align}
where $\xi$ is a positive constant added according to the ratio of the Lebesgue weighted L2-norm error and the Lebesgue weighting, see Eq. \ref{eq:lebesgueWeight}, such that
\begin{align}
	0 \leq \xi \leq \frac{1}{\uplambda(\mathcal{X})} \lVert \tilde{g} - \sqrt{\pi} \rVert_{L^2_{\uplambda}(x)}^2\, \,  \text{\cite[Eq. 35]{cui2022deep}}. \label{eq:gamErr}
\end{align}
This leads to the normalised target function
\begin{align}
	f_X(\bm{x})  \approx \frac{1}{z} \left( \uplambda(\bm{x}) \xi  + \uplambda(\bm{x}) \tilde{g}(\bm{x})^2 \right)\, \,  \text{\cite[Eq. 19]{cui2022deep}},
\end{align}
which is the normalisation constant $z = \int_{\mathcal{X}} f_X(\bm{x}) \diff \bm{x} $.
Given the tensor train approximation of $\sqrt{\pi}$, the marginal function $f_{X_k}(x_k)$ can be expressed as
\begin{align}
	\begin{split}
		f_{X_k}(x_k)  \approx \frac{1}{z} \Bigg(&\xi \prod_{i=1}^{k-1} \uplambda_i(\mathcal{X}_i) \prod_{i=k+1}^{d} \uplambda_i(\mathcal{X}_i) \\
		&+ \left( \int_{\mathbb{R}} \uplambda_1(x_1) \bm{G}_1^2(x_1)  \, \diff x_1 \right) \cdots 
		\left( \int_{\mathbb{R}} \uplambda_{k-1}(x_{k-1}) \bm{G}_{k-1}^2(x_{k-1}) \, \diff x_{k-1} \right) \\
		& \uplambda_k(x_k) \bm{G}_k^2(x_k)  \\
		&\left( \int_{\mathbb{R}} \uplambda_{k+1}(x_{k+1}) \bm{G}_{k+1}^2(x_{k+1})  \,\diff x_{k+1} \right) \cdots 
		\left( \int_{\mathbb{R}} \uplambda_d(x_d) \bm{G}_d^2(x_d)  \, \diff x_d \right) \Bigg).
	\end{split}
\end{align}




\subsection{Marginal Functions}
\label{subsec:TTMarg}
TT-approximations are handy when approximating integrals, as marginal functions can be easily computed which may simplify the integration significantly.
We compute those by a procedure, to which Cui et al. \cite{cui2022deep} refer to as backwards marginalisation, see Prop. \ref{prob:ForMarg}, and to which I add the forward marginalisation, see Prob. \ref{prob:backMarg}. 
This is similar to the left and right orthogonalisation of TT-cores~\cite{oseledets2011tensor, Oseledets2011DMRG}.
The backwards marginalisation provides us with the coefficient matrices $\bm{B}_k$, while the forward marginalisation gives the coefficient matrices $\bm{B}_{\text{pre}, k}$. 
These matrices enable the efficient evaluation of marginal functions since they integrate over the coordinates either left or right of the $k$-th dimension, as in \cite{cui2022deep}.
In doing so, we define the mass matrix $\bm{M}_k \in \mathbb{R}^{n_k \times n_k}$ as
\begin{equation}
	\bm{M}_k[i, j] = \int_{\mathcal{X}_k} \phi^{(i)}_k(x_k) \phi^{(j)}_k(x_k) \uplambda(x_k) \, \diff x_k, \quad i, j = 1, \dots, n_k, \, \,  \text{\cite[Eq. 22]{cui2022deep}},
\end{equation}
where $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ denotes the set of basis functions for the $k$-th coordinate.
The proposition used to compute $\bm{B}_k$, stated in Proposition~\ref{prob:backMarg}, is adapted directly from~\cite{cui2022deep}.

\begin{prop}[Backwards Marginalisation as in~\cite{cui2022deep}]
	\label{prob:backMarg}
	Starting with the last coordinate $k = d$, we set $\bm{B}_d = \bm{A}_d$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{k-1} \in \mathbb{R}^{r_{k-2} \times n_{k-1} \times r_{k-1}}$, which we need for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_k[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{B}_k[\alpha_{k-1}, i, l_k] \bm{L}_k[i, \tau] \text{\cite[Eq. (27)]{cui2022deep}}.
		\end{equation}
		\item Unfold $\bm{C}_k$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_k^{(R)} \in \mathbb{R}^{r_{k-1} \times (n_k r_k)}$:
		\begin{equation}
			\bm{Q}_k \bm{R}_k = {(\bm{C}_k^{(R)})}^{\top} \, \,  \text{\cite[Eq. 28]{cui2022deep}}.
		\end{equation}
		\item Compute the new coefficient tensor:
		\begin{equation}
			\bm{B}_{k-1}[\alpha_{k-2}, i, l_{k-1}] = \sum_{\alpha_{k-1}=1}^{r_{k-1}} \bm{A}_{k-1}[\alpha_{k-2}, i, \alpha_{k-1}] \bm{R}_k[l_{k-1}, \alpha_{k-1}]\, \,  \text{\cite[Eq. 29]{cui2022deep}}.
		\end{equation}
	\end{enumerate}
\end{prop}

\begin{prop}[Forward Marginalisation]
	\label{prob:ForMarg}
	Starting with the first coordinate $k = 1$, we set $\bm{B}_{\text{pre},1} = \bm{A}_1$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{\text{pre},k+1} \in \mathbb{R}^{r_{k} \times n_{k+1} \times r_{k+1}}$ for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_{\text{pre},k}[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{L}_k[i, \tau] \bm{B}_{\text{pre},k}[\alpha_{k-1}, i, l_k] .
		\end{equation}
		\item Unfold $\bm{C}_{\text{pre},k}$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_{\text{pre},k}^{(R)} \in \mathbb{R}^{(r_{k-1} n_k ) \times r_k}$:
		\begin{equation}
			\bm{Q}_{pre,k}\bm{R}_{\text{pre},k} = {(\bm{C}_{\text{pre},k}^{(R)})}.
		\end{equation}
		\item Compute the new coefficient tensor $\bm{B}_{\text{pre}, k+1} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k} $:
		\begin{equation}
			\bm{B}_{\text{pre}, k+1}[l_{k+1}, i, \alpha_{k+1}] = \sum_{\alpha_{k}=1}^{r_{k}} \bm{R}_{\text{pre},k}[l_{k+1}, \alpha_{k}] \bm{A}_{k+1}[\alpha_{k}, i, \alpha_{k+1}] .
		\end{equation}
	\end{enumerate}
\end{prop}
After computing the coefficient tensors $\bm{B}_{\text{pre}, k+1}$ as in Prop.~\ref{prob:ForMarg} and $\bm{B}_{k+1}$ from Prop.~\ref{prob:backMarg}, the marginal PDF of $k$-th dimension can be expressed as
\begin{equation}
	f_{X_k}(x_k)  \approx \frac{1}{z} \left(\xi \prod_{i=1}^{k-1} \uplambda_i(X_i) \prod_{i=k+1}^{d} \uplambda_i(X_i) + \sum_{l_{k-1}=1}^{r_{k-1}} \sum_{l_k=1}^{r_k} \left(\sum_{i=1}^{n} \phi^{(i)}_k(x_k) \bm{D}_k[l_{k-1},i, l_k] \right)^2 \right) \uplambda_k(x_k),
\end{equation}
where $\bm{D}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ and $\bm{R}_{\text{pre},k-1}\in \mathbb{R}^{r_{k-1} \times r_{k-1}}$ and $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$
\begin{equation}
	\bm{D}_k[l_{k-1},i,l_k] = \sum_{\alpha_{k-1}=1}^{r_{k-1}}  \bm{R}_{\text{pre},k-1}[l_{k-1}, \alpha_{k-1}] \bm{B}_k[\alpha_{k-1}, i, l_k].
\end{equation}

For the first dimension, $f_{X_1}(x_1)$ can be expressed as
\begin{equation}
	f_{X_1}(x_1)  \approx \frac{1}{z} \left(\xi \prod_{i=2}^{d} \uplambda_i(\mathcal{X}_i) + \sum_{l_1=1}^{r_1} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_1[i, l_1] \right)^2 \right) \uplambda_1(x_1)\, \,  \text{\cite[Eq. 30]{cui2022deep}},
	\label{eq:firstMarg}
\end{equation}
where $\bm{D}_1[i, l_1] = \bm{B}_1[\alpha_0, i, l_1]$ and $\alpha_0 = 1$,
and similarly in the last dimension
\begin{equation}
	f_{X_d}(x_d)  \approx \frac{1}{z} \left(\xi \prod_{i=1}^{d-1} \uplambda_i(\mathcal{X}_i) + \sum_{l_{n-1}=1}^{r_{d-1}} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_d[l_{n-1},i] \right)^2 \right) \uplambda_d(x_d),
\end{equation}
where $\bm{D}_d[l_{n-1},i] = \bm{B}_{\text{pre},d}[l_{n-1}, i, \alpha_{n+1}]$ and $\alpha_{d+1} = 1$.
Note that we calculate the normalisation numerically within the process of finding the marginals so that $\sum f_{X_k}(x_k) = 1$.

\subsection{Sampling from the TT Approximation}
If instead of evaluating integrals we like to draw samples from the approximated function we do this via the inverse Rosenblatt transform (IRT), as in \cite{dolgov2020approximation}, to preserve the correlation structure.
Since we approximate the square root of the target function, Cui et. al. \cite{cui2022deep} call that the squared inverse Rosenblatt transform (SIRT).

\begin{algorithm}[!th]
	\caption{Squared Inverse Rosenblatt Transform (SIRT)}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} seeds $\{ \bm{u}^{(1)},\dots, \bm{u}^{(N)} \} \sim \mathcal{U}(0,1)^d $ and $\bm{B}_1 , \dots,\bm{B}_d$  from Prob. \ref{prob:backMarg}
		\FOR{ \( s = 1, \dots, N\)}
		\FOR{ \( k = 1, \dots, d\)}
		\STATE compute normalised PDF $ f_{X_k|X_{<k}}(x_k|x^{(s)}_{k-1},\dots,x^{(s)}_1)$, Eq. \ref{eq:CurrMarg}
		\STATE compute cumulative distribution function $F_{X_k|X_{<k}}(x_k)$, Eq. \ref{eq:CurrCDF},
		\STATE project sample $x^{(s)}_k = F_{X_k|X_{<k}}^{-1}(u^{(s)}_k)$
		\STATE interpolate $\bm{G}_k(x^{(s)}_k)$, Eq. \ref{eq:LinPol}
		\STATE update $\bm{G}_{\leq k}(x^{(s)}_{\leq k}) = \bm{G}_{<k}(x^{(s)}_{<k}) \bm{G}_k(x^{(s)}_k)$
		\ENDFOR
		\ENDFOR
		\STATE \textbf{Output:} samples $\{ \bm{x}^{(1)},\dots, \bm{x}^{(N)} \} $, where each $\bm{x}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N$
	\end{algorithmic}
	\label{alg:SIRT}
\end{algorithm}
We start by calculating the Backward marginals $\bm{B}_1 , \dots,\bm{B}_d$ as in Prob \ref{prob:backMarg} and draw $N$ uniformly distributed seeds $\{ \bm{u}^{(1)},\dots, \bm{u}^{(N)} \} \sim \mathcal{U}(0,1)^d $, where each $\bm{u}^{(s)}$ is d-dimensional for $s = 1, \dots, N$.
Then we calculate the first marginal $f_{X_1}(x_1)$ as in Eq. \ref{eq:firstMarg} and normalise with $z = \int_{\mathcal{X}_1} f_{X_1}(x_1) d x_1$.
Next, we compute the cumulative distribution function (CDF)
\begin{align}
	F_{X_k|X_{<k}}(x_k) \approx \int_{-\infty}^{x_k} f_{X_k|X_{<k}}(\tilde{x}_k|x_{k-1},\dots,x_1) \diff \tilde{x}_k  \, \, \text{\cite[Eq. 17]{cui2022deep}}
	\label{eq:CurrCDF}
\end{align}
for the first dimension $k = 1$ and then project the seed  on the parameter space $x^{(s)}_k = F_{X_k}^{-1}(u^{(s)}_k)$.
Once that is done, we use a piecewise polynomial interpolation
\begin{align}
	\bm{G}_k(x^{(s)}_k) \approx   \frac{x^{(s)}_k - x^{(i)}_k }{x^{(i+1)}_k -x^{(i)}_k } \bm{G}_k(x^{(i+1)}_k) + \frac{ x^{(i+1)}_k - x^{(s)}_k}{x^{(i+1)}_k -x^{(i)}_k } \bm{G}_k(x^{(i)}_k)
	\label{eq:LinPol}
\end{align}
for $x^{(i)}_k \leq x^{(s)}_k \leq x^{(i+1)}_k$ in between two grid points $i$ and $i+1$ as in \cite{dolgov2020approximation}.
Through $\bm{G}_k(x^{(s)}_k)\in \mathbb{R}^{1 \times r_{k-1}}$ we condition on the previous samples, which denotes the product of all approximated tensors of the previous $k-1$ samples to preserve the correlation structure.
Then we marginalise over the dimensions $k+1 , \dots, d$ via $\bm{B}_k$ so that the next "conditional marginal" is given as:
\begin{align}\begin{split} 
	 f_{X_k|X_{<k}}(x_k|x^{(s)}_{k-1},\dots,x^{(s)}_1) \approx \frac{1}{z}
\Bigg( 
\xi \prod_{i=k+1}^{d} \uplambda_i(X_i) +&  \\
\sum_{l_{k} = 1}^{r_{k}} \Bigg( \sum_{i = 1}^{n}  \phi^{(i)}_k(x^{(s)}_k) \bigg( \sum_{\alpha_{k-1} = 1}^{r_{k-1}} \bm{G}^{(\alpha_{k-1})}_{<k}(x^{(s)}_{<k}) &\bm{B}_k[\alpha_{k-1},i,l_k] \bigg) \Bigg)^2 \Bigg) \uplambda_k(x_k) \, \,  \text{\cite[Eq. 31]{cui2022deep}}.
	\end{split} 
	\label{eq:CurrMarg} 
\end{align}
We repeat the procedure for each $u^{(s)}_k \in \bm{u}^{(s)}$ to gain samples $\bm{x}^{(s)} \sim f_{X}(x)$, see algorithmic box \ref{alg:SIRT} for a summarised version.




%Note that with Cartesian basis $ \sum \phi^{(i)}_k(x_k) \Bigg( \sum \bm{G}^{(\alpha_{k-1})}_{<k}(x_{<k}) \bm{B}_k[\alpha_{k-1},i,l_k] \Bigg) \Bigg)^2 
%$  and $\bm{G}_{<k}(x^{(s)}_{<k}) = \bm{G}_{1}(x^{(s)}_{1}) \cdots \bm{G}_{k-1}(x^{(s)}_{k-1}) $ are simple matrix multiplications for each grid point $i$ or sample $\bm{x}^{(s)}$.


\subsubsection{MH - correction step}
\begin{algorithm}[!ht]
	\caption{MH correction step}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} samples $\{ \bm{x}^{(1)},\dots, \bm{x}^{(N+1)} \} $, where each $\bm{x}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N+1$
		\FOR{ \( s = 1, \dots, N\)}
		\STATE compute MH ratio $\frac{w^{(s+1)}}{w^{(s)} } =\frac{\pi(\bm{x}^{(s+1)})}{\pi(\bm{x}^{(s)})} \frac{f_X(\bm{x}^{(s)})}{f_X(\bm{x}^{(s+1)})}$ 
		\STATE compute acceptance probability $\alpha = \text{min}(w^{(s+1)}/w^{(s)}, 1)$ 
		\STATE Draw $u \sim \mathcal{U}(0,1)$
		\IF{$\alpha \geq u$ }
		\STATE Accept and set $\bm{x}_{\text{MH}}^{(s+1)} = \bm{x}^{(s+1)}$
		\ELSE  
		\STATE Reject and keep $\bm{x}_{\text{MH}}^{(s+1)} = \bm{x}^{(s)}$
		\ENDIF
		\ENDFOR
		\STATE \textbf{Output:} corrected sample chain $\{ \bm{x}_{\text{MH}}^{(1)},\dots, \bm{x}_{\text{MH}}^{(N)} \} $, where each $\bm{x}_{\text{MH}}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N$
	\end{algorithmic}
	\label{alg:SIRT}
\end{algorithm}
Since the samples using the SIRT scheme are samples from an approximation it is sensible to correct those using a Metropolis-Hastings importance sampler.
In doing so we compute the acceptance probability $  \alpha = \text{min}(w^{(s+1)}/w^{(s)}, 1)$, where 
\begin{align}
	w(x) = \frac{\pi(\bm{x})}{f_X(\bm{x})} = \frac{\pi(\bm{x})}{\gamma + \tilde{g}(\bm{x})^2} 
\end{align}
is the importance ratio.
In practise we calculate the importance ratio in the log space, where $\log f_X(\bm{x})  =  \log f_{X_1}(x_1) + \log f_{X_2|X_1}(x_2|x_1) + \cdots + \log f_{X_k|X_{<k}}(x_k|x_{k-1},\dots,x_1)$ is given as in Eq. \ref{eq:CurrMarg}, see \cite{dolgov2020approximation}.
In theory this leads to the corrected chain $ \{ \bm{x}_{\text{MH}}^{(1)},\dots, \bm{x}_{\text{MH}}^{(N)}  \} \sim \pi(\bm{x}) $.


\subsection{On the Error in the TT Approximation}
A straight forward way to asses the error from the TT approximation is to calculate the relative root mean squared error (RMS)
\begin{align}
	\Bigg( \frac{ \int_{\mathcal{X}} (\pi(\bm{x}) - (\gamma + \tilde{g}(\bm{x})^2))^2 \uplambda(\bm{x}) \diff \bm{x}}{ \int_{\mathcal{X}} \pi(\bm{x})^2 \uplambda(x)  \diff \bm{x} } \Bigg)^{1/2} =	\frac{\lVert 	\pi(\bm{x}) - (\gamma + \tilde{g}(\bm{x})^2)  \rVert_{L^2_{\uplambda}(\mathcal{X})}}{\lVert 	\pi(\bm{x}) \rVert_{L^2_{\uplambda}(\mathcal{X})}  } \, .
\end{align}
We can approximate the this integral as 
\begin{align}
	\Bigg( \frac{1}{N} \sum^{N}_{i =1} \Big(\pi(\bm{x}^{(i)}) - \big(\gamma + \tilde{g}(\bm{x}^{(i)})^2\big)\Big)^2 \uplambda(\bm{x})\Bigg)^{1/2}    \approx \Bigg(  \int_{\mathcal{X}} (\pi(\bm{x}) - \big(\gamma + \tilde{g}(\bm{x})^2\big)\Big)^2 \uplambda(x) \diff \bm{x} \Bigg)^{1/2} 
\end{align}
and similarly $\int_{\mathcal{X}} \pi(\bm{x})^2 \uplambda(\bm{x})  \diff \bm{x}$.


\subsubsection{Absolute Error Bound}
One way to assess the error between two distributions is to calculate the Wasserstein distance, because the Kantorovich-Rubinstein duality, as in \cite{thickstun2019kantorovich, Ambrosio2024Kanta}, says that the 1-Wasserstein distance is equal to the upper bound of differences in expectations of a function $h$ between two probability distributions.

We define the 1-Wasserstein distance as
\begin{align}
	W_1(\pi,\tilde{\pi}) = \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}}\int_{\mathcal{X} \times \mathcal{X}} c(\bm{x},\tilde{\bm{x}}) \, \nu(\bm{x},\tilde{\bm{x}}) d\bm{x} d\tilde{\bm{x}}
	\label{eq:wass} \, ,
\end{align}
where $\nu$ couples $\bm{x}$ and $\tilde{\bm{x}}$ so that the integral over the distance $ c(\bm{x},\tilde{\bm{x}}) $ weighted by the probability measures $\pi$ and $\tilde{\pi}$ is the greatest lower bound of all integrals with respect to a $\nu$ in the set of all couplings $ \Pi(\pi,\tilde{\pi})$.
Often $\nu$ is called a transport plan, where $c(\bm{x},\tilde{\bm{x}})$ is the (ground) cost function, and $\nu(\bm{x}, \tilde{\bm{x}})$ is related to the mass which has to be transported and the 1-Wasserstein distance is the earth mover distance.
On the other hand (Kantorovich-Rubinstein duality), we can describe the 1-Wasserstein distance 
\begin{align}
	W_1(\pi,\tilde{\pi})  =& \underset{ \lVert h(\bm{x})- h(\tilde{\bm{x}}) \rVert_{L^2} \leq   \lVert \bm{x} - \tilde{\bm{x}}  \rVert_{L^2} }{ \text{sup}} \Bigg\{  \int_{\mathcal{X}} h(\bm{x}) d \pi (\bm{x})  - \int_{\mathcal{X}} h(\tilde{\bm{x}}) d \tilde{\pi} (\tilde{\bm{x}}) \Bigg\} \\
	=& \underset{ \lVert h(\bm{x})- h(\tilde{\bm{x}}) \rVert_{L^2} \leq \lVert \bm{x} -\tilde{\bm{x}}  \rVert_{L^2} }{ \text{sup}}  \Bigg\{  \underset{\bm{x} \sim  \pi }{\mathbb{E}} \big[ h(\bm{x}) \big]  -  \underset{\tilde{\bm{x}}\sim \tilde{\pi}}{\mathbb{E}} \big[ h(\tilde{\bm{x}}) \big] \Bigg\} .
\end{align}
as the lowest upper bound of differences in expecations of the 1-Lipschitz function $h$ in between the two distributions $\pi$ and $\tilde{\pi}$, with distance measure $c(\bm{x},\tilde{\bm{x}})= \lVert \bm{x} -\tilde{\bm{x}} \rVert_{L^2} $ for $\mathcal{X} \in \mathbb{R}^d$.
For two sample sets $\{ \bm{x}^{(1)},\dots,\bm{x}^{(N)}\} \sim \pi$ and $\{\tilde{ \bm{x}}^{(1)},\dots,\tilde{\bm{x}}^{(M)}\} \sim \tilde{\pi}$ the calculation of the Wasserstein distance becomes an optimisation problem, that is to find the best coupling of samples weighted by their distribution value according to an appropriate distance measure \cite{feydy2020OT}.
More specifically the 1-Wasserstein distance becomes
\begin{align}
W_1(\pi,\tilde{\pi}) = 	\underset{\nu \in \Pi(\pi,\tilde{\pi}) }{\text{min}} \sum^M_{j = 1} \sum^N_{i =1}  \nu_{ij} \lVert\bm{x}^{(i)}  -  \tilde{\bm{x}}^{(j)} \rVert_{L^2} \, ,
\end{align}
where the transport plan $\nu \in \mathbb{R}^{N \times M}_ {\geq 0}$ defines the copling $\nu_{ij} \in \nu $ as $ \nu_{ij} \coloneqq \pi(\bm{x}^{(i)}) \tilde{\pi}(\tilde{\bm{x}}^{(j)})$ similar to \cite[Eq. 3.166]{feydy2020OT}.
Additionally we require that $\sum^N_{i =1} \pi(\bm{x}^{(i)}) = \sum^M_{j = 1} \tilde{\pi}(\tilde{\bm{x}}^{(j)})= 1 $.
This gives us an upper bound of the absolute error in between the expected value of any 1-Lipschitz function $h$, e.g the upper bound of absolute differences in means related to the probaility measures $\pi$ and $\tilde{\pi}$. 

%Now consider the 2-Wasserstein distance
%\begin{align}
%	W_2(\pi,\tilde{\pi}) = \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}}  \Bigg(\int_{\mathcal{X} \times \mathcal{X}} c(\bm{x},\tilde{\bm{x}})^2 \, \nu(\bm{x},\tilde{\bm{x}}) d\bm{x} d\tilde{\bm{x}} \Bigg)^{1/2}
%	\label{eq:wass} \, ,
%\end{align}
%which for two multivariate normal distributions $\pi \sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$ and $\tilde{\pi} \sim \mathcal{N}(\tilde{\bm{\mu}},  \tilde{\bm{\Sigma}})$ is defined as 
%\begin{align}
%	W_2(\pi,\tilde{\pi}) = \Bigg(  \lVert \bm{\mu} -\tilde{\bm{\mu}} \rVert^2_{L^2} + \text{trace}(\bm{\Sigma}+  \tilde{ \bm{\Sigma}} - 2 (\tilde{ \bm{\Sigma}}^{1/2} \bm{\Sigma} \tilde{ \bm{\Sigma}}^{1/2} )^{1/2} )  \Bigg)^{1/2} \, \text{\cite{Asuka2011Wasser},}
%\end{align}
%with distance measure set to $c(\bm{x},\tilde{\bm{x}})= \lVert \bm{x} - \tilde{\bm{x}}\rVert_{L^2} $.
%To relate that to the 1-Wassertein distance, we write the p-Wassertein distance as the infimum of expectations
%\begin{align}
%	W_p(\pi,\tilde{\pi}) =  \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}} \Bigg( \underset{ \bm{x},\tilde{\bm{x}} \sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^p \Bigg)^{1/p}
%\end{align}
%and one can show that
%\begin{align}
% \Bigg(   \underset{ \bm{x},\tilde{\bm{x}} \sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^p \Bigg)^{1/p} \leq  \Bigg(  \underset{ \bm{x},\tilde{\bm{x}}\sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^q \Bigg)^{1/q}
%\end{align}
%for every $p \leq q$, \cite{Chizat2020LecNot}.
%Then we can bound the absolute difference in expectation of a 1-Lipschitz function $h$ by the 2-Wasserstein distance, so that $W_1(\pi,\tilde{\pi}) \leq W_2(\pi,\tilde{\pi})$.


\section{Affine Map}
\label{sec:affine}
The forward map, which we introduce in Ch. \ref{ch:formodel}, poses a weakly non-linear forward problem, which we could tackle by treating the problem as a linear problem and then iteratively updating the non-linear part after each parameter sample.
Instead, we approximate the non-linear model using an affine map $ \bm{M}:\bm{A}_L \rightarrow \bm{A}_{NL}$, which maps from the linear model to the non-linear model, so that we set $ \bm{A} = \bm{M} \bm{A}_{NL} \approx \bm{A}_{NL} $.
Here, we give a brief introduction to affine maps and present our approach to calculating the affine map deterministically. 
Alternatively, one can also determine this map using other methods, e.g. machine learning methods or matrix inversion.


An affine map is any linear map between two vector spaces or affine spaces, where an affine space does not need to preserve a zero origin, see~\cite[Def. 2.3.1]{berger2009geometry}.
In other words, an affine map does not need to map to the origin of the associated vector space or be a linear map on vector spaces, including a translation, or, in the words of my supervisor, C. F., an affine map is a Taylor series of first order.
For more information on affine spaces and maps, we refer to the books \cite{berger2009geometry, katsumi1994affine}
\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[rectnode] at (2,-4) (NL)    {$\bm{V}$};
		\node[rectnode] at (-2,-4) (L)    {$\bm{W}$};
		\draw[<-, very thick] (NL.west) -- (L.east); 
		\node[align=center] at (-5.5,-4) (f3) {linear forward model};
		\node[align=center] at (5.5,-4) (f4) {non-linear forward model};
		\node[align=center] at (0,-5) (f5) {$\bm{A}_{NL} \bm{x}  \approx \bm{M A}_L  \bm{x} = \bm{A x} $ };
		\node[align=center] at (0,-4) (f5) {affine Map \\ $\bm{M}$};
	\end{tikzpicture}
	\caption[Schematics of the affine map]{This Figure shows the schematic representation of the affine map $\bm{M}$, which approximates the non-linear forward model from the linear forward model. Here, $\bm{V}$ contains values produced by the linear forward model, and $\bm{W}$ contains the corresponding values from the non-linear forward model. Both $\bm{V}$ and $\bm{W}$ are affine subspaces over the same field. The affine map $\bm{M}$ projects elements from the linear forward model space $\bm{V}$ onto their counterparts in the non-linear forward model space $\bm{W}$.
	}
\end{figure}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

Consequently, to map between the linear and non-linear forward map, we generate two affine subspaces $\bm{V}$ and $\bm{W}$ over the same field.
Assume we draw samples $\{\bm{x}^{(1)}, \dots, \bm{x}^{(j)}, \dots ,\bm{x}^{(m)}\} \sim \pi(\bm{x}|\bm{y})$ and the affine subspace associated with the linear forward model is \begin{align}
	\bm{W} = \begin{bmatrix}
		\vert&   &  \vert & & \vert \\
		\bm{A}_{L} \bm{x}^{(1)} &  \cdots& \bm{A}_{L} \bm{x}^{(j)} &  \cdots & \bm{A}_{L} \bm{x}^{(m)} \\
		\vert&   &  \vert & & \vert 
	\end{bmatrix}
	%	= \begin{bmatrix}
  %\begin{array}{ccc}
	%\horzbar & w_{1} & \horzbar \\
	%	& \vdots    &          \\
	%\horzbar & w_{j} & \horzbar \\
	%& \vdots    &          \\
	%\horzbar &w_{m} & \horzbar
%end{array}
%	\end{bmatrix} 
\in \mathbb{R}^{m \times m}
\end{align} and with the non-linear forward model is 
\begin{align}
	\bm{V} = \begin{bmatrix}
		\vert&   &  \vert & & \vert \\
		\bm{A}_{NL}\bm{x}^{(1)} &  \cdots& \bm{A}_{NL}\bm{x}^{(j)} &  \cdots & \bm{A}_{NL} \bm{x}^{(m)}  \\
		\vert&   &  \vert & & \vert 
	\end{bmatrix} = 
		\begin{bmatrix}
	\begin{array}{ccc}
		\horzbar & v_{1} & \horzbar \\
		& \vdots    &          \\
		\horzbar & v_{j} & \horzbar \\
		& \vdots    &          \\
		\horzbar &v_{m} & \horzbar
	\end{array}
\end{bmatrix}\in \mathbb{R}^{m \times m} \, .
\end{align}
Then the we calculate affine map 
\begin{align}
	\bm{V}\bm{W}^{-1} = \bm{M} =
		\begin{bmatrix}
	\begin{array}{ccc}
		\horzbar & r_{1} & \horzbar \\
		& \vdots    &          \\
		\horzbar & r_{j} & \horzbar \\
		& \vdots    &          \\
		\horzbar &r_{m} & \horzbar
	\end{array}
\end{bmatrix}\, \in \mathbb{R}^{m \times m} ,
\end{align}
using that, for the forward model in this thesis, each row of $\bm{V}$ is independent of each other, we solve $v_j =r_j \bm{W} $ for each row $ r_j  \in \bm{M}$, where $j = 1, \dots, m$.


\section{Regularisation}
\label{sec:regularise}
As mentioned in the introduction, the currently most used method to analyse data in atmospheric physics is regularisation-based.
Since we want to show that our methods are computationally comparable if not faster, and provide more information than regularisation, we choose a linear-Gaussian Bayesian framework closest to our regulariser, see section \ref{sec:BayModel}.

The Tikhonov regularisation approach provides one solution $\bm{x}_{\lambda}$ that minimises both the data misfit norm
\begin{align}
	\left\lVert \bm{y} - \bm{A} \bm{x} \right\rVert
\end{align} and a regularisation semi-norm
\begin{align}
	\lambda \left\lVert \bm{T} \bm{x} \right\rVert \, , \label{semiNorm}
\end{align}
for a given regularisation parameter $\lambda > 0 $ as described in~\cite{fox2016fast}, with a linear forward model matrix $\bm{A}$, the data $\bm{y}$, a regularisation operator $\bm{T}$.
The regularisation parameter weights the semi-norm and penalises $\bm{x}$ according to that.
If $\lambda$ is large, then the effect of the data on the solution $\bm{x}_{\lambda}$ is small or negligible.
If $\lambda$ is small, the solution $\bm{x}_{\lambda}$ will be dominated by the noisy data, resulting in an overfitted $\bm{x}_{\lambda}$.
We refer to  \cite{hansen1989GSVD} and \cite{tan2016LecNot} for a more comprehensive analysis of the effects of the regularisation parameter on the solution, e.g. due to small singular values of the forward model.

For a fixed $\lambda$, the regularised solution
\begin{align}
	\bm{x}_{\lambda} = \underset{\bm{x}}{\mathrm{arg\,min}} \left\lVert \bm{y} - \bm{A} \bm{x} \right\rVert^2 + \lambda \left\lVert \bm{T} \bm{x} \right\rVert^2
\end{align}
is obtained by taking the derivative with respect to $\bm{x}$:
\begin{align}
	& & \nabla_{\bm{x}} \left\{ (\bm{y} - \bm{A} \bm{x})^T (\bm{y} - \bm{A} \bm{x}) + \lambda \bm{x}^T \bm{T}^T \bm{T} \bm{x} \right\} &= 0 \\
	&\iff & \nabla_{\bm{x}} \left\{ \bm{y}^T \bm{y} + \bm{x}^T \bm{A}^T \bm{A} \bm{x} - 2 \bm{y}^T \bm{A} \bm{x} + \lambda \bm{x}^T \bm{T}^T \bm{T} \bm{x} \right\} &= 0 \\
	&\iff & 2 \bm{A}^T \bm{A} \bm{x} - 2 \bm{A}^T \bm{y} + 2 \lambda \bm{T}^T \bm{T} \bm{x} &= 0,
\end{align}
also know as the "regularised normal equations" $\bm{A}^T \bm{y} = \bm{A}^T \bm{A} \bm{x} + \lambda \bm{T}^T \bm{T} \bm{x}$ \cite{Hansen2001LCurve}.
Solving this equation yields the regularised solution
\begin{align}
	\bm{x}_{\lambda} = (\bm{A}^T \bm{A} + \lambda \bm{L})^{-1} \bm{A}^T \bm{y} \, , \label{eq:regSol}
\end{align}
where we define $\bm{L} \coloneqq \bm{T}^T \bm{T}$, which typically represents a discrete matrix approximation of a differential operator choice~\cite{tan2016LecNot}.
For example
\begin{align}
	\bm{T} = \frac{1}{h}
	\begin{bmatrix}
		-1 & 1 & & &  \\
		0 & -1 & 1 & &   \\
		& \ddots & \ddots & \ddots &\\ 
		& & 0 & -1 & 1  \\
		& & & 0 & -1 
	\end{bmatrix} 
\end{align}
is the first order derivative with equal spacing $h$ as in \cite{tan2016LecNot} then
\begin{align}
	\bm{L} = \frac{1}{h^2}
	\begin{bmatrix}
		1 & -1 & & &  \\
		-1 & 2& -1 & &   \\
		& \ddots & \ddots & \ddots &\\ 
		& & -1 & 2 & -1  \\
		& & & -1 & 2 
	\end{bmatrix} 
\end{align}
is the second order derivative with Neumann boundary conditions, see \cite{wang2015graphs}.

In practice, $\bm{x}_{\lambda}$ is computed for a range of $\lambda$-values and evaluated based on the trade-off between the data misfit and the regularisation semi-norm. The optimal value of $\lambda$ corresponds to the point of maximum curvature of the so-called L-curve~\cite{hansen1993use}, where the data misfit norm versus the regularisation semi-norm is plotted, see Fig.~\ref{fig:LCurve}.

Additionally one can think about regularisation as a Lagrange multiplier $ \mathcal {L}(\bm{x}, \lambda) \coloneq\lambda \bm{x}^T \bm{L} \bm{x} + \left\lVert \bm{y} - \bm{A} \bm{x} \right\rVert $, which minimises $ \bm{x}_\lambda = \text{arg min}_{\bm{x}} \bm{x}^T \bm{L} \bm{x} $ with respect to a constant $\left\lVert \bm{y} - \bm{A} \bm{x} \right\rVert$, see \cite[fn. 6]{fox2016fast} and \cite[Fig. 2.13]{SANTOSH202265}.
So every solution $\bm{x}_{\lambda}$ is an extremum (the most regularised solution for a given data misfit norm) and almost every sample of the posterior, which represents a feasible solution given the data, has a higher $\bm{x}^T \bm{L} \bm{x}$ value and lays above the L-Curve.


%We refer show that the filter factors as in \cite{tan2016LecNot} for a regularisation parameter as shown here affects the solution the generalised singular value decomposition is given in  of $A, T$ as in \cite{hansen1989GSVD}.
%Where on can show the the so-called filter factors are dominated by the default regularisaion solution 
%then the filter factors 
%
%
%
%
%We refer to \cite{hansen1989GSVD} and \cite{tan2016LecNot} for the curious reader, who wants to know how the regularisation parameter affects the reconstruction especially for regions where the forward model does not provide much information (small singular values).
%
%
%
%To show how the regularisation parameter effects the solution one can do a singular value decomposition of $A$
%and the generalised singular value decomposition of $A, T$ as in \cite{hansen1989GSVD}.
%
%Then $\bm{A} = \bm{U} \bm{\Lambda}_A \bm{M}^{-1}$ and $\bm{L} = \bm{V} [\bm{\Lambda}_L,0] \bm{M}^{-1} $
%Where the general singular values $\bm{\Lambda}_A = \text{diag}(\sigma_{A,1},\dots,\sigma_{A,1},1,\dots,1)$ and $\bm{\Lambda}_L = \text{diag}(\sigma_{L,1},\dots,\sigma_{L,1},1,\dots,1)$
%
%
%
%Then one can show that the solution is %$\x_{\lambda} = \bm{M} \bm{F} \bm{U}^T \bm{y} $
%with filter factors
%\begin{align}
%	f_i = \frac{(\sigma_{A,i}/\sigma_{L,i})^2 }{(\sigma_{A,i}/\sigma_{L,i})^2 + \lambda^2} \approx
%	\begin{cases}
%		\frac{(\sigma_{A,i}/\sigma_{L,i})^2}{ \lambda^2},  & \sigma_{A,i}/\sigma_{L,i} \gg \lambda\\
%				1 ,  & \sigma_{A,i}/\sigma_{L,i} \ll \lambda 
%	\end{cases}
%\, \, \text{for}\, i= 1,\dots,p
%\end{align}
%Then small singular values depend on prior information only
%for large $(\sigma_{A,i}/\sigma_{L,i})^2$ singular values the solution is unaffected
%for small $(\sigma_{A,i}/\sigma_{L,i})^2$ singular values the solution is affected by the regularisation parameter




