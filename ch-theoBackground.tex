%\the\columnwidth
\chapter{Theoretical and Technical Background}
\label{ch:background}
\thispagestyle{empty}
This Chapter introduces the hierarchical Bayesian approach to inverse problems, along with key concepts of Markov Chain Monte Carlo (MCMC) methods and tensor-train (TT) approximations for high-dimensional probability distributions.
We keep it as general as possible.
Specific sampling algorithms are not introduced here, as they are specifically tailored towards the structure of the forward model and the particular problem. Therefore, they will be presented in detail when applied.
%We will introduce the Metropolis-within Gibbs in Sec.~\ref{subsec:MWG} and the radnommise then optimse in Sec.~\ref{subsec:RTO}.


\section{Hierarchical Bayesian Inference}
\label{sec:bayes}
\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[roundnode2] at (0,3.5) (th)    {$\bm{\theta}$};
		\node[roundnode2] at (0,1.5) (x)    {$\bm{x}$};
		\node[roundnode2] at (0,-1.5) (u)    {$\Omega$};
		\node[rectnode] at (0,-3.5) (y)    {$\bm{y}$};
		
		\draw[->, very thick] (th.south) -- (x.north); 
		\draw[->, very thick, mydotted] (x.south) -- (u.north); 
		\draw[->, very thick] (u.south) -- (y.north); 
		\draw[->, very thick] (th) edge[bend right=60] (y);  
		
		\node[align=center] at (2.8,3.5) (tht) {$\sim \pi_{\bm{\theta}}(\cdot) $ hyper-parameters};
		\node[align=center] at (2.4,1.5) (xt) {$\sim \pi_{\bm{x}}(\cdot|\bm{\theta}) $ parameters};
		\node[align=center] at (2.5,0) (At) {$\{\bm{A}(\bm{x})\}$ ``noise-free data''};
		\node[align=center] at (3,-1.5) (ut) {space of all measurables};
		\node[align=center] at (2,-3.5) (yt) {$\sim \pi_{\bm{y}}(\cdot|\bm{x},\bm{\theta})$ data};
		\node[align=center] at (-3.25,0) (nt) {noise\\$\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$};
	\end{tikzpicture}
	\caption[Hierarchical Bayesian Model]{A directed acyclic graph (DAG) for an inverse problem visualises statistical dependencies as solid line arrows and deterministic dependencies as dotted arrows.
		The hyper-parameters $\bm{\theta}$ are distributed as ($\sim$) the hyper-prior distribution $\pi(\bm{\theta})$.
		The prior distribution $ \pi_{\bm{x}}(\cdot|\bm{\theta})$ for the parameter $\bm{x}$ and the noise  $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$ are statistically dependent on some of those hyper-parameters.
		Then a parameter $\bm{x} \sim \pi_{\bm{x}}(\cdot|\bm{\theta})$ is deterministically mapped onto the space of all measurables $\Omega=\bm{A}(\bm{x})$ through the forward model.
		From the space of all measurable noise-free data we observe (square box) a data set $\bm{y} = \bm{A}(\bm{x}) + \bm{\eta}$ with some additive random noise, which determines the likelihood function $\pi(\bm{y}|\bm{x},\bm{\theta})$.}
	\label{fig:FirstDAG}
\end{figure}

Assume we observe some data
\begin{align}
	\bm{y} = \bm{A} (\bm{x}) + \bm{\eta},
	\label{eq:NonLinDat}
\end{align}
based on a forward model $\bm{A}(\bm{x})$, which may be non-linear, an unknown parameter vector $\bm{x}$ and some additive random noise $\bm{\eta}$.

Naturally, due to the noise, the observation process in Eq. \ref{eq:NonLinDat} is a random process.
Hence, in Bayesian modelling, the aim is to determine a probability distribution over the parameter $\bm{x}$ given some data $\bm{y}$.
Further, a hierarchical Bayesian model incorporates (auxiliary) hyper-parameters $\bm{\theta}$.
Within a Bayesian approach all unknown hyper-parameters and parameters are treated as random variables \cite[Chapter 3]{kaipio2005statinv}.

According to Bayes' theorem, the joint posterior distribution over the parameters $\bm{x}$ and the hyper-parameter $\bm{\theta}$ is given as
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) = \frac{ \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta})}{\pi(\bm{y})} \propto \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta}) \, ,
\end{align}
with finite and non-zero $\pi(\bm{y})$.
The likelihood function $\pi(\bm{y}|\bm{x},\bm{\theta})$ is defined by the nature of the noise and the noise-free data $\bm{A}(\bm{x})$, which we read as the distribution over $\bm{y}$ conditioned on $\bm{x}$ and $\bm{\theta}$.
Here $\bm{\theta}$ may describe multiple hyper-parameters.
The hyper-parameters can e.g.,~model the noise vector $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$, where $\sim$ reads as ``is distributed as'', and account for physical properties or functional dependencies of $\bm{x}$, such as the smoothness of $\bm{x}$.
Because unknown parameter are treated as random variables the joint prior distribution is introduced as $\pi(\bm{x}, \bm{\theta}) = \pi(\bm{x}|\bm{\theta}) \pi(\bm{\theta})$ with the parameter prior distribution $\pi(\bm{x}|\bm{\theta})$ and the hyper-prior distribution $\pi(\bm{\theta})$.
Choosing these prior distributions is ultimately a modeller's choice and is crucial, as those shall be as uninformative as possible for regions in hyper-parameter and parameter space where the data is informative.
If the data is uninformative, the prior distributions can be informative and may represent a rather restrictive range of (physically) feasible hyper-parameters and parameters.

Fig.~\ref{fig:FirstDAG} visualises the conditional dependencies between hyper-parameters and parameters as well as how distributions progress through to an observation (square box) using a directed acyclic graph (DAG).
We plot statistical dependencies as solid arrows and deterministic dependencies as dotted arrows.
%\textcolor{red}{conditional dependencies and sqaure box is observation}

% not affect the posterior distribution \textcolor{red}{what, of course the prior affects the posterior. What are you trying to say?}

%$\pi(\bm{x},\bm{\theta}|\bm{y}) $ reads as the distribution of $\bm{x}$ and $\bm{\theta}$ given (conditioned on) the data $\bm{y}$. \textcolor{red}{two sentences, talk anbout posterior and then model }

%Here $\bm{\theta}$ models the random noise $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$, 
%The h
%This already incorporates hierarchically ordered modelling as we model the noise thought the hyper-parameters $\bm{\theta}$.
%which we classify through a hyper-parameter, we deal with a random process. \textcolor{red}{Maybe more clear is to say that the observation process (2.1) is a random process.}
%We incorporate that in our hierarchically ordered modelling through the likelihood function $\pi(\bm{y}|\bm{x},\bm{\theta})$, which includes all relevant information captured by the model $\bm{A}(\bm{x})$, and is defined by the measurement process and the nature of the noise. \textcolor{red}{how does the model "capture information"? Oh, do you mean the observation process? Might pay to not use 'model' for lots of things.}
%
%Consequently we define hyper-prior distributions $\pi(\bm{\theta})$ and prior distributions $\pi(\bm{x}|\bm{\theta})$.


%Note that here we include the hyper-parameters within the posterior distribution, which is the key idea of hierarchical Bayesian modelling. \textcolor{red}{"the" key idea? Maybe say that it is intrinsic to Bayes models, or something. You could quote Kaipio and Somersalo that all unknowns are treated as random variables.} \cite[Chapter 3]{kaipio2005statinv}
%


Usually, the objective is to calculate the expectation of a function $h(\bm{x})$, which is defined as
\begin{align}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})] =  \underbrace{\int \int   h(\bm{x}) \,  \pi(\bm{x}, \bm{\theta} | \bm{y} ) \, \diff \bm{x}  \, \diff \bm{\theta}}_{\bar{h}}   \label{eq:expPos} \, .
\end{align}
If it is a high-dimensional integral and computationally not feasible to solve we approximate 
\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})] \approx \underbrace{ \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)})  }_{\bar{h}_N} \, ,
\end{align}
with an the unbiased sample-based Monte-Carlo estimate \cite{roberts2004general} for large enough $N$ (law of large numbers \cite[Chapter 17]{tweedie2009measprob}).
Here the posterior samples $\{\bm{x}^{(k)},\bm{\theta}^{(k)} \}\sim \pi_{\bm{x}, \bm{\theta}}(\cdot|\bm{y})$, for $k = 1, \dots, N$, form a sample set $\mathcal{M} =\{ (\bm{x},\bm{\theta})^{(1)}, \dots ,  (\bm{x},\bm{\theta})^{(N)} \}$.
The central limit theorem states that the sample mean $\bar{h}^{(i)}_N $ of independent sample sets $\mathcal{M}^{(i)}\sim\pi (\bm{x}, \bm{\theta}| \bm{y})$, for $i = 1, \dots, n$, converges to be normally distributed, so that
\begin{align}
	\sqrt{n} (\bar{h}^{(i)}_N -  \bar{h} ) \overset{\mathcal{D}}{\longrightarrow} \mathcal{N} (0,\sigma^2) \text{\cite{geyer1992practical}},
\end{align}
and if $\sigma^2 < \infty$ the Monte-Carlo error $\bar{h}^{(i)}_N -  \bar{h} $ is bounded.
In practice, the Monte-Carlo error from a sample set $\mathcal{M}^{(i)}$ is approximated as
\begin{align}
	(\sigma^{(i)})^2  =  \text{Var}(\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x})]) 
	\approx \frac{\text{Var}(h(\bm{x}) )}{N} \Bigg( \underbrace{  1 + 2 \sum_{t = 1}^{W} \frac{\Gamma(t)}{\Gamma(0)}  }_{ \coloneqq 	\tau_{\text{int}} }\Bigg) = \text{Var}(h(\bm{x})) \frac{ \tau_{\text{int}} }{N} \, , \label{eq:MCerr}
\end{align}
where we have to take into account that the samples generated by any system or algorithm are correlated.
We define the integrated autocorrelation time (IACT) $\tau_{\text{int}}$ as in \cite{fox2016fast} which is a common definition in statistics.
This is twice the value of the IACT in~\cite[pp. 103-105]{wolff2002LecNot} and~\cite{wolff2004monte,drikHesse}, as commonly defined within the physics community.
Here the autocorrelation coefficient $\Gamma(t) \propto \exp\{ - |t| /\tau \} \longrightarrow 0$ for $t \rightarrow \infty$ at lag $t$ decays exponentially and $\Gamma(0) =\text{Var}(h(\bm{x}) ) $.
Choosing the summation window $W$ is crucial because it has to be large compared to the decay time $\tau$, but for too large $t$ the autocorrelation coefficient $\Gamma(t)$ is noise-dominated.
U. Wolff~\cite{wolff2004monte} (and the Python implementation by D. Hesse~\cite{drikHesse}) provide a way to not only calculate the IACT safely but also to quantify the errors of the estimated IACT.

The IACT provides a good estimate of the number of steps the sampling algorithm needs to take to produce one independent sample.
According to the IACT, the effective sample size is defined as $ \tau_{\text{int}} /N$.
We point out that for uncorrelated samples $\tau_{\text{int}} = 1$ the error $(\sigma^{(i)})^2$ is a typical Monte-Carlo estimate.
See Appendix~\ref{ap:IATC} and~\cite{Sokal1997, wolff2004monte, wolff2002LecNot} for a more detailed derivation.

\subsection{Marginal and then Conditional Method}
\label{subsec:TheoMTC}
Quickly generating a representative sample set from the posterior distribution often presents a significant challenge. %\textcolor{red}{this word is misplaced (for English). Better is Quickly generating (it's an adverb)} 
This is mainly due to the strong correlations that usually exist between the parameters and hyper-parameters, as discussed by Rue and Held in~\cite{rue2005gaussian} and illustrated in Appendix~\ref{ap:Correlation}.
%If $\bm{x}$ cannot be parametrised directly in terms of the hyper-parameters $\bm{\theta}$, so that $\bm{x}(\bm{\theta})$ is function of $\bm{\theta}$, it is beneficial to factorise the posterior distribution as \textcolor{red}{this is unclear. Of course x(theta) is a function of theta. What are you trying to say?}
%If $\bm{x}$ cannot be parametrised directly in terms of the hyper-parameters $\bm{\theta}$, and only a low-level representation~\cite{fox2016fast, Watzenig_2009} of $\bm{x}$ is available, 
Depending on the problem and the available model it is beneficial to factorise the joint posterior distribution
\begin{align}
	\pi(\bm{x}, \bm{\theta} |  \bm{y}) = \pi(\bm{x} |  \bm{\theta}, \bm{y}) \, \pi(\bm{\theta} |   \bm{y}) \label{eq:MTC}
\end{align}
into the full conditional posterior $\pi(\bm{x} |  \bm{\theta}, \bm{y})$ over the latent field $\bm{x}$ and the marginal posterior $ \pi(\bm{\theta} |   \bm{y})$ over hyper-parameter $\bm{\theta}$.
This approach, known as the MTC method, is particularly advantageous when $\bm{x}\in \mathbb{R}^n$ is high-dimensional, while $\bm{\theta}$ is low-dimensional and the evaluation of the marginal posterior
\begin{align}
	\pi(\bm{\theta} |   \bm{y}) =  \frac{ \pi(   \bm{y} | \bm{x},\bm{\theta})  \pi( \bm{x} | \bm{\theta} )  \pi(\bm{\theta}) }{ \pi(\bm{x} | \bm{\theta} ,   \bm{y})   \pi( \bm{y})} \propto \frac{ \pi(   \bm{y} | \bm{x},\bm{\theta})  \pi( \bm{x} | \bm{\theta} )  \pi(\bm{\theta}) }{ \pi(\bm{x} | \bm{\theta} ,   \bm{y}) } \label{eq:margGen}\, 
\end{align}
as in~\cite[Lemma 2]{fox2016fast} is relatively cheap.
%Bayesian modelling is especially powerful if a representation of $\bm{x}$ is available so that a number of hyper-parameters much smaller than $n$ effectively represent the parameter space.
%This can reduce the dimensionality of the posterior.
%Further, prior knowledge may be imposed through those hyper-parameters much more precisely.
%See e.g.~Chapter~\ref{ch:FullBay} where ozone, temperature and pressure are retrieved jointly, and a temperature profile $\bm{T}\in \mathbb{R}^{34} $ is described through 14 hyper-parameters as in Eq.~\ref{eq:tempFunc} and only two hyper-parameters are needed to form a pressure profile $\bm{p}\in \mathbb{R}^{34} $ (see Eq.~\ref{eq:pressFunc}).


%In \cite{norton2018sampling}, they classify inverse problems into problems with known or unknown conditional posterior distributions, and conclude that if $\pi(\bm{x} | \bm{\theta} ,   \bm{y}) = \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}| \bm{\theta})  / \pi(   \bm{y}| \bm{\theta})$ has a known form, the normalising constant of $\pi(\bm{\theta} |   \bm{y})$ is available \newline $\int \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}| \bm{\theta}) \text{d} \bm{x} = \pi(   \bm{y}| \bm{\theta})  \propto \pi( \bm{\theta}|\bm{y}) / \pi(\bm{\theta})$ and one can almost surely determine the $\bm{\theta}$-dependence of the marginal posterior $\pi(\bm{\theta} |   \bm{y})$.

%\textcolor{red}{horrible notation, try better.}
Applying the law of total expectation~\cite{champ2022generalizedlawtotalcovariance}, Eq.~\eqref{eq:expPos} becomes
\begin{align}
	\mathbb{E}_{\bm{x} ,\bm{\theta}  |\bm{y}} [h(\bm{x})] &= \int \int   h(\bm{x}) \pi(\bm{x} |  \bm{\theta}, \bm{y}) \, \diff \bm{x} \,  \pi(\bm{\theta} |   \bm{y}) \, \diff \bm{\theta} \\
	&= \int \mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} \left[ h(\bm{x}) \right] \, \pi(\bm{\theta} |  \bm{y}) \, \diff \bm{\theta}\label{eq:2fullCond} \\
		&= \mathbb{E}_{\bm{\theta} |  \bm{y}} \left[ \mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} [h(\bm{x})] \right] \, .
	\label{eq:fullCond}
\end{align}
In the case of a linear-Gaussian hierarchical Bayesian model, both the marginal distribution $\pi (\bm{\theta}| \bm{y})$ %\textcolor{red}{separate sentence (You have a tendency to let sentences wander to different ideas. Split into single topics.)} 
and the inner expectation $\mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} \left[ h(\bm{x}) \right]$ are well defined (see next subsection).
If the integral in Eq.~\ref{eq:2fullCond} is expensive to calculate, we use sample-based methods to produce a Markov chain $\{ (\bm{x}, \bm{\theta})^{(1)}, \dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y}) $ and sample from $\pi(\bm{\theta} |  \bm{y})$ first and then draw samples from the full conditional posterior $\pi(\bm{x} | \bm{\theta} , \bm{y})$ (see Sec.~\ref{subsec:RTO}). 


\subsubsection{Linear-Gaussian hierarchical Bayesian model}
\label{subsec:LinBay}
In case of normally distributed noise $\bm{\eta} \sim \mathcal{N}(\bm{0},\bm{\Sigma}(\bm{\theta}))$ with zero mean and covariance $\bm{\Sigma}(\bm{\theta})$ and a linear forward model matrix $\bm{A}$ Eq.~\ref{eq:NonLinDat} simplifies to
\begin{align}
	\bm{y} = \bm{A} \bm{x} + \bm{\eta} \, .
	\label{eq:LinDat}
\end{align}
Then we can obtain the marginal and full conditional posterior distribution explicitly.
Our hierarchical linear-Gaussian Bayesian model is defined as
\begin{subequations}
	\label{eq:GenBayMode}
	\begin{align}
		\bm{y} |  \bm{x}, \bm{\theta} &\sim \mathcal{N}(\bm{A} \bm{x}, \bm{\Sigma}(\bm{\theta}) ) \\
		\bm{x} |  \bm{\theta} &\sim \mathcal{N}(\bm{\mu}, \bm{Q}(\bm{\theta})^{-1} ) \\
		\bm{\theta} &\sim \pi(\bm{\theta}) \,  ,
	\end{align}
\end{subequations}
with a Gaussian likelihood function $\pi(\bm{y} | \bm{x}, \bm{\theta} )$, a normally distributed prior $\pi(\bm{x}|\bm{\theta})$, with prior mean $\bm{\mu}$ and prior precision $\bm{Q}(\bm{\theta})$, and a hyper-prior distribution $\pi(\bm{\theta})$.
For the derivation of the marginal posterior and the full conditional posterior distribution, consider the joint multivariate Gaussian distribution
\begin{align}
	\begin{pmatrix}
		\bm{x} \\
		\bm{y}
	\end{pmatrix}\sim \mathcal{N}\left[  \begin{pmatrix}
		\bm{\mu} \\
		\bm{A}\bm{\mu}
	\end{pmatrix},\begin{pmatrix}
		\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A} & - \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \\
		\bm{\Sigma}(\bm{\theta})^{-1} \bm{A} & \bm{\Sigma}(\bm{\theta})^{-1} 
	\end{pmatrix}^{-1} \right] \, , 	\label{eq:jointMultiGaus}
\end{align}
with the joint precision matrix as in~\cite{SIMPSON201216} (see also~\cite{rue2005gaussian, fox2016fast}).
Immediately\footnote{$\text{Assume }
		\begin{pmatrix}
			\bm{x} \\
			\bm{y}
		\end{pmatrix}\sim \mathcal{N}\left[  \begin{pmatrix}
			\bm{\mu}_{\bm{x}} \\
			\bm{\mu}_{\bm{y}}
		\end{pmatrix},\begin{pmatrix}
			\bm{Q}_{\bm{x} \bm{x}} & \bm{Q}_{\bm{x} \bm{y}} \\
			\bm{Q}_{\bm{y} \bm{x}} & \bm{Q}_{\bm{y} \bm{y}} 
		\end{pmatrix}^{-1} \right] \text{, then }
		\bm{x} |  \bm{y} \sim \mathcal{N}\big(	\bm{\mu}_{\bm{x}} -   \bm{Q}^{-1}_{\bm{x} \bm{x}} \bm{Q}_{\bm{x} \bm{y}}(\bm{y} - \bm{\mu}_{\bm{y}} ) , \bm{Q}^{-1}_{\bm{x} \bm{x}} \big).$}, the full conditional posterior distribution can be formulated as
\begin{equation}
	\label{eq:CondPostLin}
	 \adjustbox{max width=0.99\textwidth}{$
	\bm{x} | \bm{\theta} , \bm{y}\sim \mathcal{N}\Big(\underbrace{ \bm{\mu} + \big(\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A} \big)^{-1} \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} (\bm{y} - \bm{A}\bm{\mu})}_{\bm{\mu}_{\bm{x} | \bm{\theta} , \bm{y}}},\underbrace{ \big(\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A}\big)^{-1}}_{\bm{\Sigma}_{\bm{x} | \bm{\theta} , \bm{y}}} \Big)\,$.}
\end{equation}
Then the marginal posterior distribution over the hyper-parameters in Eq.~\ref{eq:margGen} is derived as
\begin{align}\begin{split}
		\pi(\bm{\theta} | \bm{y}) \propto & \sqrt{\frac{\det{(\bm{\Sigma}(\bm{\theta})^{-1})} \det{(\bm{Q}(\bm{\theta}))} }{\det{(\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A})} } }  \exp \Bigg\{  -\frac{1}{2} (\bm{y} - \bm{A} \bm{\mu})^T \\ &\Big[ \bm{\Sigma}(\bm{\theta})^{-1} - \bm{\Sigma}(\bm{\theta})^{-1} \bm{A}  \big(\bm{Q}(\bm{\theta}) + \bm{A}^T \bm{\Sigma}(\bm{\theta})^{-1} \bm{A}\big)^{-1} \bm{A}^T \bm{\Sigma} (\bm{\theta})^{-1} \Big] (\bm{y} - \bm{A} \bm{\mu}) \Bigg\} \pi(\bm{\theta}) \, ,
	\end{split} 
\end{align} 
where, as noted by Fox and Norton~\cite{fox2016fast}, the parameter $\bm{x}$ cancels.
Having the marginal posterior distribution $\pi (\bm{\theta}| \bm{y})$ (independent of $\bm{x}$) available breaks up the correlation structure between $\bm{x}$ and $\bm{\theta}$ and makes the MTC approach very efficient~\cite{fox2016fast} (see Appendix~\ref{ap:Correlation}).
Within this scheme, we evaluate the marginal posterior first and then either condition on hyper-parameters to draw full conditional posterior samples $\bm{x} \sim \pi (\bm{x} | \bm{\theta}, \bm{y})$ (see Sec.~\ref{subsec:RTO}) or evaluate the posterior mean
\begin{align}
	\mu_{\bm{x}|\bm{y}} = \int \bm{\mu}_{\bm{x} | \bm{\theta} , \bm{y}} \pi(\bm{\theta}| \bm{y}) \diff \bm{\theta} \label{eq:MeanGenInt}
\end{align} and the posterior covariance matrix
\begin{align}
	\Sigma_{\bm{x}|\bm{y}} = \int \bm{\Sigma}_{\bm{x} | \bm{\theta} , \bm{y}} \pi(\bm{\theta}| \bm{y}) \diff \bm{\theta}\label{eq:CoVarGenInt}
\end{align}
of $\pi(\bm{x}| \bm{y})$ by some quadrature rule.

\section{Sample-Based Estimates via Markov Chain Monte-Carlo Methods}
One may use Markov chain Monte-Carlo (MCMC) methods to calculate sample-based estimates of $\mathbb{E}_{\bm{x} ,\bm{\theta}  |\bm{y}} [h(\bm{x})] $ as in Eq.~\ref{eq:sampMean}.
Within the MTC scheme we draw samples $\{  \bm{\theta}^{(1)}, \dots,  \bm{\theta}^{(k)}, \dots, \bm{\theta}^{(N)} \} \sim \pi(\bm{\theta} | \bm{y})$ from the marginal posterior first and then characterise the full conditional posterior $\pi(\bm{x}|\bm{\theta} , \bm{y})$.
In doing so we generate a Markov chain $\mathcal{M} = \{  \bm{\theta}^{(1)}, \dots,  \bm{\theta}^{(k)}, \dots, \bm{\theta}^{(N)} \} $, where every new sample $\bm{\theta}^{(k)}$ is proposed according to a random proposal $\bm{\theta}^{(k)} \sim q( \cdot |  \bm{\theta}^{(k-1)})$ and only affected by the previous one $\bm{\theta}^{(k-1)}$.
For large enough $N$, this chain of random variables can be used to calculate Monte-Carlo estimates, where ergodicity of the Markov chain $\mathcal{M}$ is a sufficient criterion to do so~\cite{tan2016LecNot, roberts2004general}.

The ergodicity theorem in~\cite{tan2016LecNot} states that, if a Markov chain $\mathcal{M}$ is aperiodic, irreducible, and reversible, then it converges to a unique stationary equilibrium distribution.
In other words, the chain can reach any state from any other state (irreducibility), is not stuck in periodic cycles (aperiodicity), and satisfies the detailed balance condition~\cite{tan2016LecNot} (reversibility).
Then the samples in that chain $ \mathcal{M} \sim \pi( \bm{\theta} |  \bm{y})$ are samples from the desired target distribution.
In practice, one can inspect the trace $\pi(\bm{\theta}^{(k)} |  \bm{y})$ for $k = N_{\text{burn-in}}, \dots, N$ after a ``burn-in'' period $N_{\text{burn-in}}$ and visually assess if the chain is consistent with ergodicity.
The ``burn-in'' period $N_{\text{burn-in}}$ removes initialisation bias.
The specific sampling methods in this thesis possess proven ergodic properties, and we therefore provide the reader with corresponding literature for further details when the methods are introduced.


If the instance $\bm{\theta}^{(k)}$ of an ergodic Markov chain represents an independent sample of the marginal posterior $\pi(\bm{\theta} | \bm{y})$ and $\bm{x}^{(k)}$ is a sample from the full conditional posterior $ \pi(\bm{x}|   \bm{\theta}^{(k)}, \bm{y})$ e.g.,~as in Sec.~\ref{subsec:RTO}, then the resulting sample $(\bm{x}^{(k)}, \bm{\theta}^{(k)})$ is an independent sample from the joint posterior $\pi(\bm{x},   \bm{\theta} | \bm{y})$~\cite{fox2016fast,acosta2022markov}.
Repeating this procedure gives the chain $\{(\bm{x}, \bm{\theta})^{(1)},\dots, (\bm{x}, \bm{\theta})^{(k)} ,\dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x} ,  \bm{\theta} | \bm{y}) $ of independent samples from the joint posterior.

%\cite{acosta2022markov}.
%%\textcolor{red}{reference Jun Liu, or something.}
%$\{ (\bm{x}, \bm{\theta})^{(1)}, \dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y}) =  \pi(\bm{x} |  \bm{\theta} , \bm{y}) \pi( \bm{\theta} | \bm{y})$ is also ergodic \textcolor{red}{for the posterior}~\cite{acosta2022markov}. \textcolor{red}{strictly, ergodic for [theta|y] -- ergodicity is with respect to a particular distribution.}
%
%In this section we present the methods that draw samples from the marginal posterior $ \mathcal{M} = \{  \bm{\theta}^{(1)}, \dots,  \bm{\theta}^{(k)}, \dots, \bm{\theta}^{(N)} \} \sim \pi(\bm{\theta} |  \bm{y})$ \textcolor{red}{this reads as if M is the marginal posterior. Rewrite.} as well as the RTO method to draw samples from a normally distributed full conditional posterior $\pi(\bm{x}|\bm{\theta} , \bm{y})$. \textcolor{red}{what is the "full posterior" as opposed to the "posterior"?}
%
%MCMC methods generate such a Markoc chain $\mathcal{M}$ using random (Monte-Carlo) proposals $(\bm{x}, \bm{\theta})^{(k)} \sim q( \cdot |  \bm{\theta}^{(k-1)})$ \textcolor{red}{This is defining "Markov". . Be more explicit.} according to a proposal distribution conditioned on the previous sample (Markov), where ergodicity of the chain $\mathcal{M}$ is a sufficient criterion for using sample-based estimates~\cite{tan2016LecNot, roberts2004general}. \textcolor{red}{you are a little unclear about a chain of random variables, and an instance of such a chain.}
%convergence  \textcolor{red}{converge to, so after 'burn in'}  period.
%d mixing properties of the chain to evaluate if the behavours is consistent with ergodicty.
%\textcolor{red}{this is a bit loose. really, one evaluates if the behaviour is consistent with ergodicity.}

\clearpage
\section{Numerical Function Approximation -- Tensor-Train (TT)}
\label{sec:tensortrain}
%\textcolor{red}{Explain how to find normalisation constant and say that due to aproxiamting the sqaure root we ensure posivyt later when squaring it}
%First, we provide a short overview of probability spaces and their associated measures. 
%I am not claiming this overview to be complete but consider it helpful to understand the notation in \cite{cui2022deep}, which we follow to approximate functions in the TT format.
Instead of relying on sampling-based methods to explore an unnormalised density function $\pi(\bm{x})$, which in our case will be the marginal posterior distribution over the hyper-parameters, we can approximate this function using a tensor-train (TT) approximation.
The TT approximation $\tilde{\pi}(\bm{x}) \approx \pi(\bm{x})$, where $\bm{x} \in \mathbb{R}^d$, on a $d$-dimensional grid requires far fewer function evaluations compared to conventional sampling methods.
In the following, we describe how to compute a normalised marginal probability density function (PDF) $f_{X_k}(x_k)$, for an $x_k \in \bm{x}$ and $k = 1, \dots,d$, from a target function $\pi(\bm{x})$ approximated in TT format.
Further, a scheme similar to the inverse Rosenblatt transform (IRT) in~\cite{dolgov2020approximation} is introduced to generate samples from $\pi(\bm{x})$.
In doing so, we follow the notation and procedure introduced by Cui and Dolgov~\cite{cui2022deep}.

%\textcolor{red}{We represent a probability distribution on a d-dimensional grid with grid points n in each dimension.}

As in~\cite{cui2022deep}, the parameter space is defined as the product space $\mathcal{X} = \mathcal{X}_1 \times \mathcal{X}_2 \times \dots \times \mathcal{X}_d$ with $ x_k \in \mathcal{X}_k \subseteq \mathbb{R}$.
% and $\bm{x} = ( x_1,\dots ,x_k,\dots,x_d )$.
The marginal PDF for the $k$-th component is then given by
\begin{align}
	f_{X_k}(x_k) = \frac{1}{z} \int_{\mathcal{X}_1} \cdots \int_{\mathcal{X}_{k-1}} \int_{\mathcal{X}_{k+1}} \cdots  \int_{\mathcal{X}_d} \uplambda(\bm{x}) \, \pi(\bm{x}) \, \diff x_1 \cdots \diff x_{k-1} \, \diff x_{k+1} \cdots \diff x_d, \label{eq:margInt}
\end{align}
where we integrate over all dimensions except the $k$-th, and $z$ is a normalisation constant.
Cui and Dolgov~\cite{cui2022deep} refer to $\uplambda(x)$ as the ``product-form Lebesgue-measurable weighting function'', which can be useful for quadrature rules~\cite{davis2007methods}, and define it as
\begin{align}
	\uplambda(\mathcal{X}) = \prod_{i = 1}^{d} \uplambda_i(\mathcal{X}_i), \quad \text{where} \quad \uplambda_i(\mathcal{X}_i) = \int_{\mathcal{X}_i} \uplambda_i(x_i) \, \diff x_i. \label{eq:lebesgueWeight}
\end{align}

\begin{figure}[ht!]
	\centering
	\begin{subfigure}{\textwidth}
		\input{TTSchem.pdf_tex}
		\caption{}
	\end{subfigure}
	\centering
	\begin{subfigure}{\textwidth}
		\begin{tikzpicture} 
			\node[rectnode] at (-5,0) (T1)    {$1 \times n \times r_1$};
			\node[rectnode] at (-2,0) (T2)    {$r_1 \times n \times r_{2}$};
			
			\node[rectnode] at (3,0) (Tn1)    {$r_{d-2} \times n \times r_{d-1} $};
			\node[rectnode] at (6.75,0) (Tn)    {$r_{d-1} \times n \times 1$};
			\draw[-, very thick] (T1.east) -- (T2.west); 
			\draw[-, very thick] (Tn.west) -- (Tn1.east); 
			\draw[-, mydotted, very thick] (T2.east) -- (Tn1.west);
			
			\node[align=center] at (-3.5,0.25) (R) {$r_1$};
			\node[align=center] at (5,0.25) (R) {$r_{d-1}$};	
		\end{tikzpicture} 
		\caption{}
	\end{subfigure}
	\caption[Visualisation of a tensor train]{Here the TT cores are visualised as a train of two- and three-dimensional matrices. 
		Each core has a length $n$, corresponding to the number of grid points in each dimension, and the cores are connected through ranks $r_k$. 
		More specifically, a core $\tilde{\pi}_k$ has dimensions $r_{k-1} \times n \times r_k$, with outer ranks $r_0 = r_d = 1$.
		Using the TT format enables us to represent a $d$-dimensional grid with only $2nr + (d-2)nr^2$ evaluation points instead of $n^d$ grid points.
		Figure~(a) is adapted from~\cite{fox2021grid}.}
	\label{fig:TTfig}
\end{figure}
The approximation of a function in the TT format requires a predefine $d$-dimensional discrete univariate grid over the parameter space $\mathcal{X}$ with $n$ grid points in each direction.
For a fixed grid point $\bm{x} = (x_1, \dots, x_d)$,  $\pi(\bm{x})$ is approximated by
\begin{align*}
	\tilde{\pi}(\bm{x}) = 	\tilde{\pi}_1(x_1)  \tilde{\pi}_2(x_2)  \cdots \tilde{\pi}_d(x_d),
\end{align*}
which is a sequence of matrix multiplications with $\tilde{\pi}_k(x_k) \in \mathbb{R}^{r_{k-1} \times r_k}$.
A TT core  $\tilde{\pi}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ has ranks $ r_{k-1}$ and $r_k$, which connect the core to its respective neighbouring dimensions.
The outer ranks of a TT are $r_0 = r_d = 1$.
Then, a discrete parameter space $\mathcal{X}$ is approximated by $\pi(\mathcal{X})\approx \tilde{\pi}_1  \tilde{\pi}_2  \cdots \tilde{\pi}_d$ with $2nr + (d-2)nr^2$ evaluation points for fixed ranks $r =r_{k-1} = r_k $, as illustrated in Figure~\ref{fig:TTfig}, instead of $n^d$ function evaluations.
In the TT format the integral in Eq.~\ref{eq:margInt} can be calculated at a low computational cost and the marginal PDF
\begin{align}
	\begin{split}
		f_{X_k}(x_k) \approx \frac{1}{z} \Bigg|\, 
		&\left( \int_{\mathcal{X}_1} \uplambda_1(x_1) \tilde{\pi}_1(x_1) \, \diff x_1 \right) \cdots 
		\left( \int_{\mathcal{X}_{k-1}} \uplambda_{k-1}(x_{k-1}) \tilde{\pi}_{k-1}(x_{k-1}) \, \diff x_{k-1} \right) \\
		&\quad \uplambda_k(x_k) \tilde{\pi}_k(x_k) \\
		& \left( \int_{\mathcal{X}_{k+1}} \uplambda_{k+1}(x_{k+1}) \tilde{\pi}_{k+1}(x_{k+1}) \,\diff x_{k+1} \right) \cdots 
		\left( \int_{\mathcal{X}_{d}} \uplambda_d(x_d) \tilde{\pi}_d(x_d) \, \diff x_d \right)
		\Bigg| 
	\end{split}
\end{align}
is computed by integrating over all TT cores except the $k$-th core $\pi_k$, as in~\cite{dolgov2020approximation}, and normalised by the constant $z$~\cite{cui2022deep}.


%%%%%% Squared and Basis Function %%%%%
In practice, TT approximations may suffer from numerical instability.
In particular when the target function is non-negative the TT approximation can have negative values in regions where true function values are very small.
%because it is not advantageous yet to approximate the target function $\pi(\bm{x})$ in e.g. the logarithmic space. 
One way to ensure non-negativity is to square the target function, hence~\cite{cui2022deep} approximate the square root of the target function and 
%\textcolor{red}{Thsi sentence needs rewriting - -it's a jumble of ideas.Hence implies an implication -- is it that or just one possible resolution?}
define the approximation as \cite[Eq.~18]{cui2022deep}
\begin{align}
	\sqrt{\pi(\bm{x})} \approx \tilde{g}(\bm{x}) = \bm{G}_1(x_1), \dots, \bm{G}_k(x_k), \dots, \bm{G}_d(x_d)\, .
\end{align}
%and intuitively ``reduces the range of numbers'' (excuse this unmathematical expression).
Here, each TT core is given by \cite[Eq.~21]{cui2022deep}
\begin{equation}
	G^{(\alpha_{k-1},\alpha_k)}_k(x_k) = \sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{A}_k[\alpha_{k-1}, i, \alpha_k], \quad k = 1, \dots, d,\, \,  ,
\end{equation}
where $\bm{A}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ is the $k$-th coefficient tensor and $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ are the basis functions corresponding to the $k$-th coordinate.
The approximated unnormalised density function is written as \cite[Eq.~19]{cui2022deep}:
\begin{align}
	\pi(\bm{x}) \approx \xi + \tilde{g}(\bm{x})^2\,  ,
\end{align}
to ensure a positivity a small constant $\xi > 0$ is added according to the ratio of the Lebesgue weighted L2-norm error and the Lebesgue weighting (see Eq.~\ref{eq:lebesgueWeight} and \cite[Eq.~35]{cui2022deep}) such that 
\begin{align}
	0 \leq \xi \leq \frac{1}{\uplambda(\mathcal{X})} \lVert \tilde{g} - \sqrt{\pi} \rVert_{L^2_{\uplambda}(\mathcal{X})}^2\,  \label{eq:gamErr}.
\end{align}
This leads to the normalised PDF \cite[Eq.~19]{cui2022deep}
\begin{align}
	f_X(\bm{x})  \approx \frac{1}{z} \left( \uplambda(\bm{x}) \xi  + \uplambda(\bm{x}) \tilde{g}(\bm{x})^2 \right)\, \, ,
\end{align}
with the normalisation constant $z = \int_{\mathcal{X}} f_X(\bm{x}) \diff \bm{x} $.
Given the tensor train approximation of $\sqrt{\pi}$, the marginal PDF $f_{X_k}(x_k)$ can be expressed as
\begin{align}
	\begin{split}
		f_{X_k}(x_k)  \approx \frac{1}{z} \Bigg(&\xi \prod_{i=1}^{k-1} \uplambda_i(\mathcal{X}_i) \prod_{i=k+1}^{d} \uplambda_i(\mathcal{X}_i) \\
		&+ \left( \int_{\mathcal{X}_1} \uplambda_1(x_1) \bm{G}_1^2(x_1)  \, \diff x_1 \right) \cdots 
		\left( \int_{\mathcal{X}_{k-1}} \uplambda_{k-1}(x_{k-1}) \bm{G}_{k-1}^2(x_{k-1}) \, \diff x_{k-1} \right) \\
		& \uplambda_k(x_k) \bm{G}_k^2(x_k)  \\
		&\left( \int_{\mathcal{X}_{k+1}} \uplambda_{k+1}(x_{k+1}) \bm{G}_{k+1}^2(x_{k+1})  \,\diff x_{k+1} \right) \cdots 
		\left( \int_{\mathcal{X}_d} \uplambda_d(x_d) \bm{G}_d^2(x_d)  \, \diff x_d \right) \Bigg).
	\end{split}
\end{align}




\subsection{Marginal Functions}
\label{subsec:TTMarg}
The marginal functions $f_{X_k}(x_k)$ of the PDF $f_{X}(\bm{x})$ are computed by a procedure to which Cui and Dolgov~\cite{cui2022deep} refer to as backward marginalisation, see Prop.~\ref{prob:ForMarg}, and to which we add the forward marginalisation, see Prop.~\ref{prob:backMarg}. 
%\textcolor{red}{By now you have refered to multiple marginal distributions, so you'll need to be more clear.}
This is similar to the left and right orthogonalisation of TT cores~\cite{oseledets2011tensor, Oseledets2011DMRG}.
%\textcolor{red}{why abbreviate?} 
The backward marginalisation provides the coefficient matrices $\bm{B}_k$, while the forward marginalisation gives the coefficient matrices $\bm{R}_{\text{pre}, k}$. 
These matrices enable the efficient evaluation of marginal functions since they are formed by integration over the parameter space either left or right of the $k$-th dimension, as in~\cite{cui2022deep}. 
%\textcolor{red}{are they marginal functions or marginal distributions. Pick a language and stick to it.}
In doing so, the mass matrix $\bm{M}_k \in \mathbb{R}^{n_k \times n_k}$ is defined as in \cite[Eq.~22]{cui2022deep}
\begin{equation}
	\bm{M}_k[i, j] = \int_{\mathcal{X}_k} \phi^{(i)}_k(x_k) \phi^{(j)}_k(x_k) \uplambda(x_k) \, \diff x_k, \quad i, j = 1, \dots, n_k, \, \,   \label{eq:MassMat},
\end{equation}
where $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ denotes the set of basis functions for the $k$-th coordinate.
The proposition used to compute $\bm{B}_k$, stated in Prop.~\ref{prob:backMarg}, is adapted directly from~\cite{cui2022deep}.

After computing the coefficient tensors $\bm{R}_{\text{pre},k-1}$ as in Prop.~\ref{prob:ForMarg} and $\bm{B}_{k}$ from Prop.~\ref{prob:backMarg}, the marginal PDF of $k$-th dimension can be expressed as
\begin{equation}
	f_{X_k}(x_k)  \approx \frac{1}{z} \left(\xi \prod_{i=1}^{k-1} \uplambda_i(X_i) \prod_{i=k+1}^{d} \uplambda_i(X_i) + \sum_{l_{k-1}=1}^{r_{k-1}} \sum_{l_k=1}^{r_k} \left(\sum_{i=1}^{n} \phi^{(i)}_k(x_k) \bm{D}_k[l_{k-1},i, l_k] \right)^2 \right) \uplambda_k(x_k), \label{eq:MargTT}
\end{equation}
where $\bm{D}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ and given as
\begin{equation}
	\bm{D}_k[l_{k-1},i,l_k] = \sum_{\alpha_{k-1}=1}^{r_{k-1}}  \bm{R}_{\text{pre},k-1}[l_{k-1}, \alpha_{k-1}] \bm{B}_k[\alpha_{k-1}, i, l_k] \, ,
\end{equation}
with $\bm{R}_{\text{pre},k-1}\in \mathbb{R}^{r_{k-1} \times r_{k-1}}$ and $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$.

For the first dimension, $f_{X_1}(x_1)$ can be expressed as \cite[Eq.~30]{cui2022deep}
\begin{equation}
	f_{X_1}(x_1)  \approx \frac{1}{z} \left(\xi \prod_{i=2}^{d} \uplambda_i(\mathcal{X}_i) + \sum_{l_1=1}^{r_1} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_1[i, l_1] \right)^2 \right) \uplambda_1(x_1)\, \, ,
	\label{eq:firstMarg}
\end{equation}
where $\bm{D}_1[i, l_1] = \bm{B}_1[\alpha_0, i, l_1]$ and $\alpha_0 = 1$,
and similarly in the last dimension
\begin{equation}
	f_{X_d}(x_d)  \approx \frac{1}{z} \left(\xi \prod_{i=1}^{d-1} \uplambda_i(\mathcal{X}_i) + \sum_{l_{d-1}=1}^{r_{d-1}} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_d[l_{d-1},i] \right)^2 \right) \uplambda_d(x_d),
\end{equation}
where $\bm{D}_d[l_{d-1},i] = \bm{B}_{\text{pre},d}[l_{d-1}, i, \alpha_{d+1}]$ and $\alpha_{d+1} = 1$.
Note that in practice we calculate $z$ numerically within the process of computing the marginal PDFs so that $\sum f_{X_k}(x_k) =1 $ and for Cartesian basis $\bm{M}_k = \text{diag}(\uplambda_k(\mathcal{X}_k))$ with $\uplambda(x) = 1$.
\clearpage
\begin{prop}[Backward Marginalisation as in~\cite{cui2022deep}]
	\label{prob:backMarg}
	Starting with the last coordinate $k = d$, we set $\bm{B}_d = \bm{A}_d$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{k} \in \mathbb{R}^{r_{k-1} \times n_{k} \times r_{k}}$, which is needed for defining the marginal function $f_{X_k}(x_k)$ or to draw samples from $\tilde{\pi}(\bm{x})$ via the squared IRT scheme (see Alg. Box~\ref{alg:SIRT}):
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ \cite[Eq.~27]{cui2022deep}:
		\begin{equation}
			\bm{C}_k[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{B}_k[\alpha_{k-1}, i, l_k] \bm{L}_k[i, \tau]  \label{eq:constrCBack}.
		\end{equation}
		\item Unfold $\bm{C}_k$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_k^{(R)} \in \mathbb{R}^{r_{k-1} \times (n_k r_k)}$  \cite[Eq.~28]{cui2022deep}:
		\begin{equation}
			\bm{Q}_k \bm{R}_k = {(\bm{C}_k^{(R)})}^{\top} \, \, .\label{eq:thinQRBack}
		\end{equation}
		\item Compute the new coefficient tensor \cite[Eq.~29]{cui2022deep}:
		\begin{equation}
			\bm{B}_{k-1}[\alpha_{k-2}, i, l_{k-1}] = \sum_{\alpha_{k-1}=1}^{r_{k-1}} \bm{A}_{k-1}[\alpha_{k-2}, i, \alpha_{k-1}] \bm{R}_k[l_{k-1}, \alpha_{k-1}]\, \,   \label{eq:nextCoeffTBack}.
		\end{equation}
	\end{enumerate}
\end{prop}
\begin{prop}[Forward Marginalisation]
	\label{prob:ForMarg}
	Starting with the first coordinate $k = 1$, we set $\bm{B}_{\text{pre},1} = \bm{A}_1$. The following procedure can be used to obtain $\bm{R}_{\text{pre},k-1} \in \mathbb{R}^{r_{k-1}  \times r_{k-1}}$ for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_{\text{pre},k}[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{L}_k[i, \tau] \bm{B}_{\text{pre},k}[\alpha_{k-1}, i, l_k] .\label{eq:constrCForw}
		\end{equation}
		\item Unfold $\bm{C}_{\text{pre},k}$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_{\text{pre},k}^{(R)} \in \mathbb{R}^{(r_{k-1} n_k ) \times r_k}$:
		\begin{equation}
			\bm{Q}_{pre,k}\bm{R}_{\text{pre},k} = {(\bm{C}_{\text{pre},k}^{(R)})}.\label{eq:thinQRForw}
		\end{equation}
		\item Compute the new coefficient tensor $\bm{B}_{\text{pre}, k+1} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k} $:
		\begin{equation}
			\bm{B}_{\text{pre}, k+1}[l_{k+1}, i, \alpha_{k+1}] = \sum_{\alpha_{k}=1}^{r_{k}} \bm{R}_{\text{pre},k}[l_{k+1}, \alpha_{k}] \bm{A}_{k+1}[\alpha_{k}, i, \alpha_{k+1}] .\label{eq:nextCoeffTForw}
		\end{equation}
	\end{enumerate}
\end{prop}

\clearpage
\subsection{Sampling from a TT Approximation}
\label{subsec:SamplTT}
Instead of evaluating marginal functions for quadrature, the inverse Rosenblatt transform (IRT) provides a scheme to draw samples from an approximated function in the TT format~\cite{dolgov2020approximation}.
The idea is that a target function can be represented as the sequence $ f_X(\bm{x})  =   f_{X_1}(x_1)  f_{X_2|X_1}(x_2|x_1) \cdots  f_{X_k|X_{<k}}(x_k|x_{k-1},\dots,x_1)$.
Within the IRT scheme samples are iteratively drawn from each $f_{X_k|X_{<k}}(x_k|x_{k-1},\dots,x_1)$ conditioned on the previous left $k-1$ samples and marginalised over the right $k + 1$ dimensions, for $k = 2 , \dots, d-1$.
Since the square root of the target function is approximated, Cui and Dolgov~\cite{cui2022deep} call that the squared inverse Rosenblatt transform (SIRT).

\begin{algorithm}[!th]
	\caption{Squared Inverse Rosenblatt Transform (SIRT)}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} seeds $\{ \bm{u}^{(1)},\dots, \bm{u}^{(N)} \} \sim \mathcal{U}(0,1)^d $ and $\bm{B}_1 , \dots,\bm{B}_d$  from Prop.~\ref{prob:backMarg}
		\FOR{ \( s = 1, \dots, N\)}
		\FOR{ \( k = 1, \dots, d\)}
		\STATE compute normalised PDF $ f_{X_k|X_{<k}}(x_k|x^{(s)}_{k-1},\dots,x^{(s)}_1)$, Eq.~\ref{eq:CurrMarg}
		\STATE compute cumulative distribution function $F_{X_k|X_{<k}}(x_k)$, Eq.~\ref{eq:CurrCDF},
		\STATE project sample $x^{(s)}_k = F_{X_k|X_{<k}}^{-1}(u^{(s)}_k)$
		\STATE interpolate $\bm{G}_k(x^{(s)}_k)$, Eq. \ref{eq:LinPol}
		\STATE update $\bm{G}_{\leq k}(x^{(s)}_{\leq k}) = \bm{G}_{<k}(x^{(s)}_{<k}) \bm{G}_k(x^{(s)}_k)$
		\ENDFOR
		\ENDFOR
		\STATE \textbf{Output:} samples $\{ \bm{x}^{(1)},\dots, \bm{x}^{(N)} \} $, where each $\bm{x}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N$
	\end{algorithmic}
	\label{alg:SIRT}
\end{algorithm}
Given the backward marginal coefficient tensors $\bm{B}_1 , \dots,\bm{B}_d$ as in Prop.~\ref{prob:backMarg} and $N$ uniformly distributed seeds $\{ \bm{u}^{(1)},\dots, \bm{u}^{(N)} \} \sim \mathcal{U}(0,1)^d $, where each $\bm{u}^{(s)}$ is $d$-dimensional for $s = 1, \dots, N$, the first marginal $f_{X_1}(x_1)$ is calculated as in Eq.~\ref{eq:firstMarg} and normalised with $z = \int_{\mathcal{X}_1} f_{X_1}(x_1) d x_1$. 
%\textcolor{red}{OK, another task -- remove all but one 'we' per page, and even then you will have 100, which is too many. FYI, I counted 397 occurrences of 'we' in this text -- so you only have to rewrite 297 sentences! Holy Toledo, here's one to zap. and another. Is this a novel about Lennart's adventures in MCMC for 10 year olds?}
Next, the cumulative distribution function (CDF) $F_{X_1}(x_k) = \int^{x_k}_{-\infty} f_{X_1}(\hat{x}_1) d \hat{x}_1$ is formed, which for the general case is given as \cite[Eq. 17]{cui2022deep}:
\begin{align}
	F_{X_k|X_{<k}}(x_k) = \int_{-\infty}^{x_k} f_{X_k|X_{<k}}(\hat{x}_k|x_{k-1},\dots,x_1) \diff \hat{x}_k  \, \,  \, ;
	\label{eq:CurrCDF}
\end{align}
Then the seed $u^{(s)}_k$ is projected onto the parameter space to generate the sample $x^{(s)}_k = F_{X_k|X_{<k}}^{-1}(u^{(s)}_k)$. %\textcolor{red}{Argghhhh!}
For $k = 2, \dots,d$ the general ``conditional marginal'' is given as \cite[Eq.~31]{cui2022deep}:
\begin{align}\begin{split} 
		f_{X_k|X_{<k}}(x_k|x^{(s)}_{k-1},\dots,x^{(s)}_1) \approx \frac{1}{z}
		\Bigg( 
		\xi \prod_{i=k+1}^{d} \uplambda_i(X_i) +&  \\
		\sum_{l_{k} = 1}^{r_{k}} \Bigg( \sum_{i = 1}^{n}  \phi^{(i)}_k(x^{(s)}_k) \bigg( \sum_{\alpha_{k-1} = 1}^{r_{k-1}} \bm{G}^{(\alpha_{k-1})}_{<k}(x^{(s)}_{<k}) &\bm{B}_k[\alpha_{k-1},i,l_k] \bigg) \Bigg)^2 \Bigg) \uplambda_k(x_k) \, \,  ,
	\end{split} 
	\label{eq:CurrMarg} 
\end{align}
where we marginalise over the dimensions $k+1 , \dots, d$ via $\bm{B}_k$ and condition on the previous $k-1$ samples through the product $\bm{G}_k(x^{(s)}_k)\in \mathbb{R}^{1 \times r_{k-1}}$. 
%\textcolor{red}{God dammit, another pesky we.}
Function values between grid points $i$ and $i+1$ are approximated with a piecewise polynomial interpolation
\begin{align}
	\bm{G}_k(x^{(s)}_k) \approx   \frac{x^{(s)}_k - x^{(i)}_k }{x^{(i+1)}_k -x^{(i)}_k } \bm{G}_k(x^{(i+1)}_k) + \frac{ x^{(i+1)}_k - x^{(s)}_k}{x^{(i+1)}_k -x^{(i)}_k } \bm{G}_k(x^{(i)}_k) \, ,
	\label{eq:LinPol}
\end{align}
for $x^{(i)}_k \leq x^{(s)}_k \leq x^{(i+1)}_k$ as in~\cite{dolgov2020approximation} for the next ``conditional marginal''.

The procedure is repeated for each $u^{(s)}_k \in \bm{u}^{(s)}$ to produce the samples $\bm{x}^{(s)} \sim f_{X}(\bm{x})$, as summarised in Alg.~Box~\ref{alg:SIRT}.%\textcolor{red}{The procedure is repeated ... (the 'we' is entirely redundant, and distracting.)}
%\textcolor{red}{Function or distribution}

%Note that with Cartesian basis $ \sum \phi^{(i)}_k(x_k) \Bigg( \sum \bm{G}^{(\alpha_{k-1})}_{<k}(x_{<k}) \bm{B}_k[\alpha_{k-1},i,l_k] \Bigg) \Bigg)^2 
%$  and $\bm{G}_{<k}(x^{(s)}_{<k}) = \bm{G}_{1}(x^{(s)}_{1}) \cdots \bm{G}_{k-1}(x^{(s)}_{k-1}) $ are simple matrix multiplications for each grid point $i$ or sample $\bm{x}^{(s)}$.


\subsubsection{Metropolis--Hastings -- correction step}
Since the samples by the SIRT scheme are generated from an approximation, it is sensible to correct those using a Metropolis--Hastings (MH) importance step as in~\cite{dolgov2020approximation}.
\begin{algorithm}[!ht]
	\caption{MH correction step}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} samples $\{ \bm{x}^{(1)},\dots, \bm{x}^{(N+1)} \} $, where each $\bm{x}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N+1$
		\FOR{ \( s = 1, \dots, N\)}
		\STATE compute MH ratio $\frac{w^{(s+1)}}{w^{(s)} } =\frac{\pi(\bm{x}^{(s+1)})}{\pi(\bm{x}^{(s)})} \frac{f_X(\bm{x}^{(s)})}{f_X(\bm{x}^{(s+1)})}$ 
		\STATE compute acceptance probability $\alpha = \text{min}(w^{(s+1)}/w^{(s)}, 1)$ 
		\STATE Draw $u \sim \mathcal{U}(0,1)$
		\IF{$\alpha \geq u$ }
		\STATE Accept and set $\bm{x}_{\text{MH}}^{(s+1)} = \bm{x}^{(s+1)}$
		\ELSE  
		\STATE Reject and keep $\bm{x}_{\text{MH}}^{(s+1)} = \bm{x}^{(s)}$
		\ENDIF
		\ENDFOR
		\STATE \textbf{Output:} corrected sample chain $\{ \bm{x}_{\text{MH}}^{(1)},\dots, \bm{x}_{\text{MH}}^{(N)} \} $, where each $\bm{x}_{\text{MH}}^{(s)} \in \mathbb{R}^d$ for $s = 1, \dots, N$
	\end{algorithmic}
	\label{alg:MHCorr}
\end{algorithm}
In doing so, we compute the acceptance probability $  \alpha = \text{min}(w^{(s+1)}/w^{(s)}, 1)$, where 
\begin{align}
	w(x) = \frac{\pi(\bm{x})}{f_X(\bm{x})} = \frac{\pi(\bm{x})}{\xi + \tilde{g}(\bm{x})^2} 
\end{align}
is the importance ratio.
Note the normalising constants in the ratio $w^{(s+1)}/w^{(s)}$ cancel.
In practice, the importance ratio is calculated in the log-space so that $\log f_X(\bm{x})  =  \log f_{X_1}(x_1) + \log f_{X_2|X_1}(x_2|x_1) + \cdots + \log f_{X_k|X_{<k}}(x_k|x_{k-1},\dots,x_1)$ (see Eq.~\ref{eq:CurrMarg}).
We refer to this as the SIRT-MH scheme, which provides the corrected chain $\{\bm{x}_{\text{MH}}^{(1)},\dots,\bm{x}_{\text{MH}}^{(N)}\}\sim\pi(\bm{x})$.


\subsection{Error of the TT Approximation}
A straightforward way to assess an average error of a TT approximation is to calculate the relative root mean squared (RMS) error 
%\textcolor{red}{explain why this is a sensible thing to do.}
\begin{align}
	\Bigg( \frac{ \int_{\mathcal{X}} (\pi(\bm{x}) - (\xi + \tilde{g}(\bm{x})^2))^2 \uplambda(\bm{x}) \diff \bm{x}}{ \int_{\mathcal{X}} \pi(\bm{x})^2 \uplambda(x)  \diff \bm{x} } \Bigg)^{1/2} =	\frac{\lVert 	\pi(\bm{x}) - (\xi + \tilde{g}(\bm{x})^2)  \rVert_{L^2_{\uplambda}(\mathcal{X})}}{\lVert 	\pi(\bm{x}) \rVert_{L^2_{\uplambda}(\mathcal{X})}  } \, .
\end{align}
The RMS is approximated by
\begin{align}
	\Bigg( \frac{1}{N} \sum^{N}_{i =1} \Big(\pi(\bm{x}^{(i)}) - \big(\xi + \tilde{g}(\bm{x}^{(i)})^2\big)\Big)^2 \uplambda(\bm{x}^{(i)})\Bigg)^{1/2}    \approx \Bigg(  \int_{\mathcal{X}} (\pi(\bm{x}) - \big(\xi + \tilde{g}(\bm{x})^2\big)\Big)^2 \uplambda(\bm{x}) \diff \bm{x} \Bigg)^{1/2} \label{eq:RMSTT}
\end{align}
and similarly $\int_{\mathcal{X}} \pi(\bm{x})^2 \uplambda(\bm{x})  \diff \bm{x}$.


\subsubsection{Absolute error bound}
\label{subsec:wasser}
If large errors occur in regions with low probability, the RMS is sensitive to those, whereas the Wasserstein distance weighs differences according to their respective probability values.

The Wasserstein distance is the infimum over all couplings between two probability distributions with respect to some distance measure.
%\textcolor{red}{why is this sensible, or is it just 'another way'?}
The Kantorovich-Rubinstein duality, as in~\cite{thickstun2019kantorovich, Ambrosio2024Kanta}, says that the 1-Wasserstein distance is equal to the supremum of differences in expectations over all 1-Lipschitz functions $h$ between two probability distributions.
So the 1-Wasserstein distance provides an upper absolute error bound and is defined as
\begin{align}
	W_1(\pi,\tilde{\pi}) = \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}}\int_{\mathcal{X} \times \mathcal{X}} c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}}) \, \nu(\bm{x},\tilde{\bm{x}}) \diff \bm{x} \diff \tilde{\bm{x}}
	\label{eq:wass} \, ,
\end{align}
where $\nu$ couples $\bm{x}$ and $\tilde{\bm{x}}$ so that the integral over the distance $c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}}) $ weighted by the probability measures $\pi$ and $\tilde{\pi}$ is the greatest lower bound of all integrals with respect to $\nu$ in the set of all couplings $ \Pi(\pi,\tilde{\pi})$.
Often $\nu$ is the transport plan, where $c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}})$ is the (ground) cost function, and $\nu(\bm{x}, \tilde{\bm{x}})$ is related to the mass which has to be transported and the 1-Wasserstein distance is the earth mover distance.
On the other hand (Kantorovich-Rubinstein duality), the 1-Wasserstein distance
\begin{align}
	W_1(\pi,\tilde{\pi})  =& \underset{ h(\bm{x}); \, c_{\mathcal{Y}}(h(\bm{x}), h(\tilde{\bm{x}}) ) \leq  c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}}) }{ \text{sup}} \Bigg\{  \int_{\mathcal{X}} h(\bm{x}) \diff \pi (\bm{x})  - \int_{\mathcal{X}} h(\tilde{\bm{x}}) \diff \tilde{\pi} (\tilde{\bm{x}}) \Bigg\} \\
	=& \underset{ h(\bm{x});  \, c_{\mathcal{Y}}(h(\bm{x}), h(\tilde{\bm{x}}) )  \leq  c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}}) }{ \text{sup}}  \Bigg\{  \underset{\bm{x} \sim  \pi }{\mathbb{E}} \big[ h(\bm{x}) \big]  -  \underset{\tilde{\bm{x}}\sim \tilde{\pi}}{\mathbb{E}} \big[ h(\tilde{\bm{x}}) \big] \Bigg\} .
\end{align}
is the lowest upper bound of differences in expectations over all 1-Lipschitz function $h(\bm{x}) : \mathcal{X} \rightarrow \mathcal{Y}$ in between the two distributions $\pi$ and $\tilde{\pi}$, with the distance measure $c_{\mathcal{X}}$ on the set $\mathcal{X}$ forming the metric space $( \mathcal{X}, c_{\mathcal{X}} )$ and similarly the metric space $(\mathcal{Y}, c_{\mathcal{Y}})$.

For two sample sets $\{ \bm{x}^{(1)},\dots,\bm{x}^{(N)}\} \sim \pi$ and $\{\tilde{ \bm{x}}^{(1)},\dots,\tilde{\bm{x}}^{(M)}\} \sim \tilde{\pi}$ the calculation of the Wasserstein distance becomes an optimisation problem that is to find the best coupling of samples weighted by their distribution value according to an appropriate distance measure~\cite{feydy2020OT}, which we set to $c_{\mathcal{X}}(\bm{x},\tilde{\bm{x}})= \lVert \bm{x} -\tilde{\bm{x}} \rVert_{L^2} $.
More specifically,
\begin{align}
	W_1(\pi,\tilde{\pi}) = 	\underset{\nu \in \Pi(\pi,\tilde{\pi}) }{\text{min}} \sum^M_{j = 1} \sum^N_{i =1}  \nu_{ij} \lVert\bm{x}^{(i)}  -  \tilde{\bm{x}}^{(j)} \rVert_{L^2} \, , \label{eq:applWasser}
\end{align}
where the transport plan $\nu \in \mathbb{R}^{N \times M}_ {\geq 0}$ defines the coupling $\nu_{ij} \in \nu $ as $ \nu_{ij} \coloneqq \pi(\bm{x}^{(i)}) \tilde{\pi}(\tilde{\bm{x}}^{(j)})$ similar to~\cite[Eq. 3.166]{feydy2020OT}.
Additionally it is required that $\sum^N_{i =1} \pi(\bm{x}^{(i)}) = \sum^M_{j = 1} \tilde{\pi}(\tilde{\bm{x}}^{(j)})= 1 $.
This gives us an upper bound of the absolute error between the expected value of any 1-Lipschitz function $h$.
%, e.g the upper bound of absolute differences in means related to the probability measures $\pi$ and $\tilde{\pi}$. 


\clearpage
\section{Regularisation Approach}
\label{sec:reg}

The currently most used method to analyse data in atmospheric physics is regularisation-based.
Since we want to show that Bayesian methods provide more information than regularisation at a similar computational cost, the chosen regularisation approach is the closest equivalent to the linear-Gaussian Bayesian framework~\cite{fox2016fast} in Sec. \ref{sec:BayModelO3}.
%\textcolor{red}{Don't say that you picked an example to give a particular result. The implication is that you could pick another example to get a different result. Actually, this model is chosen to be the closest equivalent in a Bayesian model.}
% Tikhonov \textcolor{red}{you seem you have misunderstood naming, Tikhonov is only T=I, in your notation.}

For a linear forward model matrix $\bm{A}$, data $\bm{y}$ and a regularisation operator $\bm{T}$, the regularisation approach provides one solution $\bm{x}$ that minimises both the data misfit norm
\begin{align}
	\left\lVert  \bm{A} \bm{x}- \bm{y} \right\rVert_{L^2}
\end{align} and a regularisation norm 
%\textcolor{red}{why is lambda in the semi-norm. If it is there, this is a family of semi-norms.}
\begin{align}
	\left\lVert \bm{T} \bm{x} \right\rVert_{L^2} \, . \label{semiNorm}
\end{align}
For a fixed regularisation parameter $\lambda> 0 $, the regularised solution as in~\cite{hansen2010discrete, fox2016fast, tan2016LecNot} is given by $\bm{x}_{\lambda}$ that minimises the weighted sum
\begin{align}
	\bm{x}_{\lambda} = \underset{\bm{x}}{\mathrm{arg\,min}} \left\lVert  \bm{A} \bm{x} -\bm{y} \right\rVert_{L^2}^2 + \lambda \left\lVert \bm{T} \bm{x} \right\rVert_{L^2}^2\, ,
\end{align}
which can be calculated by taking the derivative with respect to $\bm{x}$:
\begin{align}
	& & \nabla_{\bm{x}} \left\{ ( \bm{A} \bm{x}-\bm{y})^T ( \bm{A} \bm{x}-\bm{y}) + \lambda \bm{x}^T \bm{T}^T \bm{T} \bm{x} \right\} &= 0 \\
	&\iff & \nabla_{\bm{x}} \left\{ \bm{y}^T \bm{y} + \bm{x}^T \bm{A}^T \bm{A} \bm{x} - 2 \bm{y}^T \bm{A} \bm{x} + \lambda \bm{x}^T \bm{T}^T \bm{T} \bm{x} \right\} &= 0 \\
	&\iff & 2 \bm{A}^T \bm{A} \bm{x} - 2 \bm{A}^T \bm{y} + 2 \lambda \bm{T}^T \bm{T} \bm{x} &= 0 \label{eq:lastRegDiff}.
\end{align}
Eq.~\ref{eq:lastRegDiff} yields the regularised solution
\begin{align}
	\bm{x}_{\lambda} = (\bm{A}^T \bm{A} + \lambda \bm{L})^{-1} \bm{A}^T \bm{y} \, , \label{eq:regSol}
\end{align}
where we define $\bm{L} \coloneqq \bm{T}^T \bm{T}$.
Typically, $\bm{L} $ represents a discrete matrix approximation of a differential operator~\cite{tan2016LecNot}.
%\textcolor{red}{more sentence creep. Yoiu need to define L separately, what is a "differential operator choice"? This reads very strangely.}
For example
\begin{align}
	\bm{T} = \frac{1}{h}
	\begin{bmatrix}
		-1 & 1 & & &  \\
		0 & -1 & 1 & &   \\
		& \ddots & \ddots & \ddots &\\ 
		& & 0 & -1 & 1  \\
		& & & 0 & -1 
	\end{bmatrix} \, ,
\end{align}
is the first order forward difference operator with equal spacing $h$ as in~\cite{tan2016LecNot} that approximates the first derivative.
%\textcolor{red}{no, it's a first order forward differnece operator, that approximates a derivative operator.}
Then 
\begin{align}
	\bm{T}^T\bm{T} = \frac{1}{h^2}
	\begin{bmatrix}
		1 & -1 & & &  \\
		-1 & 2& -1 & &   \\
		& \ddots & \ddots & \ddots &\\ 
		& & -1 & 2 & -1  \\
		& & & -1 & 1 
	\end{bmatrix} \, ,
\label{eq:LaplRegTheo}
\end{align}
is a discrete approximation to the second derivative with Neumann boundary conditions~\cite{wang2015graphs}.
%\textcolor{red}{no it's not, it's a second-difference operator, that is a discrete approximation to the second derivative (no 'order')}

If $\lambda$ is large, then the effect of the data on the solution $\bm{x}_{\lambda}$ is small and dominated by the regulariser, resulting in an under-fitted $\bm{x}_{\lambda}$.
For example, if the regulariser imposes smoothness, the solutions will be overly smooth and not sensitive to structures from the data.
If $\lambda$ is small, the solution $\bm{x}_{\lambda}$ will be dominated by the data misfit norm.
Then $\bm{x}_{\lambda}$ is sensitive to noise, resulting in an over-fitted solution inheriting the structure of the noise.
We refer to~\cite{hansen1989GSVD} and~\cite{tan2016LecNot} for a more comprehensive analysis on the effects of the regularisation parameter on the solution e.g.,~due to small singular values of the forward model.

%\textcolor{red}{Next part is your sentence creep. Make two clear sentences, not one lengthy, going on too long, garbled while saying something else and the queen likes to read it on Sundays, when there is no rain or a poodle has too many walks, and also the evening news to be interesting.}
%\textcolor{red}{what does that mean -- the solution is essentially equal to the noisy data, or what? If you are going to say anything, you need to be specific. Currently this says essentially nothing. A person who knows what is going on does not need to read this sentence (or section) while a person who does not know will still have no idea after reading this. }

In practice, $\bm{x}_{\lambda}$ is computed for a range of $\lambda$-values and the data misfit norm versus the regularisation norm is plotted in log-space to form an L-curve (see Fig.~\ref{fig:LCurve}).
Based on the trade-off between the data misfit and the regularisation norm, the regularisation solution corresponds to the point of maximum curvature of the L-curve~\cite{hansen1993use}.
%\textcolor{red}{you use of the tem 'optimal' is offensive -- because it is a tautology that you define 'optimal' to mean that it is given by the L-curve. So, say it is optimal because it is given by th eL-curve is saying nothing, other than defining what you mean by an 'optimal regularisation parameter'. If that's what you want to say, then say it. otherwise this sentence is content free. }

\begin{figure}[ht!]
	\centering
	\input{Lagrangian.pdf_tex}
	\caption[]{This Figure is not to scale and directly inspired by~\cite{ColinPhD}. One solution $\bm{x}_{\lambda}$ is obtained by following a contour line $\left\lVert \bm{A} \bm{x} -\bm{y} \right\rVert^2_{L^2}= c$ until $\bm{x}^T \bm{L} \bm{x}$ is minimised. In the centre of the likelihood contours a solution $\bm{x}_{0}$ uneffected by the regulariser is obtained, whereas the solution $\bm{x}_{\infty}$ is determined a-priori.}
	\label{fig:Langrangian}
\end{figure}
Alternatively one can introduce a Lagrangian $\mathcal{L}(\bm{x},\lambda)\coloneqq \left\lVert \bm{A} \bm{x} -\bm{y} \right\rVert^2_{L^2} + \lambda \, \bm{x}^T\bm{L} \bm{x}$ similar to~\cite{LiLagrange}, where $\lambda$ is a Lagrange multiplier.
For a given $\lambda$, a solution $\bm{x}_{\lambda} $ that minimises $\mathcal{L}(\bm{x},\lambda)$ is usually obtained by finding the minimum of $\left\lVert \bm{A} \bm{x} - \bm{y} \right\rVert^2_{L^2}$ with respect to the constant constrained $\bm{x}^T \bm{L} \bm{x} = c$~\cite[Fig.~2.13]{SANTOSH202265}.
This is equivalent to $\bm{x}_{\lambda}  = \underset{\bm{x}}{\text{arg min }}\bm{x}^T \bm{L} \bm{x}$ subject to a constant constrain $\left\lVert  \bm{A} \bm{x} -\bm{y} \right\rVert^2_{L^2}= c$ (see~\cite[fn.~6]{fox2016fast}). 
%\textcolor{red}{You need to fix this sentence, as L(x,lambda) is not a Lagrange multiplier -- it does have a name, and you should use it and not wite sentences that say : I introduce the chicken p=7 which minimizes frog. }
So every solution $\bm{x}_{\lambda}$ is extremely regularised for a given data misfit and the L-curve presents a lower boundary to an open set of high-dimensional posterior samples~\cite{VowelsMultiD}.
Hence, every sample of the high-dimensional posterior, which represents a feasible solution given the data, lies above the L-Curve and is less regularised because it has a higher $ \bm{x}^T \bm{L} \bm{x}$ value.
%\textcolor{red}{not accurate enough}
%\textcolor{red}{say on or above - -do any points in the posterior actually lie in the L-Cerve, I don't think so. Are you claiming that the regularised solutions are in the posterior? The L-curve could be the limit of an open set.}
%Now consider the 2-Wasserstein distance
%\begin{align}
%	W_2(\pi,\tilde{\pi}) = \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}}  \Bigg(\int_{\mathcal{X} \times \mathcal{X}} c(\bm{x},\tilde{\bm{x}})^2 \, \nu(\bm{x},\tilde{\bm{x}}) d\bm{x} d\tilde{\bm{x}} \Bigg)^{1/2}
%	\label{eq:wass} \, ,
%\end{align}
%which for two multivariate normal distributions $\pi \sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$ and $\tilde{\pi} \sim \mathcal{N}(\tilde{\bm{\mu}},  \tilde{\bm{\Sigma}})$ is defined as 
%\begin{align}
%	W_2(\pi,\tilde{\pi}) = \Bigg(  \lVert \bm{\mu} -\tilde{\bm{\mu}} \rVert^2_{L^2} + \text{trace}(\bm{\Sigma}+  \tilde{ \bm{\Sigma}} - 2 (\tilde{ \bm{\Sigma}}^{1/2} \bm{\Sigma} \tilde{ \bm{\Sigma}}^{1/2} )^{1/2} )  \Bigg)^{1/2} \, \text{\cite{Asuka2011Wasser},}
%\end{align}
%with distance measure set to $c(\bm{x},\tilde{\bm{x}})= \lVert \bm{x} - \tilde{\bm{x}}\rVert_{L^2} $.
%To relate that to the 1-Wassertein distance, we write the p-Wassertein distance as the infimum of expectations
%\begin{align}
%	W_p(\pi,\tilde{\pi}) =  \underset{  \nu \in \Pi(\pi,\tilde{\pi}) }{ \text{inf}} \Bigg( \underset{ \bm{x},\tilde{\bm{x}} \sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^p \Bigg)^{1/p}
%\end{align}
%and one can show that
%\begin{align}
% \Bigg(   \underset{ \bm{x},\tilde{\bm{x}} \sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^p \Bigg)^{1/p} \leq  \Bigg(  \underset{ \bm{x},\tilde{\bm{x}}\sim  \nu  }{\mathbb{E}}  c(\bm{x},\tilde{\bm{x}})^q \Bigg)^{1/q}
%\end{align}
%for every $p \leq q$, \cite{Chizat2020LecNot}.
%Then we can bound the absolute difference in expectation of a 1-Lipschitz function $h$ by the 2-Wasserstein distance, so that $W_1(\pi,\tilde{\pi}) \leq W_2(\pi,\tilde{\pi})$.



%Consequently, to map between the linear and non-linear forward map, we generate two affine subspaces $\bm{V}$ and $\bm{W}$ over the same field.
%Assume we draw samples $\{\bm{x}^{(1)}, \dots, \bm{x}^{(j)}, \dots ,\bm{x}^{(m)}\} \sim \pi(\bm{x}|\bm{y})$ and the affine subspace associated with the linear forward model is \begin{align}
%	\bm{W} = \begin{bmatrix}
%		\vert&   &  \vert & & \vert \\
%		\bm{A}_{L} \bm{x}^{(1)} &  \cdots& \bm{A}_{L} \bm{x}^{(j)} &  \cdots & \bm{A}_{L} \bm{x}^{(m)} \\
%		\vert&   &  \vert & & \vert 
%	\end{bmatrix}
%	%	= \begin{bmatrix}
%  %\begin{array}{ccc}
%	%\horzbar & w_{1} & \horzbar \\
%	%	& \vdots    &          \\
%	%\horzbar & w_{j} & \horzbar \\
%	%& \vdots    &          \\
%	%\horzbar &w_{m} & \horzbar
%%end{array}
%%	\end{bmatrix} 
%\in \mathbb{R}^{m \times m}
%\end{align} and with the non-linear forward model is 
%\begin{align}
%	\bm{V} = \begin{bmatrix}
%		\vert&   &  \vert & & \vert \\
%		\bm{A}_{NL}\bm{x}^{(1)} &  \cdots& \bm{A}_{NL}\bm{x}^{(j)} &  \cdots & \bm{A}_{NL} \bm{x}^{(m)}  \\
%		\vert&   &  \vert & & \vert 
%	\end{bmatrix} = 
%		\begin{bmatrix}
%	\begin{array}{ccc}
%		\horzbar & v_{1} & \horzbar \\
%		& \vdots    &          \\
%		\horzbar & v_{j} & \horzbar \\
%		& \vdots    &          \\
%		\horzbar &v_{m} & \horzbar
%	\end{array}
%\end{bmatrix}\in \mathbb{R}^{m \times m} \, .
%\end{align}
%Then the we calculate affine map 
%\begin{align}
%	\bm{V}\bm{W}^{-1} = \bm{M} =
%		\begin{bmatrix}
%	\begin{array}{ccc}
%		\horzbar & r_{1} & \horzbar \\
%		& \vdots    &          \\
%		\horzbar & r_{j} & \horzbar \\
%		& \vdots    &          \\
%		\horzbar &r_{m} & \horzbar
%	\end{array}
%\end{bmatrix}\, \in \mathbb{R}^{m \times m} ,
%\end{align}
%using that, for the forward model in this thesis, each row of $\bm{V}$ is independent of each other, we solve $v_j =r_j \bm{W} $ for each row $ r_j  \in \bm{M}$, where $j = 1, \dots, m$.





%We refer show that the filter factors as in \cite{tan2016LecNot} for a regularisation parameter as shown here affects the solution the generalised singular value decomposition is given in  of $A, T$ as in \cite{hansen1989GSVD}.
%Where on can show the the so-called filter factors are dominated by the default regularisaion solution 
%then the filter factors 
%
%
%
%
%We refer to \cite{hansen1989GSVD} and \cite{tan2016LecNot} for the curious reader, who wants to know how the regularisation parameter affects the reconstruction especially for regions where the forward model does not provide much information (small singular values).
%
%
%
%To show how the regularisation parameter effects the solution one can do a singular value decomposition of $A$
%and the generalised singular value decomposition of $A, T$ as in \cite{hansen1989GSVD}.
%
%Then $\bm{A} = \bm{U} \bm{\Lambda}_A \bm{M}^{-1}$ and $\bm{L} = \bm{V} [\bm{\Lambda}_L,0] \bm{M}^{-1} $
%Where the general singular values $\bm{\Lambda}_A = \text{diag}(\sigma_{A,1},\dots,\sigma_{A,1},1,\dots,1)$ and $\bm{\Lambda}_L = \text{diag}(\sigma_{L,1},\dots,\sigma_{L,1},1,\dots,1)$
%
%
%
%Then one can show that the solution is %$\x_{\lambda} = \bm{M} \bm{F} \bm{U}^T \bm{y} $
%with filter factors
%\begin{align}
%	f_i = \frac{(\sigma_{A,i}/\sigma_{L,i})^2 }{(\sigma_{A,i}/\sigma_{L,i})^2 + \lambda^2} \approx
%	\begin{cases}
%		\frac{(\sigma_{A,i}/\sigma_{L,i})^2}{ \lambda^2},  & \sigma_{A,i}/\sigma_{L,i} \gg \lambda\\
%				1 ,  & \sigma_{A,i}/\sigma_{L,i} \ll \lambda 
%	\end{cases}
%\, \, \text{for}\, i= 1,\dots,p
%\end{align}
%Then small singular values depend on prior information only
%for large $(\sigma_{A,i}/\sigma_{L,i})^2$ singular values the solution is unaffected
%for small $(\sigma_{A,i}/\sigma_{L,i})^2$ singular values the solution is affected by the regularisation parameter




