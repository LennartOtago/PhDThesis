\chapter{Theoretical and Technical Background}
\label{ch:background}
In this chapter, we provide a brief introduction to the methods used in this thesis. We keep the discussion as general as possible, as more specific details will be presented in the results section~\ref{sec:results}.
We begin by introducing the forward model in section~\ref{sec:formodel}, which we use to simulate the data. Since the forward model is weakly non-linear, we employ an affine transformation, see section~\ref{sec:affine}, to project the linear model onto the non-linear one, allowing us to treat the problem as a linear inverse problem.
This enables the application of Bayesian inference in section~\ref{sec:bayes}, where we formulate a hierarchical linear-Gaussian model to define and structure the posterior distribution.
For comparison, we briefly present the Tikhonov regularization approach, see section~\ref{sec:regularise}.
In section~\ref{sec:sampling}, we introduce Markov Chain Monte Carlo (MCMC) methods to sample from the posterior distribution.
Finally, in section~\ref{sec:tensortrain}, instead of sampling, we can approximate the posterior distribution using the tensor train format.


\section{Forward Model}
\label{sec:formodel}
The forward model is based on a satellite measuring through the amtosphere, known as limb sounding, as shown in Figure~\ref{fig:FirstLIMB}.
One measurement $y_j$ of a stationary satellite can be describes as the path integral through the atmosphere along the line of sight, for $j=1,2,\ldots,m$.
For each measurement we can define a tangent height $h_{\ell_j}$ as the shortest distance along the line of sight to the earth.

\begin{figure}[ht!]
	\centering
	\scalebox{0.9}{\input{FirstLIMB.pdf_tex}}
	\caption[General schematics of measurement setup]{This figure illustrates a limb sounding measurement setup, specifically how the line of sight of a satellite at altitude $h_{\text{obs}}$ is partitioned according to a discretized atmospheric model. The atmosphere is divided into $n$ layers, allowing the line of sight $\Gamma_j$ to be discretized into segments $\Delta r_i$ for $i = \ell_j, \dots, n$.
	Here, $\ell_j \in \mathbb{N}$ denotes the index corresponding to the tangent height $h_{\ell_j}$ relative to the Earth's radius $R_E$. This setup forms the basis for the numerical solution of the integral in Eq.~\ref{eq:RTE}, known as the radiative transfer equation.}
	\label{fig:FirstLIMB}
\end{figure}

The $j^\text{th}$ measurement, is modelled by the radiative transfer equation (RTE)~\cite{mipas2000handbook}
\begin{align}
	\label{eq:RTE} 
	y_j =   \int_{\Gamma_j}  B(\nu,T) k(\nu, T)   \frac{p(T)}{k_{\text{B}} T(r)}  x(r)  \tau(r) \text{d}r + \eta_j \, \\
	\tau(r) = \exp{ \Bigl\{ - \int^{r}_{r_\text{obs}}  k(\nu, T)   \frac{p(T)}{k_B T(r^{\prime})}  x(r^{\prime}) \text{d}r^{\prime} \Bigr\} }
\end{align}
where the path from the satellite along the line-of-sight of the $j^\text{th}$ pointing direction is $\Gamma_j$ and the ozone concentration at distance $r$ from the radiometer is $x(r)$ plus some noise $\eta_j$.
Within the stratosphere the number density $p(T) / (k_{\text{B}} T(r))$ of molecules is dependent on the pressure $p(T)$, the temperature $T(r)$, and the Boltzmann constant $k_{\text{B}}$.
The factor $\tau(r)\leq 1$ accounts for re-absorption of the radiation along the line-of-sight, which makes the RTE non linear.
The absorption constant $k(\nu, T)$ for a single gas molecule at a specific wavenumber $\nu$ is given by the HITRAN database \cite{gordon2022hitran2020} and acts as a source function when multiplied with the black body radiation $B(\nu,T)$, given by Planck`s law \cite{rybicki2000rte}.
For fundamentals on the Radiative transfer equation we recommend \cite[Chapter 1]{rybicki2000rte}.
 
To enable Matrix-Vector multiplication, we parametrise the ozone profile as a function of height, discretised into the $n$ values for each of $n$ layers of the discretised atmosphere where the $i^\text{th}$ layer is defined by two spheres of radii  $h_{i-1} < h_{i}$, $i = 1, \dots, n$, with $h_0$ and $h_{n} $.
In between the heights $h_{i-1}$ and $h_{i}$, each of the ozone concentration $x_{i}$, the pressure $p_{i}$, the temperature $T_{i}$, and thermal radiation is assumed to be constant.
Above $h_{n}$ and below $h_0$, the ozone concentration is set to zero, so no signal can be obtained.
Then depending on the parameter of interest, which is either the ozone volume mixing ratio $\bm{x} =\{x_1,x_2,\ldots,x_n\} \in \mathbb{R}^{n}$ or the fraction of pressure and temperature $\bm{p/T}= \{p_1/T_1,p_2/T_2,\ldots,p_n/T_n\} \in \mathbb{R}^{n} $, we can rewrite the integral in Eq.~\eqref{eq:RTE} as e.g. as a vector multiplication $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} $, where the non-linear absorption $\tau(r)$ is included in $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T})$.
Here, the row vector $\bm{A_{j}}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{n}$  defines a Kernel for each measurement so that the data vector
\begin{align}
	\bm{y} = \bm{A}(\bm{x},  \bm{p},\bm{T}) \, \bm{x} + \bm{\eta}= \bm{A}(\bm{x},  \bm{p},\bm{T}) \,
	\frac{ \bm{p}}{\bm{T}} + \bm{\eta} \, .
\end{align}
can be written as a matrix vector multiplication, where the matrix $\bm{A}(\bm{x},  \bm{p},\bm{T}) \in \mathbb{R}^{m \times n}$ and the noise vector $\bm{\eta} \in \mathbb{R}^{m}$.

Since the measurement process includes absorption $\tau(r)$ reducing measurements slighty and making the inverse problem weakly non-linear. 
Hence, we can approximate the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T})$ with a map $\bm{M}$ and the linear forward model $\bm{A}_L$, so that $\bm{A}(\bm{x},  \bm{p},\bm{T}) \approx \bm{M} \bm{A}_L $.
Here, $\bm{A}_{L,j} $ of matrix as $\bm{A}_L \in \mathbb{R}^{m \times n}$ is defined by the linear forward model, where absorption is neglected, e.g. $\tau = 1$. 
Then each entry in the row vector $\bm{A}_{L,j} $ is either defined by $ B(\nu,T) S(\nu, T)   \frac{\bm{p}(T)}{k_{\text{B}} \bm{T}(r)}  \text{d}r$ or $B(\nu,T) S(\nu, T)   \frac{\bm{x}}{k_{\text{B}}}  \text{d}r$, as in Eq.~\eqref{eq:RTE}, depending on the parameter of interest.
This poses a linear inverse problem with the forward map defined by the matrix $\bm{A} = \bm{M} \bm{A}_L$, where $\bm{M}$ is, more specifically, an affine map.


\section{Affine Map}
\label{sec:affine}
An affine map is any linear map in between two vector spaces or affine spaces, where an affine space does not need to preserve a zero origin, see Def. 2.3.1. in \cite{berger2009geometry}.
In other words an affine map does not need to map to the origin of the associated vector space, or is a linear map on vector spaces including translation, or in the words of my supervisor, C. F., an affine map is a Taylor series of first order.
For more information on affine spaces and maps we refer to the books \cite{berger2009geometry, katsumi1994affine}
\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
		\node[rectnode] at (2,-4) (NL)    {$W$};
		\node[rectnode] at (-2,-4) (L)    {$V$};
		\draw[<-, very thick] (NL.west) -- (L.east); 
		\node[align=center] at (-5.5,-4) (f3) {linear forward model};
		\node[align=center] at (5.5,-4) (f4) {non-linear forward model};
		\node[align=center] at (0,-5) (f5) {$\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}  \approx \bm{M A}_L  \bm{x} = \bm{A x} $ };
		\node[align=center] at (0,-4) (f5) {affine Map \\ $\bm{M}$};
	\end{tikzpicture}
	\caption[Schematics of Affine Map]{Schematics of Affine Map, which approximates the linear forward model to the non-linear forward model.}
\end{figure}

Consequently, we introduce an affine map $ \bm{M}:\bm{A}_L \bm{x} \rightarrow \bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}$, which maps the linear forward model $\bm{A}_L \bm{x}$ onto the non-linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T}) \bm{x}$.
Then the non linear forward model $\bm{A}(\bm{x},  \bm{p},\bm{T}) \approx \bm{M} \bm{A}_L$ is approximated by the affine map $\bm{M}$ and the linear forward model $\bm{A}_L$.
In practise we generate two affine subspaces spaces $V = \big\{ \bm{A}(\bm{x}^{(1)}, \bm{p,T}), \dots ,\bm{A}(\bm{x}^{(m)}, \bm{p,T})\big\} $ and $W = \big\{ \bm{A}_L\bm{x}^{(1)}, \dots ,\bm{A}_L\bm{x}^{(m)}\big\}$ over the same field, with fixed $\bm{p,T}$ and find the mapping in between those.
Here, the parameter $\bm{x}$ is distributed as the posterior distribution $\big\{  \bm{x}^{(1)} , \dots, \bm{x}^{(m)} \big\} \sim \pi(\bm{x}|\bm{\theta},\bm{y})$ conditioned on the hyper-parameters $\bm{\theta}$, according to our Bayesian hierarchical model.


\section{Bayesian Inference}
\label{sec:bayes}
In this section, we introduce the basics of Bayesian inference for a general unknown parameter $\bm{x}$ given observed data
\begin{align}
	\bm{y} = \bm{A} \bm{x} + \bm{\eta},
	\label{eq:LinDat}
\end{align}
based on a linear forward model $\bm{A}$ and additive noise $\bm{\eta}$.
A more advanced Bayesian framework, applied/tailored to the previously introduced forward model, will be developed in Section~\ref{}.


We can visualise the correlation structure between parameters as well as how distributions progress in a measurement process, using a hierarchically ordered directed acyclic graph (DAG), see Figure~\ref{fig:FirstDAG}.
Since any observational process naturally involves random noise, we include this in the DAG and classify the noise variance as a hyper-parameter within $\bm{\theta}$ \cite{fox2016fast}.  
Other hyper-parameters, to which we assign a hyper-prior distribution $\pi(\bm{\theta})$, may influence the parameters $\bm{x}$ either statistically (indicated by solid arrows), as in Figure~\ref{fig:FirstDAG}, or deterministically (indicated by dashed arrows).
Here we incorporate prior knowledge of $\bm{\theta}$ and the parameter $\bm{x}$ by defining $\pi(\bm{\theta})$ and the prior distribution $\pi(\bm{x}|\bm{\theta})$ according to their receptive physical properties or functional dependences.
This is one of the great strength of Bayesian modelling compared to e.g. regularisation, see section \ref{sec:regularise}.
Then the parameter $\bm{x}$ is mapped deterministically through the forward model onto the space of all measurable quantities $\bm{u}$. From this space, we statistically observe the actual data $\bm{y}$, which includes random (statistical) noise as mentioned above.
The distribution of the data conditioned on the  hyper-parameters $\bm{\theta}$ and the parameters $\bm{x}$ is called the likelihood function $\pi(\bm{y}|\bm{\theta},\bm{x})$, which includes information about the measurement process through the forward model.
Then given some observed data, we like to characterise the posterior distribution $\pi(\bm{\theta}, \bm{x}|\bm{y})$ of the underlying parameters and hyper-parameters by reversing the arrows in Figure~\ref{fig:FirstDAG}. 
\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
	\node[roundnode2] at (0,3.5) (th)    {$\bm{\theta}$};
	\node[roundnode2] at (0,1.5) (x)    {$\bm{x}$};
	\node[roundnode2] at (0,-1.5) (u)    {$\bm{u}$};
	\node[rectnode] at (0,-3.5) (y)    {$\bm{y}$};
	
	\draw[->, very thick] (th.south) -- (x.north); 
	\draw[->, very thick, mydotted] (x.south) -- (u.north); 
	\draw[->, very thick] (u.south) -- (y.north); 
	\draw[->, very thick] (th) edge[bend right=60] (y);  
	
	\node[align=center] at (2.8,3.5) (tht) {$\sim \pi_{\bm{\theta}}(\cdot) $ hyper-parameters};
	\node[align=center] at (2.4,1.5) (xt) {$\sim \pi_{\bm{x}}(\cdot|\bm{\theta}) $ parameters};
	\node[align=center] at (2.25,0) (At) {$\bm{u} = \bm{Ax}$ noise free data};
	\node[align=center] at (3,-1.5) (ut) {space of all measurables};
	\node[align=center] at (2,-3.5) (yt) {$\sim \pi_{\bm{y}}(\cdot|\bm{\theta},\bm{x})$ data};
	\node[align=center] at (-3,0) (nt) {noise};
\end{tikzpicture}
	\caption[Bayesian Inference DAG]{The directed acyclic graph (DAG) for a linear inverse problem visualises statistical dependencies as solid line arrows and deterministic dependencies as dotted arrows.
	The parameters $\bm{x}$ have some statistical dependency of those hyper-parameters $\bm{\theta}$, which are distributed as $\pi(\bm{\theta})$. Then a parameter $\bm{x} \sim \pi_{\bm{x}}(\cdot|\bm{\theta})$ is mapped onto the space of all measurables $\bm{u}=\bm{Ax}$ deterministically through the linear forward model $\bm{A}$.
	From the space of all measurables we observe some data $\bm{y} = \bm{Ax} + \bm{\eta}$, statistically,  so that $\bm{y}\sim \pi_{\bm{y}}(\cdot|\bm{\theta},\bm{x})$ , with naturally some random noise $\bm{\eta} \sim \pi_{\bm{\eta}}(\cdot|\bm{\theta})$.}
	\label{fig:FirstDAG}
\end{figure}

The posterior distribution, our the function of interest, is defined by Bayes theorem
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) = \frac{ \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta})}{\pi(\bm{y})} \, ,
\end{align}
with the prior distribution $\pi(\bm{x}, \bm{\theta}) = \pi(\bm{x}| \bm{\theta}) \pi(\bm{\theta}) $ and the normalising constant $\pi(\bm{y})$.
If the normalising constant is finite and non-zero we approximate the posterior distribution
\begin{align}
	\pi(\bm{x},\bm{\theta}|\bm{y}) \propto \pi(\bm{y} | \bm{x}, \bm{\theta} ) \pi(\bm{x}, \bm{\theta}) \, .
\end{align}
and the expectation of any a function $h(\bm{x}_{\bm{\theta}})$, where $\bm{x}$ may depend on $\bm{\theta}$, is described as 
\begin{align}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}_{\bm{\theta}})] =  \underbrace{\int \int   h(\bm{x}_{\bm{\theta}}) \,  \pi(\bm{x}, \bm{\theta} | \bm{y} ) \, \text{d} \bm{x}  \, \text{d} \bm{\theta}}_{\bm{\mu}_{\text{int}}}   \label{eq:expPos} \, ,
\end{align}
which is may a high dimensional integral and computationally not feasible to solve.
Then the sample based Monte Carlo estimate
\begin{align}
	\label{eq:sampMean}
	\text{E}_{\bm{x},\bm{\theta}|\bm{y}} [h(\bm{x}_{\bm{\theta}})] \approx \underbrace{ \frac{1}{N} \sum_{k=1}^{N} h(\bm{x}^{(k)}_{\bm{\theta}})  }_{\bm{\mu}_{\text{samp}}} \, ,
\end{align}
for large enough $N$ (law of large numbers \cite[Chapter 17]{tweedie2009measprob}) is unbiased \cite{roberts2004general}, where the samples $\{\bm{x}^{(k)},\bm{\theta}^{(k)} \}\sim \pi_{\bm{x}, \bm{\theta}}(\cdot|\bm{y})$, for $k = 1, \dots, N$, form a sample set $\mathcal{M} =\{ (\bm{x},\bm{\theta})^{(1)}, \dots ,  (\bm{x},\bm{\theta})^{(N)} \}$.
Furthermore the central limit theorem states that the samples means $ \bm{\mu}^{(i)}_{samp} $, of independent samples sets $\mathcal{M}_i$ for $i = 1, \dots, n$ of any distribution, converge in distribution to a normal distribution so that
\begin{align}
	\sqrt{n} (\bm{\mu}^{(i)}_{\text{samp}} -  \bm{\mu}_{\text{int}} ) \overset{\mathcal{D}}{\longrightarrow} \mathcal{N} (0,\sigma^2) \text{\cite{geyer1992practical}},
\end{align}
and if $\sigma^2 < \infty$ the Monte Carlo error $\bm{\mu}^{(i)}_{\text{samp}} -  \bm{\mu}_{\text{int}} $ is bounded.

Generating a representative sample set from the posterior distribution presents a significant challenge. This is due to the strong correlations that often exist between the parameters and hyper-parameters, as discussed by Rue and Held in \cite{rue2005gaussian} and illustrated in Appendix~\ref{ap:Correlatation}.
One way to address this issue is to parametrise $\bm{x}$ directly in terms of the hyper-parameters $\bm{\theta}$, i.e., $\bm{x}(\bm{\theta})$, or to factorise the posterior distribution as
\begin{align}
	\pi(\bm{x}, \bm{\theta} |  \bm{y}) = \pi(\bm{x} |  \bm{\theta}, \bm{y}) \, \pi(\bm{\theta} |   \bm{y}),
\end{align}
into the conditional posterior $\pi(\bm{x} |  \bm{\theta}, \bm{y})$ over the latent field $\bm{x}$ and the marginal posterior $\pi(\bm{\theta} |  \bm{y})$ over the hyper-parameters $\bm{\theta}$.
This approach is particularly advantageous when $\bm{x}$ is high-dimensional (e.g., $\bm{x} \in \mathbb{R}^n$ with $n = 45$), while $\bm{\theta}$ is low-dimensional (e.g., two-dimensional).  
Applying the law of total expectation~\cite{champ2022generalizedlawtotalcovariance}, Eq.~\eqref{eq:expPos} becomes
\begin{align}
	\mathbb{E}_{\bm{x} |  \bm{y}} [h(\bm{x})] 
	= \mathbb{E}_{\bm{\theta} |  \bm{y}} \left[ \mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} [h(\bm{x}_{\bm{\theta}})] \right] 
	= \int \mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} \left[ h(\bm{x}_{\bm{\theta}}) \right] \, \pi(\bm{\theta} |  \bm{y}) \, \mathrm{d}\bm{\theta},
	\label{eq:MargExpPos}
\end{align}
where, in the case of a linear-Gaussian Bayesian hierarchical model, both the marginal distribution and the inner expectation $\mathbb{E}_{\bm{x} |  \bm{\theta}, \bm{y}} \left[ h(\bm{x}_{\bm{\theta}}) \right]$ are well defined.

Assuming Gaussian noise $\bm{\eta} \sim \mathcal{N}(0, \bm{\Sigma}(\bm{\theta}))$, we define a linear-Gaussian Bayesian hierarchical model as:
\begin{subequations}
	\begin{align}
		\bm{y} |  \bm{x}, \bm{\theta} &\sim \mathcal{N}(\bm{A} \bm{x}, \bm{\Sigma}(\bm{\theta})) \label{eq:likelihood} \\
		\bm{x} |  \bm{\theta} &\sim \mathcal{N}(\bm{\mu}, \bm{Q}^{-1}(\bm{\theta})) \label{eq:xPrior} \\
		\bm{\theta} &\sim \pi(\bm{\theta}) \label{eq:gammaPrior},
	\end{align}
	\label{eq:BayMode}
\end{subequations}
with a normally distributed likelihood $\pi(\bm{y} |  \bm{x}, \bm{\theta})$ and prior $\pi(\bm{x} |  \bm{\theta})$, governed by the noise covariance matrix $\bm{\Sigma}(\bm{\theta})$ and the prior precision matrix $\bm{Q}(\bm{\theta})$. The prior mean is denoted by $\bm{\mu}$, and the hyper-prior distribution is $\pi(\bm{\theta})$~\cite{fox2016fast}.
This model enables efficient factorisation of the posterior distribution, we refer to this as the marginal and then conditional (MTC) method.



\subsection{Marginal and then conditional posterior distribution}
\label{subsec:MTC}

For the linear-Gaussian Bayesian hierarchical model specified in Eq.~\ref{eq:BayMode}, the marginal posterior distribution over the hyper-parameters is given by
\begin{align}
	\pi(\bm{\theta} |  \bm{y}) &= \int \pi(\bm{x}, \bm{\theta} | \bm{y}) \, \mathrm{d} \bm{x} \\ 
	\label{eq:condHyper}
	&\propto \sqrt{ \frac{ \det( \bm{\Sigma}^{-1} ) \,  \det( \bm{Q} ) }{\det( \bm{Q} + \bm{A}^T \bm{\Sigma}^{-1} \bm{A} ) } } \times \exp \left[ - \frac{1}{2}(\bm{y} - \bm{A} \bm{\mu})^T \bm{Q}_{\bm{\theta |  y}} (\bm{y} - \bm{A} \bm{\mu}) \right] \pi(\bm{\theta}) \, ,
\end{align}
with
\begin{align}
	\bm{Q}_{\bm{\theta |  y}} = \bm{\Sigma}^{-1} - \bm{\Sigma}^{-1} \bm{A} \left( \bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} \right)^{-1} \bm{A}^T \bm{\Sigma}^{-1} \, ,
\end{align}
see~\cite[Lemma 2]{fox2016fast}. 
Conditioned on the hyper-parameters $\bm{\theta}$, we can draw samples from the conditional posterior distribution
\begin{align}
	\bm{x} |  \bm{\theta}, \bm{y} \sim \mathcal{N} \Big(
	\underbrace{\bm{\mu} + \left( \bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} \right)^{-1} \bm{A}^T \bm{\Sigma}^{-1} (\bm{y} - \bm{A} \bm{\mu})}_{\bm{\mu}_{\bm{x} |  \bm{\theta}, \bm{y}}},
	\underbrace{ \left( \bm{A}^T \bm{\Sigma}^{-1} \bm{A} + \bm{Q} \right)^{-1} }_{\bm{\Sigma}_{\bm{x} |  \bm{y}, \bm{\theta}}}
	\Big) \, ,
\end{align}
using the Randomise-then-Optimise (RTO) method (see Section~\ref{subsec:RTO}), or compute weighted expectations, as in Eq.~\ref{eq:MargExpPos}, of the conditional mean and covariance matrix, where the weights are given by $\pi(\bm{\theta} |  \bm{y})$. 
Note that both the noise covariance $\bm{\Sigma} = \bm{\Sigma}(\bm{\theta})$ and the prior precision matrix $\bm{Q} = \bm{Q}(\bm{\theta})$ depend on the hyper-parameters $\bm{\theta}$.

\section{Regularisation}
\label{sec:regularise}

Another method for obtaining a solution to the linear inverse problem in Eq.~\ref{eq:LinDat} is regularisation. In this approach, we seek a solution $\bm{x}_{\lambda}$ that minimises both the data misfit norm and a regularisation semi-norm, as described in~\cite{fox2016fast}. Here we focus on a regularisation semi-norm for the case of Tikhonov regularisation~\cite{kaipio2005statinv, tan2016LecNot}, which closely resembles a linear-Gaussian hierarchical Bayesian model, as introduced in Eq.~\ref{eq:BayMode}.

Given a parameter vector $\bm{x}$, a linear forward model matrix $\bm{A}$, and data $\bm{y}$, the data misfit norm
\begin{align}
	\left\lVert \bm{y} - \bm{A} \bm{x} \right\rVert
\end{align}
quantifies how well the noise-free data  $\bm{A}\bm{x}$ matches the observed data.
The regularisation semi-norm
\begin{align}
	\lambda \left\lVert \bm{T} \bm{x} \right\rVert
\end{align}
penalises the solution according to the regularisation operator $\bm{T}$ and the regularisation parameter $\lambda > 0$.
For a fixed $\lambda$, the regularised solution
\begin{align}
	\bm{x}_{\lambda} = \underset{\bm{x}}{\mathrm{arg\,min}} \left\lVert \bm{y} - \bm{A} \bm{x} \right\rVert^2 + \lambda \left\lVert \bm{T} \bm{x} \right\rVert^2
\end{align}
is obtained by taking the derivative of the objective function:
\begin{align}
	& & \nabla_{\bm{x}_{\lambda}} \left\{ (\bm{y} - \bm{A} \bm{x}_{\lambda})^T (\bm{y} - \bm{A} \bm{x}_{\lambda}) + \lambda \bm{x}_{\lambda}^T \bm{T}^T \bm{T} \bm{x}_{\lambda} \right\} &= 0 \\
	&\iff & \nabla_{\bm{x}_{\lambda}} \left\{ \bm{y}^T \bm{y} + \bm{x}_{\lambda}^T \bm{A}^T \bm{A} \bm{x}_{\lambda} - 2 \bm{y}^T \bm{A} \bm{x}_{\lambda} + \lambda \bm{x}_{\lambda}^T \bm{T}^T \bm{T} \bm{x}_{\lambda} \right\} &= 0 \\
	&\iff & 2 \bm{A}^T \bm{A} \bm{x}_{\lambda} - 2 \bm{A}^T \bm{y} + 2 \lambda \bm{T}^T \bm{T} \bm{x}_{\lambda} &= 0.
\end{align}
Solving this equation yields the regularised solution
\begin{align}
	\bm{x}_{\lambda} = (\bm{A}^T \bm{A} + \lambda \bm{L})^{-1} \bm{A}^T \bm{y} \, ,
\end{align}
where we define $\bm{L} := \bm{T}^T \bm{T}$, which typically represents a discrete approximation of a derivative operator~\cite{tan2016LecNot}.

In practice, $\bm{x}_{\lambda}$ is computed for a range $\lambda$-values and evaluated based on the trade-off between the data misfit and the regularisation norm. The optimal value of $\lambda$ is often chosen as the point of maximum curvature on the so-called L-curve~\cite{hansen1993use}, which we plot in Section~\ref{}.



\section{Sampling Methods}
\label{sec:sampling}

In this section we present the sampling methods used in this thesis and show how these methods draw samples $ \mathcal{M} = \{ (\bm{x}, \bm{\theta} )^{(1)}, \dots, (\bm{x}, \bm{\theta} )^{(k)}, \dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y})$ from the desired target distribution, so that we can apply sample-based estimates as in Eq.~\ref{eq:sampMean}.
Here, $\mathcal{M}$ denotes a Markov chain, where each new sample $(\bm{x}, \bm{\theta})^{(k)}$ is affected only by the previous one, $(\bm{x}, \bm{\theta})^{(k-1)}$~\cite{}.
Markov chain Monte Carlo (MCMC) methods generate such a chain $\mathcal{M}$ using random (Monte Carlo) proposals $(\bm{x}, \bm{\theta})^{(k)} \sim q( \cdot |  (\bm{x}, \bm{\theta})^{(k-1)})$ according to a proposal distribution conditioned on the previous sample (Markov), where ergodicity of $\mathcal{M}$ is a sufficient criterion for using sample-based estimates~\cite{tan2016LecNot, roberts2004general}.

The ergodicity theorem in~\cite{tan2016LecNot} states that, if a Markov chain $\mathcal{M}$ is aperiodic, irreducible, and reversible, then it converges to a unique stationary equilibrium distribution.
In other words, if the chain can reach any state from any other state (irreducibility), is not confined to periodic cycles (aperiodicity), and is reversible (detailed balance condition~\cite{tan2016LecNot}), then it will converge to the desired target distribution $\pi(\bm{x}, \bm{\theta} |  \bm{y})$.
In practice, one can inspect the trace $\pi(\bm{x}^{(k)}, \bm{\theta}^{(k)} |  \bm{y})$ for $k = 1, \dots, N$ and visually assess convergence and mixing properties of the chain to evaluate ergodicity.
The sampling methods used in this thesis possess proven ergodic properties, and we therefore refer the reader to the corresponding literature for further details.


\subsection{Metropolis- within Gibbs sampling}
As introduced in Section~\ref{subsec:MTC}, when using the MTC method we sample separately from $\pi(\bm{\theta} |  \bm{y})$ and $\pi(\bm{x} |  \bm{\theta}, \bm{y})$. To sample from $\pi(\bm{\theta} |  \bm{y})$, we use a Metropolis-within-Gibbs sampler as described in~\cite{fox2016fast}.
In this thesis the sampler is applied to the two-dimensional case only, with $\bm{\theta} = (\theta_1, \theta_2)$, where we perform a Metropolis step in the $\theta_1$ direction and a Gibbs step in the $\theta_2$ direction.
Ergodicity for this approach is proven in~\cite{roberts2006harris}.

The Metropolis-within-Gibbs algorithm begins with an initial guess $\bm{\theta}^{(t)}$ at $t=0$. We then propose a new sample $\theta_1 \sim q(\theta_1 |  \theta_1^{(t-1)})$, conditioned on the previous state, using a symmetric proposal distribution $q(\theta_1 |  \theta_1^{(t-1)}) = q(\theta_1^{(t-1)} |  \theta_1)$.
This is a special case of the Metropolis-Hastings algorithm~\cite{roberts2006harris} and cancels in the computation of the acceptance probability $\alpha$. 
We accept and set $\theta_1^{(t)} = \theta_1$ with
\begin{align}
	\alpha(\theta_1 |  \theta_1^{(t-1)}) = \min \left\{ 1, \frac{\pi(\theta_1 |  \theta_2^{(t-1)}, \bm{y}) \, q(\theta_1^{(t-1)} |  \theta_1)}{\pi(\theta_1^{(t-1)} |  \theta_2^{(t-1)}, \bm{y}) \, q(\theta_1 |  \theta_1^{(t-1)})} \right\}
\end{align}
or reject and keep $\theta_1^{(t)} = \theta_1^{(t-1)}$, which we do by comparing $\alpha$ to a uniform random number $u \sim \mathcal{U}(0,1)$. 

Next, we perform a Gibbs step in the $\theta_2$ direction, where Gibbs sampling is again a special case of the Metropolis-Hastings algorithm with acceptance probability equal to 1, and draw the next sample $\theta_2^{(t)} \sim \pi(\cdot |  \theta_1^{(t)}, \bm{y})$, conditioned on the current value $\theta_1^{(t)}$. 

We repeat this procedure $N^{\prime}$ times and ensure convergence independently of the initial sample (irreducibility) by discarding the initial $N_{\text{burn-in}}$ samples after a so-called burn-in period, resulting in a Markov chain of length $N = N^{\prime} - N_{\text{burn-in}}$.

\begin{algorithm}[!ht]
	\caption{Metropolis within Gibbs}
	\begin{algorithmic}[1]
		\STATE Initialize and suppose two dimensional vector \( \bm{\theta}^{(0)}  =( \theta_1^{(0)} , \theta_2^{(0)}  ) \)
		\FOR{ \( k = 1, \dots, N \)}
		\STATE Propose \( \theta_1 \sim q(\cdot   | \theta_1 ^{(t-1)}) = q(\theta_1 ^{(t-1)} |\cdot  ) \)
		\STATE Compute
		\[ \alpha( \theta_1  | \theta_1^{(t-1)}) = \min \left\{ 1, \frac{\pi(\theta_1  | \theta^{(t-1)}_2, \bm{y}) \cancel{q(\theta_1^{(t-1)} | \theta_1 ) } }{\pi(\theta_1^{(t-1)}| \theta_2^{(t-1)}, \bm{y}) \cancel{q(\theta_1 | \theta_1^{(t-1)})} } \right\} \]
		\STATE Draw $u \sim \mathcal{U}(0,1)$
		\IF{$\alpha \geq u$ }
		\STATE Accept and set \( \theta_1^{(t)} = \theta_1 \)
		\ELSE  
		\STATE Reject and keep \(\theta_1^{(t)} = \theta_1^{(t-1)} \)
		\ENDIF
		\STATE Draw \(\theta_2^{(t)} \sim  \pi( \cdot | \theta_1^{(t)} , \bm{y} )\) 
		\ENDFOR
		\STATE Output: $ \bm{\theta}^{(0)}, \dots,  \bm{\theta}^{(k)} , \dots,   \bm{\theta}^{(N)} \sim \pi(\bm{\theta}| \bm{y}) $
	\end{algorithmic}
\end{algorithm}


\subsection{Draw a sample from a multivariate normal distribution \label{subsec:RTO}}

After sampling from the marginal posterior $\pi(\bm{\theta} |  \bm{y})$, we draw samples from the conditional distribution $\pi(\bm{x} |  \bm{\theta}, \bm{y})$ as part of the MTC scheme. For linear-Gaussian Bayesian hierarchical models, samples from the multivariate normal distribution $\pi(\bm{x} |  \bm{\theta}, \bm{y})$ can be efficiently generated using the Randomise-then-Optimise (RTO) method~\cite{bardsley2012mcmc}.

The full conditional distribution can be rewritten as
\begin{align}
	\pi(\bm{x} |  \bm{y}, \bm{\theta}) &\propto \pi(\bm{y} |  \bm{x}, \bm{\theta}) \, \pi(\bm{x} |  \bm{\theta}) \\
	&= \exp \left( -\left\lVert \hat{\bm{A}} \bm{x} - \hat{\bm{y}} \right\rVert^2 \right),
\end{align}
where
\begin{align}
	\label{eq:minimizer}
	\hat{\bm{A}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta}) \bm{A} \\
		\bm{Q}^{1/2}(\bm{\theta})
	\end{bmatrix}, \quad 
	\hat{\bm{y}} = 
	\begin{bmatrix}
		\bm{\Sigma}^{-1/2}(\bm{\theta}) \bm{y} \\
		\bm{Q}^{1/2}(\bm{\theta}) \bm{\mu}
	\end{bmatrix} \quad \text{\cite{bardsley2014randomize}}.
\end{align}
A sample $\bm{x}_i $ can be computed by minimising the following equation with respect to $\hat{\bm{x}}$ :
\begin{align}
	\bm{x}_i = \arg \min_{\hat{\bm{x}}} \lVert \hat{\bm{A}} \hat{\bm{x}} - ( \hat{\bm{y}} + \bm{b} ) \rVert^2 , \quad \bm{b} \sim \mathcal{N}(\bm{0}, \mathbf{I}) \, ,
\end{align}
where we add a randomised perturbation $\bm{b}$.
As in Section~\ref{sec:reg}, this expression can be rewritten as
\begin{align}
	\label{eq:RTO}
	\left( \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A} + \bm{Q}(\bm{\theta}) \right) \bm{x}_i = \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{y} + \bm{Q}(\bm{\theta}) \bm{\mu} + \bm{v}_1 + \bm{v}_2,
\end{align}
where the term $-\hat{\bm{A}}^T \bm{b}$ is decomposed as $\bm{v}_1 + \bm{v}_2$, with $\bm{v}_1 \sim \mathcal{N}(\bm{0}, \bm{A}^T \bm{\Sigma}^{-1}(\bm{\theta}) \bm{A})$ and $\bm{v}_2 \sim \mathcal{N}(\bm{0}, \bm{Q}(\bm{\theta}))$, representing independent Gaussian random variables~\cite{bardsley2012mcmc, fox2016fast}.

If the Markov chain over the marginal posterior $\pi(\bm{\theta} |  \bm{y})$ is ergodic, and the conditional samples $\bm{x}^{(k)} \sim \pi(\bm{x}|   \bm{\theta}^{(k)}, \bm{y})$ are drawn independently, then the resulting joint chain $\{ (\bm{x}, \bm{\theta})^{(1)}, \dots, (\bm{x}, \bm{\theta})^{(N)} \} \sim \pi(\bm{x}, \bm{\theta} |  \bm{y})$ is also ergodic~\cite{acosta2014markov}.

\subsection{t-walk}
If the parameters $\bm{x}$ are functionally dependent on the hyper-parameters $\bm{\theta}$, i.e., $\bm{x} = \bm{x}(\bm{\theta})$, we can sample directly from the marginal posterior $\pi(\bm{\theta} | \bm{y})$ using the t-walk algorithm by Christen and Fox~\cite{christen2010general}. 
The t-walk is employed as a black-box sampler, requiring only the specification of the number of samples, burn-in period, support region, and the sampling distribution. 
Convergence to the target distribution is guaranteed by the algorithm’s construction.

\section{Numerical Approxiamtion Methods - Tensor Train}
\label{sec:tensortrain}
First, we provide a short overview to probability spaces and their associated measures, as a foundation for deriving marginal probability distribution. This is followed by a brief introduction to the tensor train format.

Assume that the triple $(\Omega, \mathcal{F}, \mathbb{P})$ defines a probability space, where $\Omega$ denotes the complete sample space, $\mathcal{F}$ is a $\sigma$-algebra consisting of a collection of countable subsets $\{A_n\}_{n \in \mathbb{N}}$ with $A_n \subseteq \Omega$, and $\mathbb{P}$ is a probability measure defined on $\mathcal{F}$. The formal conditions for $\mathbb{P}$ to be a probability measure, and for $\mathcal{F}$ to be a $\sigma$-algebra over $\Omega$, are given in Appendix~\ref{ch:Mesure}.
We denote by $\mathbb{P}(A)$ the probability of an event $A \in \mathcal{F}$,
\begin{align}
	\mathbb{P}(A) = \int_A \mathrm{d} \mathbb{P} \text{\cite{}}.
\end{align}
By applying the Radon-Nikodym theorem~\cite{kopp2004measintprob}, we can change variables
\begin{align}
	\mathbb{P}(A) = \int_A \frac{\mathrm{d} \mathbb{P}}{\mathrm{d}x} \, \mathrm{d}x = \int_A \pi(x) \, \mathrm{d}x,
\end{align}
where $\mathrm{d}x$ is a reference measure on the same probability space, commonly referred to as the Lebesgue measure. 
The Radon-Nikodym derivative $\frac{\mathrm{d} \mathbb{P}}{\mathrm{d}x}$ of $\mathbb{P}$ with respect to $x$, and is often interpreted as the probability density function (PDF) $\pi(x)$. Thus, we say that $\mathbb{P}$ has a density $\pi(x)$ with respect to $x$~\cite[Chapter 10]{simonnet1996measprob}.

Now, let $X: \Omega \longrightarrow \mathbb{R}^d$ be a $d$-dimensional random variable mapping from the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ to the measurable space $(\mathbb{R}^d, \mathcal{X})$, where $\mathcal{X}$ is a collection of subsets in $\mathbb{R}^d$.
Then the associated PDF $\pi(x)$, is a joint density of $X$, induced by the probability measure on $\Omega$~\cite{VesaInvLect, kopp2004measintprob}.
As by Cui et al.~\cite{cui2022deep}, we can define the parameter space as the Cartesian product $\mathcal{X} = \mathcal{X}_1 \times \mathcal{X}_2 \times \dots \times \mathcal{X}_d$ with $ x_k \in \mathcal{X}_k \subseteq \mathbb{R}$ and $x = ( x_1,\dots ,x_k,\dots,x_d )$.
The marginal density function for the $k$-th component is then given by
\begin{align}
	f_{X_k}(x_k) = \int_{\mathcal{X}_1} \cdots \int_{\mathcal{X}_d} \uplambda(x) \, \pi(x) \, \mathrm{d}x_1 \cdots \mathrm{d}x_{k-1} \, \mathrm{d}x_{k+1} \cdots \mathrm{d}x_d,
\end{align}
where we integrate over all dimensions except the $k$-th.
Here, we introduce a weight function $\uplambda(x)$~\cite{davis2007methods}, which can be useful for quadrature rules??. 
Cui et al.~\cite{cui2022deep} refer to $\uplambda(x)$ as a "product-form Lebesgue-measurable weighting function" and define it as
\begin{align*}
	\uplambda(\mathcal{X}) = \prod_{i = 1}^{d} \uplambda_i(\mathcal{X}_i), \quad \text{where} \quad \uplambda_i(\mathcal{X}_i) = \int_{\mathcal{X}_i} \uplambda_i(x_i) \, \mathrm{d}x_i.
\end{align*}

Using the tensor train (TT) format, we can efficiently approximate a $d$-dimensional function $\pi(x)$ and compute marginal probability distributions at low computational cost. To do so, we first define a $d$-dimensional discrete univariate grid over the parameter space $\mathcal{X}$, with $n$ grid points in each dimension.
In the tensor train format we can represent the function over this $d$-dimensional grid as a product train of 2-rank to 3-rank tensors, or TT-cores. More specifically each core $\pi_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ has ranks $r_{k-1}$ and $r_k$, for $k = 1, \dots, d$, connecting it with its neighbouring cores, as illustrated in Figure~\ref{fig:TTfig}.
For the first and last cores, the outer ranks are set to $r_0 = r_d = 1$.
For a fixed point $x = (x_1, \dots, x_d)$ on the grid, the value $\pi(x)$ is evaluated as a sequence of matrix multiplications 
\begin{align*}
\pi_1(x_1)  \pi_2(x_2)  \cdots \pi_d(x_d) = 	\pi(x)  \in \mathbb{R},
\end{align*}
where each core $\pi_k(x_k)$, becomes a matrix of size $r_{k-1} \times r_k$.
\begin{figure}[ht!]
	\centering
\begin{subfigure}{\textwidth}
	\input{TTSchem.pdf_tex}
	\caption{}
\end{subfigure}
	\centering
\begin{subfigure}{\textwidth}
\begin{tikzpicture} 
	\node[rectnode] at (-5,0) (T1)    {$1 \times n \times r_1$};
	\node[rectnode] at (-2,0) (T2)    {$r_1 \times n \times r_{2}$};
	
	\node[rectnode] at (3,0) (Tn1)    {$r_{d-2} \times n \times r_{d-1} $};
	\node[rectnode] at (6.75,0) (Tn)    {$r_{d-1} \times n \times 1$};
	\draw[-, very thick] (T1.east) -- (T2.west); 
	\draw[-, very thick] (Tn.west) -- (Tn1.east); 
	\draw[-, mydotted, very thick] (T2.east) -- (Tn1.west);
	
	\node[align=center] at (-3.5,0.25) (R) {$r_1$};
	\node[align=center] at (5,0.25) (R) {$r_{d-1}$};	
\end{tikzpicture} 
	\caption{}
\end{subfigure}
\caption[Visualization of a tensor train]{Here, we visualise the tensor train cores as two- and three-dimensional matrices. 
Each core has a length $n$, corresponding to the number of grid points in one dimension, and the cores are connected through ranks $r_k$. 
More specifically, a core $\pi_k$ has dimensions $r_{k-1} \times n \times r_k$, with outer ranks $r_0 = r_d = 1$. 
Figure~(a) is adapted from~\cite{fox2021grid}.}
\label{fig:TTfig}
\end{figure}
Consequently, with a tensor train approximation, the marginal target function
\begin{align}
	\begin{split}
		f_{X_k}(x_k) = \frac{1}{z} \Big|\, 
		&\left( \int_{\mathbb{R}} \uplambda_1(x_1) \pi_1(x_1) \, \mathrm{d}x_1 \right) \cdots 
		\left( \int_{\mathbb{R}} \uplambda_{k-1}(x_{k-1}) \pi_{k-1}(x_{k-1}) \, \mathrm{d}x_{k-1} \right) \\
		&\quad \uplambda_k(x_k) \pi_k(x_k) \\
		& \left( \int_{\mathbb{R}} \uplambda_{k+1}(x_{k+1}) \pi_{k+1}(x_{k+1}) \, \mathrm{d}x_{k+1} \right) \cdots 
		\left( \int_{\mathbb{R}} \uplambda_d(x_d) \pi_d(x_d) \, \mathrm{d}x_d \right)
		\Big| 
	\end{split}
\end{align}
is computed by integrating over all TT cores except $\pi_k$, as in~\cite{dolgov2020approximation}, and includes a normalisation constant $z$~\cite{cui2022deep}.
\\


%%%%%% Squared and Basis Function %%%%%
In practice, tensor train approximations may suffer from numerical instability, particularly because it is not advantageous to approximate the target function $\pi(x)$ in for example, the logarithmic space. 
To address this, we follow the notation and procedure of Cui et al.~\cite{cui2022deep} and instead approximate the square root of the target function:
\begin{align}
	\sqrt{\pi(x)} \approx g(x) = \bm{G}_1(x_1), \dots, \bm{G}_k(x_k), \dots, \bm{G}_d(x_d).
\end{align}
Here, each TT-core is given by
\begin{equation}
	G^{(\alpha_{k-1},\alpha_k)}_k(x_k) = \sum_{i=1}^{n_k} \phi^{(i)}_k(x_k) \bm{A}_k[\alpha_{k-1}, i, \alpha_k], \quad k = 1, \dots, d,
\end{equation}
where $\bm{A}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$ is the $k$-th coefficient tensor and $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ are the basis functions corresponding to the $k$-th coordinate.
We then approximate the density as
\begin{align}
	\pi(x) \approx \gamma' + g^2(x),
\end{align}
where $\gamma'$ is a small positive constant added to ensure positivity, and is chosen such that
\begin{align}
	\gamma' \leq \frac{1}{\uplambda(\mathcal{X})} \lVert g - \sqrt{\pi} \rVert_2^2.
\end{align}
This leads to the normalised target function
\begin{align}
	f_X(x) = \frac{1}{z} \pi(x) \uplambda(x) = \frac{1}{z} \left( \gamma' \uplambda(x) + g^2(x) \uplambda(x) \right),
\end{align}
where $z$ is the normalisation constant.
Given the tensor train approximation of $\sqrt{\pi}$, the marginal function $f_{X_k}(x_k)$ can be expressed as
\begin{align}
	\begin{split}
		f_{X_k}(x_k) = \frac{1}{z} \Bigg(&\gamma' \prod_{i=1}^{k-1} \uplambda_i(\mathcal{X}_i) \prod_{i=k+1}^{d} \uplambda_i(\mathcal{X}_i) \\
		&+ \left( \int_{\mathbb{R}} \bm{G}_1^2(x_1) \uplambda_1(x_1) \, \mathrm{d}x_1 \right) \cdots 
		\left( \int_{\mathbb{R}} \bm{G}_{k-1}^2(x_{k-1}) \uplambda_{k-1}(x_{k-1}) \, \mathrm{d}x_{k-1} \right) \\
		&\cdot \bm{G}_k^2(x_k) \uplambda_k(x_k) \\
		&\cdot \left( \int_{\mathbb{R}} \bm{G}_{k+1}^2(x_{k+1}) \uplambda_{k+1}(x_{k+1}) \, \mathrm{d}x_{k+1} \right) \cdots 
		\left( \int_{\mathbb{R}} \bm{G}_d^2(x_d) \uplambda_d(x_d) \, \mathrm{d}x_d \right) \Bigg).
	\end{split}
\end{align}
To compute these marginals efficiently, one can use a procedure similar to left and right orthogonalisation of TT-cores~\cite{oseledets2011tensor}. For this, we define the mass matrix $\bm{M}_k \in \mathbb{R}^{n_k \times n_k}$ as
\begin{equation}
	\bm{M}_k[i, j] = \int_{\mathcal{X}_k} \phi^{(i)}_k(x_k) \phi^{(j)}_k(x_k) \uplambda(x_k) \, \mathrm{d}x_k, \quad i, j = 1, \dots, n_k,
\end{equation}
where $\{\phi^{(i)}_k(x_k)\}_{i=1}^{n_k}$ denotes the set of basis functions for the $k$-th coordinate.



\subsection{Marginal Functions}
We compute the marginal functions using two procedures, referred to as backward marginalisation~\cite{cui2022deep} and forward marginalisation. The backward marginalisation provides us with the coefficient matrices $\bm{B}_k$, while the forward marginalisation gives the coefficient matrices $\bm{B}_{\text{pre}, n}$. These matrices enable the efficient evaluation of marginal functions, similar to~\cite{cui2022deep}.
The proposition used to compute $\bm{B}_k$, stated in Proposition~\ref{prob:backMarg}, is adapted directly from~\cite{cui2022deep}.

\begin{prop}[Backward Marginalisation]
	\label{prob:backMarg}
	Starting with the last coordinate $k = d$, we set $\bm{B}_d = \bm{A}_d$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{k-1} \in \mathbb{R}^{r_{k-2} \times n_{k-1} \times r_{k-1}}$, which we need for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_k[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{B}_k[\alpha_{k-1}, i, l_k] \bm{L}_k[i, \tau].
		\end{equation}
		\item Unfold $\bm{C}_k$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_k^{(R)} \in \mathbb{R}^{r_{k-1} \times (n_k r_k)}$:
		\begin{equation}
			\bm{Q}_k \bm{R}_k = {(\bm{C}_k^{(R)})}^{\top}.
		\end{equation}
		\item Compute the new coefficient tensor:
		\begin{equation}
			\bm{B}_{k-1}[\alpha_{k-2}, i, l_{k-1}] = \sum_{\alpha_{k-1}=1}^{r_{k-1}} \bm{A}_{k-1}[\alpha_{k-2}, i, \alpha_{k-1}] \bm{R}_k[l_{k-1}, \alpha_{k-1}].
		\end{equation}
	\end{enumerate}
\end{prop}
We start the forward marginalisation with the first dimension as in Proposition \ref{prob:ForMarg}.
\begin{prop}[Forward Marginalisation]
	\label{prob:ForMarg}
	Starting with the first coordinate $k = 1$, we set $\bm{B}_{\text{pre},1} = \bm{A}_1$. The following procedure can be used to obtain the coefficient tensor $\bm{B}_{\text{pre},k+1} \in \mathbb{R}^{r_{k} \times n_{k+1} \times r_{k+1}}$ for defining the marginal function $f_{X_k}(x_k)$:
	\begin{enumerate}
		\item Use the Cholesky decomposition of the mass matrix, $\bm{L}_k \bm{L}_k^\top = \bm{M}_k \in \mathbb{R}^{n_k \times n_k}$, to construct a tensor $\bm{C}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$:
		\begin{equation}
			\bm{C}_{\text{pre},k}[\alpha_{k-1}, \tau, l_k] = \sum_{i=1}^{n_k} \bm{L}_k[i, \tau] \bm{B}_{\text{pre},k}[\alpha_{k-1}, i, l_k] .
		\end{equation}
		\item Unfold $\bm{C}_{pre,k}$ along the first coordinate and compute the thin QR decomposition, so that $\bm{C}_{\text{pre},k}^{(R)} \in \mathbb{R}^{(r_{k-1} n_k ) \times r_k}$:
		\begin{equation}
			\bm{Q}_{pre,k}\bm{R}_{\text{pre},k} = {(\bm{C}_{\text{pre},k}^{(R)})}.
		\end{equation}
		\item Compute the new coefficient tensor $\bm{B}_{\text{pre}, k+1} \in \mathbb{R}^{r_{k-1} \times n_k \times r_k} $:
		\begin{equation}
			\bm{B}_{\text{pre}, k+1}[l_{k+1}, i, \alpha_{k+1}] = \sum_{\alpha_{k}=1}^{r_{k}} \bm{R}_{\text{pre},k}[l_{k+1}, \alpha_{k}] \bm{A}_{k+1}[\alpha_{k}, i, \alpha_{k+1}] .
		\end{equation}
	\end{enumerate}
\end{prop}

The marginal PDF of $k$-th dimension can be expressed as
\begin{equation}
	f_{X_k}(x_k) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=1}^{k-1} \uplambda_i(X_i) \prod_{i=k+1}^{d} \uplambda_i(X_i) + \sum_{l_{k-1}=1}^{r_{k-1}} \sum_{l_k=1}^{r_k} \left(\sum_{i=1}^{n} \phi^{(i)}_k(x_k) \bm{D}_k[l_{k-1},i, l_k] \right)^2 \right) \uplambda_k(x_k),
\end{equation}
where $\bm{D}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$ and $\bm{R}_{\text{pre},k-1}\in \mathbb{R}^{r_{k-1} \times r_{k-1}}$ and $\bm{B}_k \in \mathbb{R}^{r_{k-1} \times n \times r_k}$
\begin{equation}
	\bm{D}_k[l_{k-1},i,l_k] = \sum_{\alpha_{k-1}=1}^{r_{k-1}}  \bm{R}_{\text{pre},k-1}[l_{k-1}, \alpha_{k-1}] \bm{B}_k[\alpha_{k-1}, i, l_k].
\end{equation}

For the first dimension, $f_{X_1}(x_1)$ can be expressed as
\begin{equation}
	f_{X_1}(x_1) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=2}^{d} \uplambda_i(\mathcal{X}_i) + \sum_{l_1=1}^{r_1} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_1[i, l_1] \right)^2 \right) \uplambda_1(x_1),
\end{equation}
where $\bm{D}_1[i, l_1] = \bm{B}_1[\alpha_0, i, l_1]$ and $\alpha_0 = 1$,
and similarly in the last dimension
\begin{equation}
	f_{X_d}(x_d) = \frac{1}{z} \left(\gamma^{\prime} \prod_{i=1}^{d-1} \uplambda_i(\mathcal{X}_i) + \sum_{l_{n-1}=1}^{r_{d-1}} \left(\sum_{i=1}^{n} \phi^{(i)}_1(x_1) \bm{D}_d[l_{n-1},i] \right)^2 \right) \uplambda_d(x_d),
\end{equation}
where $\bm{D}_d[l_{n-1},i] = \bm{B}_{\text{pre},d}[l_{n-1}, i, \alpha_{n+1}]$ and $\alpha_{d+1} = 1$.